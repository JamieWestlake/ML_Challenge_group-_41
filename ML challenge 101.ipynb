{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c920c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f09ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = open('train.json')\n",
    "train_data = json.load(train_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19885043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some very impotant changes for the project\n",
    "#Jamie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84c6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '0b341b6938308a6d5f47edf490f6e46eae3835fa',\n",
       "  'title': 'Detecting linguistic idiosyncratic interests in autism using distributional semantic models',\n",
       "  'authorId': '3188285',\n",
       "  'authorName': 'Masoud Rouhizadeh',\n",
       "  'abstract': 'Children with autism spectrum disorder often exhibit idiosyncratic patterns of behaviors and interests. In this paper, we focus on measuring the presence of idiosyncratic interests at the linguistic level in children with autism using distributional semantic models. We model the semantic space of children’s narratives by calculating pairwise word overlap, and we compare the overlap found within and across diagnostic groups. We find that the words used by children with typical development tend to be used by other children with typical development, while the words used by children with autism overlap less with those used by children with typical development and even less with those used by other children with autism. These findings suggest that children with autism are veering not only away from the topic of the target narrative but also in idiosyncratic semantic directions potentially defined by their individual topics of interest.',\n",
       "  'year': 2014,\n",
       "  'venue': 'CLPsych@ACL'},\n",
       " {'paperId': 'c682727ee058aadbe9dbf838dcb036322818f588',\n",
       "  'title': 'Bigrams and BiLSTMs Two Neural Networks for Sequential Metaphor Detection',\n",
       "  'authorId': '2782720',\n",
       "  'authorName': 'Yuri Bizzoni',\n",
       "  'abstract': 'We present and compare two alternative deep neural architectures to perform word-level metaphor detection on text: a bi-LSTM model and a new structure based on recursive feed-forward concatenation of the input. We discuss different versions of such models and the effect that input manipulation - specifically, reducing the length of sentences and introducing concreteness scores for words - have on their performance.',\n",
       "  'year': 2018,\n",
       "  'venue': 'Fig-Lang@NAACL-HLT'},\n",
       " {'paperId': '0f9b5b32229a7245e43754430c0c88f8e7f0d8af',\n",
       "  'title': 'In Factuality: Efficient Integration of Relevant Facts for Visual Question Answering',\n",
       "  'authorId': '144748442',\n",
       "  'authorName': 'Peter Vickers',\n",
       "  'abstract': 'Visual Question Answering (VQA) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. Current models are trained on labelled data that may be insufficient to learn complex knowledge representations. In this paper, we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (Vision+Language BERT) by integrating facts extracted from an external knowledge base. Evaluation on the KVQA dataset benchmark demonstrates that our method outperforms competitive baselines by 19%, achieving new state-of-the-art results. We also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9',\n",
       "  'title': 'Variational Graph Autoencoding as Cheap Supervision for AMR Coreference Resolution',\n",
       "  'authorId': '46331602',\n",
       "  'authorName': 'Irene Li',\n",
       "  'abstract': 'Coreference resolution over semantic graphs like AMRs aims to group the graph nodes that represent the same entity. This is a crucial step for making document-level formal semantic representations. With annotated data on AMR coreference resolution, deep learning approaches have recently shown great potential for this task, yet they are usually data hunger and annotations are costly. We propose a general pretraining method using variational graph autoencoder (VGAE) for AMR coreference resolution, which can leverage any general AMR corpus and even automatically parsed AMR data. Experiments on benchmarks show that the pretraining approach achieves performance gains of up to 6% absolute F1 points. Moreover, our model significantly improves on the previous state-of-the-art model by up to 11% F1.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '07588dd5d0252c7abc99b3834a81bf23741ead4b',\n",
       "  'title': 'LIMIT-BERT : Linguistics Informed Multi-Task BERT',\n",
       "  'authorId': '30887404',\n",
       "  'authorName': 'Junru Zhou',\n",
       "  'abstract': 'In this paper, we present Linguistics Informed Multi-Task BERT (LIMIT-BERT) for learning language representations across multiple linguistics tasks by Multi-Task Learning. LIMIT-BERT includes five key linguistics tasks: Part-Of-Speech (POS) tags, constituent and dependency syntactic parsing, span and dependency semantic role labeling (SRL). Different from recent Multi-Task Deep Neural Networks (MT-DNN), our LIMIT-BERT is fully linguistics motivated and thus is capable of adopting an improved masked training objective according to syntactic and semantic constituents. Besides, LIMIT-BERT takes a semi-supervised learning strategy to offer the same large amount of linguistics task data as that for the language model training. As a result, LIMIT-BERT not only improves linguistics tasks performance but also benefits from a regularization effect and linguistics information that leads to more general representations to help adapt to new tasks and domains. LIMIT-BERT outperforms the strong baseline Whole Word Masking BERT on both dependency and constituent syntactic/semantic parsing, GLUE benchmark, and SNLI task. Our practice on the proposed LIMIT-BERT also enables us to release a well pre-trained model for multi-purpose of natural language processing tasks once for all.',\n",
       "  'year': 2019,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'b3f238fad54ec008a69d3f2777dd25d658425f5d',\n",
       "  'title': 'Event Detection with Trigger-Aware Lattice Neural Network',\n",
       "  'authorId': '46649145',\n",
       "  'authorName': 'Ning Ding',\n",
       "  'abstract': 'Event detection (ED) aims to locate trigger words in raw text and then classify them into correct event types. In this task, neural net- work based models became mainstream in re- cent years. However, two problems arise when it comes to languages without natural delim- iters, such as Chinese. First, word-based mod- els severely suffer from the problem of word- trigger mismatch, limiting the performance of the methods. In addition, even if trigger words could be accurately located, the ambi- guity of polysemy of triggers could still af- fect the trigger classification stage. To ad- dress the two issues simultaneously, we pro- pose the Trigger-aware Lattice Neural Net- work (TLNN). (1) The framework dynami- cally incorporates word and character informa- tion so that the trigger-word mismatch issue can be avoided. (2) Moreover, for polysemous characters and words, we model all senses of them with the help of an external linguistic knowledge base, so as to alleviate the prob- lem of ambiguous triggers. Experiments on two benchmark datasets show that our model could effectively tackle the two issues and outperforms previous state-of-the-art methods significantly, giving the best results. The source code of this paper can be obtained from https://github.com/thunlp/TLNN.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '79f20fb39a6e78352ebbb65b1737970837a420b5',\n",
       "  'title': 'An Algorithm for Simultaneously Bracketing Parallel Texts by Aligning Words',\n",
       "  'authorId': '2390150',\n",
       "  'authorName': 'Dekai Wu',\n",
       "  'abstract': 'We describe a grammarless method for simultaneously bracketing both halves of a parallel text and giving word alignments, assuming only a translation lexicon for the language pair. We introduce inversion-invariant transduction grammars which serve as generative models for parallel bilingual sentences with weak order constraints. Focusing on transduction grammars for bracketing, we formulate a normal form, and a stochastic version amenable to a maximum-likelihood bracketing algorithm. Several extensions and experiments are discussed.',\n",
       "  'year': 1995,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c8be1740bd2f7e2635db6c5ab774108183eb4659',\n",
       "  'title': 'Data-Efficient Language Shaped Few-shot Image Classification',\n",
       "  'authorId': '151474408',\n",
       "  'authorName': 'Zhenwen Liang',\n",
       "  'abstract': 'Many existing works have demonstrated that language is a helpful guider for image under-standing by neural networks. We focus on a language-shaped learning problem in a few-shot setting, i.e., using language to improve few-shot image classiﬁcation when language descriptions are only available during training. We propose a data-efﬁcient method that can make the best usage of the few-shot images and the language available only in training. Experimental results on dataset ShapeWorld and Birds show that our method outperforms other state-of-the-art baselines in language-shaped few-shot learning area, especially when training data is more severely limited. Therefore, we call our approach data-efﬁcient language-shaped learning (DF-LSL).',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6d9dea106deddc33196dbc05b9762d91f66047f5',\n",
       "  'title': 'LIMSI Submission for WMT’14 QE Task',\n",
       "  'authorId': '1696542',\n",
       "  'authorName': 'Guillaume Wisniewski',\n",
       "  'abstract': 'This paper describes LIMSI participation to the WMT’14 Shared Task on Quality Estimation; we took part to the wordlevel quality estimation task for English to Spanish translations. Our system relies on a random forest classifier, an ensemble method that has been shown to be very competitive for this kind of task, when only a few dense and continuous features are used. Notably, only 16 features are used in our experiments. These features describe, on the one hand, the quality of the association between the source sentence and each target word and, on the other hand, the fluency of the hypothesis. Since the evaluation criterion is the f1 measure, a specific tuning strategy is proposed to select the optimal values for the hyper-parameters. Overall, our system achieves a 0.67 f1 score on a randomly extracted test set.',\n",
       "  'year': 2014,\n",
       "  'venue': 'WMT@ACL'},\n",
       " {'paperId': 'dce5ca746224a97e532a034c371305f8bddcb5fc',\n",
       "  'title': 'Learning Joint Multilingual Sentence Representations with Neural Machine Translation',\n",
       "  'authorId': '144518416',\n",
       "  'authorName': 'Holger Schwenk',\n",
       "  'abstract': 'In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics. We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.',\n",
       "  'year': 2017,\n",
       "  'venue': 'Rep4NLP@ACL'},\n",
       " {'paperId': 'dcd72571e380eacf3ddccf225f3324eb8c51ece0',\n",
       "  'title': 'A Graph Auto-encoder Model of Derivational Morphology',\n",
       "  'authorId': '1667898858',\n",
       "  'authorName': 'Valentin Hofmann',\n",
       "  'abstract': 'There has been little work on modeling the morphological well-formedness (MWF) of derivatives, a problem judged to be complex and difficult in linguistics. We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation. The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '722058a002c8f2ee8b496073888095a37db5c83b',\n",
       "  'title': 'A Query-Driven Topic Model',\n",
       "  'authorId': '2072874946',\n",
       "  'authorName': 'Zheng Fang',\n",
       "  'abstract': 'Topic modeling is an unsupervised method for revealing the hidden semantic structure of a corpus. It has been increasingly widely adopted as a tool in the social sciences, including political science, digital humanities and sociological research in general. One desirable property of topic models is to allow users to find topics describing a specific aspect of the corpus. A possible solution is to incorporate domain-specific knowledge into topic modeling, but this requires a specification from domain experts. We propose a novel query-driven topic model that allows users to specify a simple query in words or phrases and return query-related topics, thus avoiding tedious work from domain experts. Our proposed approach is particularly attractive when the user-specified query has a low occurrence in a text corpus, making it difficult for traditional topic models built on word cooccurrence patterns to identify relevant topics. Experimental results demonstrate the effectiveness of our model in comparison with both classical topic models and neural topic models.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '69116800a8a8195531d29c8e14cefb1c92cbb8a7',\n",
       "  'title': 'Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News',\n",
       "  'authorId': '1721683964',\n",
       "  'authorName': 'Ashkan Kazemi',\n",
       "  'abstract': 'In this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. We experiment with two methods: (1) an extractive method based on Biased TextRank – a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NLP4IF'},\n",
       " {'paperId': '40b3bebc595ca091b4ee654e12272ad9201c04dc',\n",
       "  'title': 'How transfer learning impacts linguistic knowledge in deep NLP models?',\n",
       "  'authorId': '145938140',\n",
       "  'authorName': 'Nadir Durrani',\n",
       "  'abstract': 'Transfer learning from pre-trained neural language models towards downstream tasks has been a predominant theme in NLP recently. Several researchers have shown that deep NLP models learn non-trivial amount of linguistic knowledge, captured at different layers of the model. We investigate how fine-tuning towards downstream NLP tasks impacts the learned linguistic knowledge. We carry out a study across popular pre-trained models BERT, RoBERTa and XLNet using layer and neuronlevel diagnostic classifiers. We found that for some GLUE tasks, the network relies on the core linguistic information and preserve it deeper in the network, while for others it forgets. Linguistic information is distributed in the pre-trained language models but becomes localized to the lower layers post-fine-tuning, reserving higher layers for the task specific knowledge. The pattern varies across architectures, with BERT retaining linguistic information relatively deeper in the network compared to RoBERTa and XLNet, where it is predominantly delegated to the lower layers.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'fcd212bc6538750197618e09ce605336ffa4e0b1',\n",
       "  'title': 'Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning: A Faroese Case Study',\n",
       "  'authorId': '31333199',\n",
       "  'authorName': 'James Barry',\n",
       "  'abstract': 'Cross-lingual dependency parsing involves transferring syntactic knowledge from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using Faroese as the target language, we compare two approaches using annotation projection: first, projecting from multiple monolingual source models; second, projecting from a single polyglot model which is trained on the combination of all source languages. Furthermore, we reproduce multi-source projection (Tyers et al., 2018), in which dependency trees of multiple sources are combined. Finally, we apply multi-treebank modelling to the projected treebanks, in addition to or alternatively to polyglot modelling on the source side. We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '40c98b4fcd589dc4ff8e507fc7f88bc71329fd94',\n",
       "  'title': 'Two Kinds of Metonymy',\n",
       "  'authorId': '145482266',\n",
       "  'authorName': 'D. Stallard',\n",
       "  'abstract': 'We propose a distinction between two kinds of metonymy: \"referential\" metonymy, in which the referent of an NP is shifted, and \"predicative\" metonymy, in which the referent of the NP is unchanged and the argument place of the predicate is shifted instead. Examples are, respectively, \"The hamburger is waiting for his check\" and \"Which airlines fly from Boston to Denver\". We also show that complications arise for both types of metonymy when multiple coercing predicates are considered. Finally, we present implemented algorithms handling these complexities that generate both types of metonymic reading, as well as criteria for choosing one type of metonymic reading over another.',\n",
       "  'year': 1993,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '4498d8229038fc5217c05a6a762bf6d0d1aa97bb',\n",
       "  'title': 'Towards Qualitative Word Embeddings Evaluation: Measuring Neighbors Variation',\n",
       "  'authorId': '46177105',\n",
       "  'authorName': 'Bénédicte Pierrejean',\n",
       "  'abstract': 'We propose a method to study the variation lying between different word embeddings models trained with different parameters. We explore the variation between models trained with only one varying parameter by observing the distributional neighbors variation and show how changing only one parameter can have a massive impact on a given semantic space. We show that the variation is not affecting all words of the semantic space equally. Variation is influenced by parameters such as setting a parameter to its minimum or maximum value but it also depends on the corpus intrinsic features such as the frequency of a word. We identify semantic classes of words remaining stable across the models trained and specific words having high variation.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '033781530f6b454aebe9786d105abd673f463dde',\n",
       "  'title': 'Feature Forest Models for Probabilistic HPSG Parsing',\n",
       "  'authorId': '1768065',\n",
       "  'authorName': 'Yusuke Miyao',\n",
       "  'abstract': 'Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures. This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into sub-structures under the assumption of statistical independence among sub-structures. For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules. These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures. This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures. The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests. Feature forests are generic data structures that represent ambiguous trees in a packed forest structure. Feature forest models are maximum entropy models defined over feature forests. A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of any data structures is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate-argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.',\n",
       "  'year': 2008,\n",
       "  'venue': 'Computational Linguistics'},\n",
       " {'paperId': '41c8bcfb6301d2bd15d19d95c4b30c886f997486',\n",
       "  'title': 'Winter is here: Summarizing Twitter Streams related to Pre-Scheduled Events',\n",
       "  'authorId': '25062613',\n",
       "  'authorName': 'Anietie U Andy',\n",
       "  'abstract': 'Pre-scheduled events, such as TV shows and sports games, usually garner considerable attention from the public. Twitter captures large volumes of discussions and messages related to these events, in real-time. Twitter streams related to pre-scheduled events are characterized by the following: (1) spikes in the volume of published tweets reflect the highlights of the event and (2) some of the published tweets make reference to the characters involved in the event, in the context in which they are currently portrayed in a subevent. In this paper, we take advantage of these characteristics to identify the highlights of pre-scheduled events from tweet streams and we demonstrate a method to summarize these highlights. We evaluate our algorithm on tweets collected around 2 episodes of a popular TV show, Game of Thrones, Season 7.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Second Workshop on Storytelling'},\n",
       " {'paperId': '7dac454392a3ad3d6aa43ea5e4b987fceb9cf00d',\n",
       "  'title': 'Kathy McKeown Interviews Bonnie Webber',\n",
       "  'authorId': '1736049',\n",
       "  'authorName': 'B. Webber',\n",
       "  'abstract': 'Abstract Because the 2020 ACL Lifetime Achievement Award presentation could not be done in person, we replaced the usual LTA talk with an interview between Professor Kathy McKeown (Columbia University) and the recipient, Bonnie Webber. The following is an edited version of the interview, with added citations.',\n",
       "  'year': 2021,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': '7495f9a7f17a2fb63b7776650efe453405aa8933',\n",
       "  'title': 'Hierarchical Pre-training for Sequence Labelling in Spoken Dialog',\n",
       "  'authorId': '1500370105',\n",
       "  'authorName': 'E. Chapuis',\n",
       "  'abstract': 'Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new benchmark we call Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE benchmark (SILICONE). SILICONE is model-agnostic and contains 10 different datasets of various sizes. We obtain our representations with a hierarchical encoder based on transformer architectures, for which we extend two well-known pre-training objectives. Pre-training is performed on OpenSubtitles: a large corpus of spoken dialog containing over 2.3 billion of tokens. We demonstrate how hierarchical encoders achieve competitive results with consistently fewer parameters compared to state-of-the-art models and we show their importance for both pre-training and fine-tuning.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '9163363956bc728c197a503a8bfec523ff4790e9',\n",
       "  'title': 'From TimeLines to StoryLines: A preliminary proposal for evaluating narratives',\n",
       "  'authorId': '2115842',\n",
       "  'authorName': 'Egoitz Laparra',\n",
       "  'abstract': 'We formulate a proposal that covers a new definition of StoryLines based on the shared data provided by the NewsStory workshop. We re-use the SemEval 2015 Task 4: Timelines dataset to provide a gold-standard dataset and an evaluation measure for evaluating StoryLines extraction systems. We also present a system to explore the feasibility of capturing StoryLines automatically. Finally, based on our initial findings, we also discuss some simple changes that will improve the existing annotations to complete our initial Story-',\n",
       "  'year': 2015,\n",
       "  'venue': ''},\n",
       " {'paperId': 'f5ba443a4fc17e1b3bb8fcfedcbace6f3dd6dd68',\n",
       "  'title': 'Wino-X: Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution',\n",
       "  'authorId': '51889641',\n",
       "  'authorName': 'Denis Emelin',\n",
       "  'abstract': 'Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to English, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that requires commonsense knowledge and whether multilingual language models (MLLMs) are capable of CSR across multiple languages. Our findings show Wino-X to be exceptionally challenging for NMT systems that are prone to undesirable biases and unable to detect disambiguating information. We quantify biases using established statistical methods and define ways to address both of these issues. We furthermore present evidence of active cross-lingual knowledge transfer in MLLMs, whereby fine-tuning models on English schemas yields CSR improvements in other languages.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'c6460bf5763355e5e4cadea256825f575896b2a3',\n",
       "  'title': 'Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension',\n",
       "  'authorId': '8367832',\n",
       "  'authorName': 'Minghao Hu',\n",
       "  'abstract': 'This paper considers the reading comprehension task in which multiple documents are given as input. Prior work has shown that a pipeline of retriever, reader, and reranker can improve the overall performance. However, the pipeline system is inefficient since the input is re-encoded within each module, and is unable to leverage upstream components to help downstream training. In this work, we present RE^3QA, a unified question answering model that combines context retrieving, reading comprehension, and answer reranking to predict the final answer. Unlike previous pipelined approaches, RE^3QA shares contextualized text representation across different components, and is carefully designed to use high-quality upstream outputs (e.g., retrieved context or candidate answers) for directly supervising downstream modules (e.g., the reader or the reranker). As a result, the whole network can be trained end-to-end to avoid the context inconsistency problem. Experiments show that our model outperforms the pipelined baseline and achieves state-of-the-art results on two versions of TriviaQA and two variants of SQuAD.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '34b274329317b8e7aa8f8af60f3aa22ed7d87cbb',\n",
       "  'title': 'How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds',\n",
       "  'authorId': '19179135',\n",
       "  'authorName': 'Prithviraj Ammanabrolu',\n",
       "  'abstract': 'We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text-game—with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '7dc43f7339e636ba49891732e3f20b3b377dfd78',\n",
       "  'title': 'Decoupling Adversarial Training for Fair NLP',\n",
       "  'authorId': '2110982198',\n",
       "  'authorName': 'Xudong Han',\n",
       "  'abstract': 'Adversarial debiasing can help to learn fairer models. Previous work has assumed that both main task labels and protected attributes are available in the dataset. However, protected labels are often unavailable, or only available in limited numbers. In this paper, we propose a training strategy which needs only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the inand crossdomain effectiveness of our method through a range of experiments.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'ee941de3addf0e93851f0b652d731e852f032f21',\n",
       "  'title': 'German Dialect Identification in Interview Transcriptions',\n",
       "  'authorId': '2854981',\n",
       "  'authorName': 'S. Malmasi',\n",
       "  'abstract': 'This paper presents three systems submitted to the German Dialect Identification (GDI) task at the VarDial Evaluation Campaign 2017. The task consists of training models to identify the dialect of Swiss-German speech transcripts. The dialects included in the GDI dataset are Basel, Bern, Lucerne, and Zurich. The three systems we submitted are based on: a plurality ensemble, a mean probability ensemble, and a meta-classifier trained on character and word n-grams. The best results were obtained by the meta-classifier achieving 68.1% accuracy and 66.2% F1-score, ranking first among the 10 teams which participated in the GDI shared task.',\n",
       "  'year': 2017,\n",
       "  'venue': 'VarDial'},\n",
       " {'paperId': 'c64c6549a5f06684b5c78b757bdc5e4723dc6cd3',\n",
       "  'title': 'What Makes You Stressed? Finding Reasons From Tweets',\n",
       "  'authorId': '40974563',\n",
       "  'authorName': 'Reshmi Gopalakrishna Pillai',\n",
       "  'abstract': 'Detecting stress from social media gives a non-intrusive and inexpensive alternative to traditional tools such as questionnaires or physiological sensors for monitoring mental state of individuals. This paper introduces a novel framework for finding reasons for stress from tweets, analyzing multiple categories for the first time. Three word-vector based methods are evaluated on collections of tweets about politics or airlines and are found to be more accurate than standard machine learning algorithms.',\n",
       "  'year': 2018,\n",
       "  'venue': 'WASSA@EMNLP'},\n",
       " {'paperId': '1dfa1288bc758c39612362d4b2b831f2d2c83589',\n",
       "  'title': 'Controllable Natural Language Generation with Contrastive Prefixes',\n",
       "  'authorId': '144130537',\n",
       "  'authorName': 'Jing Qian',\n",
       "  'abstract': 'To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.',\n",
       "  'year': 2022,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'b960415906d3a46979a819b8f6530f8253027263',\n",
       "  'title': 'WantWords: An Open-source Online Reverse Dictionary System',\n",
       "  'authorId': '51466208',\n",
       "  'authorName': 'Fanchao Qi',\n",
       "  'abstract': 'A reverse dictionary takes descriptions of words as input and outputs words semantically matching the input descriptions. Reverse dictionaries have great practical value such as solving the tip-of-the-tongue problem and helping new language learners. There have been some online reverse dictionary systems, but they support English reverse dictionary queries only and their performance is far from perfect. In this paper, we present a new open-source online reverse dictionary system named WantWords (https://wantwords.thunlp.org/). It not only significantly outperforms other reverse dictionary systems on English reverse dictionary performance, but also supports Chinese and English-Chinese as well as Chinese-English cross-lingual reverse dictionary queries for the first time. Moreover, it has user-friendly front-end design which can help users find the words they need quickly and easily. All the code and data are available at https://github.com/thunlp/WantWords.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '88c3f221a6fc8aff014268b0efb5ff119ab40906',\n",
       "  'title': 'Dataset Analysis and Augmentation for Emoji-Sensitive Irony Detection',\n",
       "  'authorId': '31998283',\n",
       "  'authorName': 'Shirley Anugrah Hayati',\n",
       "  'abstract': 'Irony detection is an important task with applications in identification of online abuse and harassment. With the ubiquitous use of non-verbal cues such as emojis in social media, in this work we aim to study the role of these structures in irony detection. Since the existing irony detection datasets have <10% ironic tweets with emoji, classifiers trained on them are insensitive to emojis. We propose an automated pipeline for creating a more balanced dataset.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)'},\n",
       " {'paperId': '5e6965e5845cb12341323f6fdf1013b7834f8dee',\n",
       "  'title': 'Complex Word Identification Using Character n-grams',\n",
       "  'authorId': '2487502',\n",
       "  'authorName': 'Maja Popovic',\n",
       "  'abstract': 'This paper investigates the use of character n-gram frequencies for identifying complex words in English, German and Spanish texts. The approach is based on the assumption that complex words are likely to contain different character sequences than simple words. The multinomial Naive Bayes classifier was used with n-grams of different lengths as features, and the best results were obtained for the combination of 2-grams and 4-grams. This variant was submitted to the Complex Word Identification Shared Task 2018 for all texts and achieved F-scores between 70% and 83%. The system was ranked in the middle range for all English texts, as third of fourteen submissions for German, and as tenth of seventeen submissions for Spanish. The method is not very convenient for the cross-language task, achieving only 59% on the French text.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': '35d26980dd30603bc9deb9f0ca187bcdc9eb9c65',\n",
       "  'title': 'UNIBA: Combining Distributional Semantic Models and Sense Distribution for Multilingual All-Words Sense Disambiguation and Entity Linking',\n",
       "  'authorId': '1731651',\n",
       "  'authorName': 'Pierpaolo Basile',\n",
       "  'abstract': 'This paper describes the participation of the UNIBA team in the Task 13 of SemEval-2015 about Multilingual All-Words Sense Disambiguation and Entity Linking. We propose an algorithm able to disambiguate both word senses and named entities by combining the simple Lesk approach with information coming from both a distributional semantic model and usage frequency of meanings. The results for both English and Italian show satisfactory performance.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '43003de1bdf12e14e917c98807ad0ab244caa923',\n",
       "  'title': 'Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation',\n",
       "  'authorId': '145756130',\n",
       "  'authorName': 'Chen Zhao',\n",
       "  'abstract': 'Open-domain question answering answers a question based on evidence retrieved from a large corpus. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and methods that rely on them cannot transfer to the more common setting, where only question–answer pairs are available. This paper investigates whether models can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the model to learn the most likely evidence. Without using any evidence labels, DistDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https://github.com/henryzhao5852/DistDR.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '296aaba05c9a790f48b501d14f82c562d202c2e5',\n",
       "  'title': 'Pairwise Supervised Contrastive Learning of Sentence Representations',\n",
       "  'authorId': '2358258',\n",
       "  'authorName': 'Dejiao Zhang',\n",
       "  'abstract': 'Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various downstream tasks that involve understanding sentence semantics at different granularities. We outperform the previous state-of-the-art method with 10%–13% averaged improvement on eight clustering tasks, and 5%–6% averaged improvement on seven semantic textual similarity (STS) tasks.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '33e486f1dc63244ae1bdd83be89379b7c8c846a2',\n",
       "  'title': '“Wikily” Supervised Neural Translation Tailored to Cross-Lingual Tasks',\n",
       "  'authorId': '2653340',\n",
       "  'authorName': 'Mohammad Sadegh Rasooli',\n",
       "  'abstract': 'We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our wikily translation models to unsupervised image captioning, and cross-lingual dependency parser transfer. In image captioning, we train a multi-tasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a wikily translation of the English captioning data. Our captioning results on Arabic are slightly better than that of its supervised model. In dependency parsing, we translate a large amount of monolingual text, and use it as an artificial training data in an annotation projection framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a1e445676097e404286fa274f1b1da249d2a48c2',\n",
       "  'title': 'Grammar Specialization Through Entropy Thresholds',\n",
       "  'authorId': '2403128',\n",
       "  'authorName': 'C. Samuelsson',\n",
       "  'abstract': 'Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees. This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage. Previously, it has been necessary to specify the tree-cutting criteria (or operationality criteria) manually; here they are derived automatically from the training set and the desired coverage of the specialized grammar. This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values.',\n",
       "  'year': 1994,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '777d399aa2753464ca1cfbac273ee6e6780f2837',\n",
       "  'title': 'An Incremental Turn-Taking Model with Active System Barge-in for Spoken Dialog Systems',\n",
       "  'authorId': '8200875',\n",
       "  'authorName': 'Tiancheng Zhao',\n",
       "  'abstract': 'This paper deals with an incremental turntaking model that provides a novel solution for end-of-turn detection. It includes a flexible framework that enables active system barge-in. In order to accomplish this, a systematic procedure of teaching a dialog system to produce meaningful system barge-in is presented. This procedure improves system robustness and success rate. It includes constructing cost models and learning optimal policy using reinforcement learning. Results show that our model reduces false cut-in rate by 37.1% and response delay by 32.5% compared to the baseline system. Also the learned system barge-in strategy yields a 27.7% increase in average reward from user responses.',\n",
       "  'year': 2015,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': 'b6bffec600168834ae031491796e20b2feb1702f',\n",
       "  'title': 'What Motivates You? Benchmarking Automatic Detection of Basic Needs from Short Posts',\n",
       "  'authorId': '1952894',\n",
       "  'authorName': 'Sanja Štajner',\n",
       "  'abstract': 'According to the self-determination theory, the levels of satisfaction of three basic needs (competence, autonomy and relatedness) have implications on people’s everyday life and career. We benchmark the novel task of automatically detecting those needs on short posts in English, by modelling it as a ternary classification task, and as three binary classification tasks. A detailed manual analysis shows that the latter has advantages in the real-world scenario, and that our best models achieve similar performances as a trained human annotator.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'bb1087e8dee2039f773c381a3449a1c382482da6',\n",
       "  'title': 'Question Answering through Transfer Learning from Large Fine-grained Supervision Data',\n",
       "  'authorId': '48872685',\n",
       "  'authorName': 'Sewon Min',\n",
       "  'abstract': 'We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'f56cda7ee6b3cfa427d045b6cc754ec68349c511',\n",
       "  'title': 'Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts',\n",
       "  'authorId': '2063935838',\n",
       "  'authorName': 'Ashutosh Baheti',\n",
       "  'abstract': 'Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '90aa6db47425277d1488ff18e214bddd9a733d76',\n",
       "  'title': 'LSIS at SemEval-2016 Task 7: Using Web Search Engines for English and Arabic Unsupervised Sentiment Intensity Prediction',\n",
       "  'authorId': '3442310',\n",
       "  'authorName': 'Amal Htait',\n",
       "  'abstract': \"In this paper, we present our contribution in SemEval2016 task7 1 : Determining Sentiment Intensity of English and Arabic Phrases, where we use web search engines for English and Arabic unsupervised sentiment intensity prediction. Our work is based, first, on a group of classic sentiment lexicons (e.g. Sen-timent140 Lexicon, SentiWordNet). Second, on web search engines' ability to find the co-occurrence of sentences with predefined negative and positive words. The use of web search engines (e.g. Google Search API) enhance the results on phrases built from opposite polarity terms.\",\n",
       "  'year': 2016,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'b561e606a74885a1c0e768874a6436e5c995eea4',\n",
       "  'title': 'A Nil-Aware Answer Extraction Framework for Question Answering',\n",
       "  'authorId': '2103994667',\n",
       "  'authorName': 'Souvik Kundu',\n",
       "  'abstract': 'Recently, there has been a surge of interest in reading comprehension-based (RC) question answering (QA). However, current approaches suffer from an impractical assumption that every question has a valid answer in the associated passage. A practical QA system must possess the ability to determine whether a valid answer exists in a given text passage. In this paper, we focus on developing QA systems that can extract an answer for a question if and only if the associated passage contains an answer. If the associated passage does not contain any valid answer, the QA system will correctly return Nil. We propose a novel nil-aware answer span extraction framework that is capable of returning Nil or a text span from the associated passage as an answer in a single step. We show that our proposed framework can be easily integrated with several recently proposed QA models developed for reading comprehension and can be trained in an end-to-end fashion. Our proposed nil-aware answer extraction neural network decomposes pieces of evidence into relevant and irrelevant parts and then combines them to infer the existence of any answer. Experiments on the NewsQA dataset show that the integration of our proposed framework significantly outperforms several strong baseline systems that use pipeline or threshold-based approaches.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'ad7eac156b117169836264eea52762f07054dc57',\n",
       "  'title': 'User Modeling in Language Learning with Macaronic Texts',\n",
       "  'authorId': '3286437',\n",
       "  'authorName': 'Adithya Renduchintala',\n",
       "  'abstract': 'Foreign language learners can acquire new vocabulary by using cognate and context clues when reading. To measure such incidental comprehension, we devise an experimental framework that involves reading mixed-language “macaronic” sentences. Using data collected via Amazon Mechanical Turk, we train a graphical model to simulate a human subject’s comprehension of foreign words, based on cognate clues (edit distance to an English word), context clues (pointwise mutual in-formation), and prior exposure. Our model does a reasonable job at predicting which words a user will be able to understand, which should facilitate the automatic construction of comprehensible text for personalized foreign language education.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ab72bccf6f3981537389510ecc609109e79595c3',\n",
       "  'title': 'Disentangled Sequence to Sequence Learning for Compositional Generalization',\n",
       "  'authorId': '2115239329',\n",
       "  'authorName': 'Hao Zheng',\n",
       "  'abstract': 'There is mounting evidence that existing neural network models, in particular the very popular sequence-to-sequence architecture, struggle to systematically generalize to unseen compositions of seen components. We demonstrate that one of the reasons hindering compositional generalization relates to representations being entangled. We propose an extension to sequence-to-sequence models which encourage disentanglement by adaptively re-encoding (at each time step) the source input. Specifically, we condition the source representations on the newly decoded target context which makes it easier for the encoder to exploit specialized information for each prediction rather than capturing it all in a single forward pass. Experimental results on semantic parsing and machine translation empirically show that our proposal delivers more disentangled representations and better generalization.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c9b54f0118fa30990d0ecc29a108a08f3fecf322',\n",
       "  'title': 'HashCount at SemEval-2018 Task 3: Concatenative Featurization of Tweet and Hashtags for Irony Detection',\n",
       "  'authorId': '153091253',\n",
       "  'authorName': 'Won Ik Cho',\n",
       "  'abstract': 'This paper proposes a novel feature extraction process for SemEval task 3: Irony detection in English tweets. The proposed system incorporates a concatenative featurization of tweet and hashtags, which helps distinguishing between the irony-related and the other components. The system embeds tweets into a vector sequence with widely used pretrained word vectors, partially using a character embedding for the words that are out of vocabulary. Identification was performed with BiLSTM and CNN classifiers, achieving F1 score of 0.5939 (23/42) and 0.3925 (10/28) each for the binary and the multi-class case, respectively. The reliability of the proposed scheme was verified by analyzing the Gold test data, which demonstrates how hashtags can be taken into account when identifying various types of irony.',\n",
       "  'year': 2018,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '2f2214bdf77097b774bb53e2daa965f3baffc790',\n",
       "  'title': 'EXPLORER: A Natural Language Processing System for Oil Exploration',\n",
       "  'authorId': '1925215',\n",
       "  'authorName': 'W. Lehnert',\n",
       "  'abstract': \"Abstract : EXPLORER (Lehnert and Shwartz, 1982; Shwartz 1982) is a non-fragile, 'hands-on' language analysis system that allows oil explorationists with no knowledge of computers or computer programming to create customized maps. Users in Tulsa, Denver, and New Orleans currently have dial-up access to a DEC-20 where EXPLORER is iplemented in TLISP. A user converses with EXPLORER about a desired map until both parties have agreed on an adequate and unambiguous set of specifications. Another phone link then carries EXPLORER's output to an IBM 3033 which runs database retrieval routines on commercial well data. When all the information has been secured from well data, a graphics system takes over to perform the actual map generation. EXPLORER is currently undergoing evaluation, and it is targetted for a 1983 installation in all regional offices of a major oil company for restricted use by geologists and geophysicists.\",\n",
       "  'year': 1983,\n",
       "  'venue': 'ANLP'},\n",
       " {'paperId': 'ec3d5bdfda2c5c841c2481f5da123b2c086e6f5c',\n",
       "  'title': 'Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking',\n",
       "  'authorId': '1382048113',\n",
       "  'authorName': 'Giovanni Campagna',\n",
       "  'abstract': 'Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'cf9830bf8d2babc3c32d192cdccb27aeaf46a048',\n",
       "  'title': 'The Role of Context Types and Dimensionality in Learning Word Embeddings',\n",
       "  'authorId': '2298649',\n",
       "  'authorName': 'Oren Melamud',\n",
       "  'abstract': 'We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words.',\n",
       "  'year': 2016,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '5626e1db3d4fa8f8de79b604ce9fb8eb96a75883',\n",
       "  'title': 'Improving Compositional Generalization with Latent Structure and Data Augmentation',\n",
       "  'authorId': '1742081895',\n",
       "  'authorName': 'Linlu Qiu',\n",
       "  'abstract': 'Generic unstructured neural networks have been shown to struggle on out-of-distribution compositional generalization. Compositional data augmentation via example recombination has transferred some prior knowledge about compositionality to such black-box neural models for several semantic parsing tasks, but this often required task-specific engineering or provided limited gains. We present a more powerful data recombination method using a model called Compositional Structure Learner (CSL). CSL is a generative model with a quasi-synchronous context-free grammar backbone, which we induce from the training data. We sample recombined examples from CSL and add them to the fine-tuning data of a pre-trained sequence-to-sequence model (T5). This procedure effectively transfers most of CSL’s compositional bias to T5 for diagnostic tasks, and results in a model even stronger than a T5-CSL ensemble on two real world compositional generalization tasks. This results in new state-of-the-art performance for these challenging semantic parsing tasks requiring generalization to both natural language variation and novel compositions of elements.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '69278ea9200ec76d6da51058afcdd2ee803f75c4',\n",
       "  'title': 'Can Latent Alignments Improve Autoregressive Machine Translation?',\n",
       "  'authorId': '2052303581',\n",
       "  'authorName': 'Adi Haviv',\n",
       "  'abstract': 'Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '9111bd79028068f8fbd8a43bb464ae007d3df838',\n",
       "  'title': 'Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization',\n",
       "  'authorId': '144797264',\n",
       "  'authorName': 'J. Clark',\n",
       "  'abstract': 'Linear models, which support efficient learning and inference, are the workhorses of statistical machine translation; however, linear decision rules are less attractive from a modeling perspective. In this work, we introduce a technique for learning arbitrary, rule-local, non-linear feature transforms that improve model expressivity, but do not sacrifice the efficient inference and learning associated with linear models. To demonstrate the value of our technique, we discard the customary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts. We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers.',\n",
       "  'year': 2014,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '584feecd19dbc910a0760aa083f747c52c851aec',\n",
       "  'title': 'Learning Structures of Negations from Flat Annotations',\n",
       "  'authorId': '3331141',\n",
       "  'authorName': 'Vinodkumar Prabhakaran',\n",
       "  'abstract': 'We propose a novel method to learn negation expressions in a specialized (medical) domain. In our corpus, negations are annotated as ‘flat’ text spans. This allows for some infelicities in the mark-up of the ground truth, making it less than perfectly aligned with the underlying syntactic structure. Nonetheless, the negations thus captured are correct in intent, and thus potentially valuable. We succeed in training a model for detecting the negated predicates corresponding to the annotated negations, by re-mapping the corpus to anchor its ‘flat’ annotation spans into the predicate argument structure. Our key idea—re-mapping the negation instance spans to more uniform syntactic nodes—makes it possible to re-frame the learning task as a simpler one, and to leverage an imperfect resource in a way which enables us to learn a high performance model. We achieve high accuracy for negation detection overall, 87%. Our re-mapping scheme can be constructively applied to existing flatly annotated resources for other tasks where syntactic context is vital.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'c185e65be906cf24c56a2312f45db8cc994ee4ef',\n",
       "  'title': 'A Dual-generator Network for Text Style Transfer Applications',\n",
       "  'authorId': '29432207',\n",
       "  'authorName': 'Xiao Li',\n",
       "  'abstract': 'We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6fd5b1f6350a45fa00cc87372815af01eb9746ff',\n",
       "  'title': 'Graph- and surface-level sentence chunking',\n",
       "  'authorId': '3449419',\n",
       "  'authorName': 'E. Muszyńska',\n",
       "  'abstract': 'The computing cost of many NLP tasks increases faster than linearly with the length of the representation of a sentence. For parsing the representation is tokens, while for operations on syntax and semantics it will be more complex. In this paper we propose a new task of sentence chunking: splitting sentence representations into coherent substructures. Its aim is to make further processing of long sentences more tractable. We investigate this idea experimentally using the Dependency Minimal Recursion Semantics (DMRS) representation.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'fb42361ee105c8fbd1beee5d0d2ec24603945ba3',\n",
       "  'title': 'A Wind of Change: Detecting and Evaluating Lexical Semantic Change across Times and Domains',\n",
       "  'authorId': '3449121',\n",
       "  'authorName': 'Dominik Schlechtweg',\n",
       "  'abstract': 'We perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task: semantic sense changes across time, and semantic sense changes across domains. Our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change, by bringing together and extending benchmark models on a common state-of-the-art evaluation task. In addition, we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd7d24a27f4e6ec6cdb948f4d926e0472c440e052',\n",
       "  'title': 'Referential Translation Machines for Predicting Translation Quality and Related Statistics',\n",
       "  'authorId': '2237884',\n",
       "  'authorName': 'Ergun Biçici',\n",
       "  'abstract': 'We use referential translation machines (RTMs) for predicting translation performance. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. We improve our RTM models with the',\n",
       "  'year': 2015,\n",
       "  'venue': 'WMT@EMNLP'},\n",
       " {'paperId': '2f541b24f69798e255e04229e8dc78f4d1873fd7',\n",
       "  'title': 'Improving Text-to-SQL Evaluation Methodology',\n",
       "  'authorId': '1403432982',\n",
       "  'authorName': 'Catherine Finegan-Dollak',\n",
       "  'abstract': 'To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '956eb2cf57b35c5f8ce8baa12f87c17adf25bff5',\n",
       "  'title': 'Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure with Supervised Learning',\n",
       "  'authorId': '2099597',\n",
       "  'authorName': 'D. Wang',\n",
       "  'abstract': 'We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language’s POS sequence (hand-engineered or neural features) that correlate with the language’s deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin.',\n",
       "  'year': 2017,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': 'bdd4f05c853bb66e8b01b70ac009aa4242c1c4a4',\n",
       "  'title': 'Zero-shot Text Classification via Reinforced Self-training',\n",
       "  'authorId': '2114132405',\n",
       "  'authorName': 'Zhiquan Ye',\n",
       "  'abstract': 'Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity. In this situation, transferring from seen classes to unseen classes is extremely hard. To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data. Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets. We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection. Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '0fa6fa53d0725442a271b8e8b28c15e006bc9308',\n",
       "  'title': 'PINGAN Omini-Sinitic at SemEval-2021 Task 4:Reading Comprehension of Abstract Meaning',\n",
       "  'authorId': '2114719042',\n",
       "  'authorName': 'Boyuan Zheng',\n",
       "  'abstract': 'This paper describes the winning system for subtask 2 and the second-placed system for subtask 1 in SemEval 2021 Task 4: ReadingComprehension of Abstract Meaning. We propose to use pre-trianed Electra discriminator to choose the best abstract word from five candidates. An upper attention and auto denoising mechanism is introduced to process the long sequences. The experiment results demonstrate that this contribution greatly facilitatesthe contextual language modeling in reading comprehension task. The ablation study is also conducted to show the validity of our proposed methods.',\n",
       "  'year': 2021,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': 'fe4fb230a362906c5320651205c1dee3864c022b',\n",
       "  'title': 'Interactions between Narrative Schemas and Document Categories',\n",
       "  'authorId': '48531944',\n",
       "  'authorName': 'D. Simonson',\n",
       "  'abstract': 'The unsupervised extraction of narrative schemas—sets of events with associated argument chains—has been explored and evaluated from many angles (Chambers and Jurafsky, 2009; Jans et al. 2012; Balasubramanian et al., 2013; Pichotta and Mooney 2014). While the extraction process and evaluation of the products has been well-researched and debated, little insight has been garnered on properties of narrative schemas themselves. We examine how well extracted narrative schemas align with existing document categories using a novel procedure for retrieving candidate category alignments. This was tested against alternative baseline alignment procedures that disregard some of the complex information the schemas contain. We find that a classifier built with all available information in a schema is more precise than a classifier built with simpler subcomponents. Coreference information plays an crucial role in schematic knowledge.',\n",
       "  'year': 2015,\n",
       "  'venue': ''},\n",
       " {'paperId': '9df13529b64a85cc28f8befc4ea097fa9aaa9251',\n",
       "  'title': 'Neural-based Natural Language Generation in Dialogue using RNN Encoder-Decoder with Semantic Aggregation',\n",
       "  'authorId': '49701439',\n",
       "  'authorName': 'Van-Khanh Tran',\n",
       "  'abstract': 'Natural language generation (NLG) is an important component in spoken dialogue systems. This paper presents a model called Encoder-Aggregator-Decoder which is an extension of an Recurrent Neural Network based Encoder-Decoder architecture. The proposed Semantic Aggregator consists of two components: an Aligner and a Refiner. The Aligner is a conventional attention calculated over the encoded input information, while the Refiner is another attention or gating mechanism stacked over the attentive Aligner in order to further select and aggregate the semantic elements. The proposed model can be jointly trained both sentence planning and surface realization to produce natural language utterances. The model was extensively assessed on four different NLG domains, in which the experimental results showed that the proposed generator consistently outperforms the previous methods on all the NLG domains.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': 'c41516420ddbd0f29e010ca259a74c1fc2da0466',\n",
       "  'title': 'What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties',\n",
       "  'authorId': '2480903',\n",
       "  'authorName': 'Alexis Conneau',\n",
       "  'abstract': 'Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '029277c5b3442af237253f998d85b4af347bdf53',\n",
       "  'title': 'What Do Language Representations Really Represent?',\n",
       "  'authorId': '3336895',\n",
       "  'authorName': 'Johannes Bjerva',\n",
       "  'abstract': 'A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just as it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, whereas genetic relationships—a convenient benchmark used for evaluation in previous work—appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Computational Linguistics'},\n",
       " {'paperId': 'e9fb6fc67f83ff3405fca08f83f069c026e4f342',\n",
       "  'title': 'Simple, Robust and (almost) Unsupervised Generation of Polarity Lexicons for Multiple Languages',\n",
       "  'authorId': '2247768',\n",
       "  'authorName': 'Iñaki San Vicente',\n",
       "  'abstract': 'This paper presents a simple, robust and (almost) unsupervised dictionary-based method, qwn-ppv (Q-WordNet as Personalized PageRanking Vector) to automatically generate polarity lexicons. We show that qwn-ppv outperforms other automatically generated lexicons for the four extrinsic evaluations presented here. It also shows very competitive and robust results with respect to manually annotated ones. Results suggest that no single lexicon is best for every task and dataset and that the intrinsic evaluation of polarity lexicons is not a good performance indicator on a Sentiment Analysis task. The qwn-ppv method allows to easily create quality polarity lexicons whenever no domain-based annotated corpora are available for a given language.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '8f3e1bbbdc1190ae6320c0520f539337a5ca5927',\n",
       "  'title': 'Improving Chinese Tokenization With Linguistic Filters On Statistical Lexical Acquisition',\n",
       "  'authorId': '2390150',\n",
       "  'authorName': 'Dekai Wu',\n",
       "  'abstract': 'The first step in Chinese NLP is to tokenize or segment character sequences into words, since the text contains no word delimiters. Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date have been based on dictionary lookup (e.g., Chang & Chen 1993; Chiang et al. 1992; Lin et al. 1993; Wu & Tseng 1993; Sproat et al. 1994).We present empirical evidence for four points concerning tokenization of Chinese text: (1) More rigorous \"blind\" evaluation methodology is needed to avoid inflated accuracy measurements; we introduce the nk-blind method. (2) The extent of the unknown-word problem is far more serious than generally thought, when tokenizing unrestricted texts in realistic domains. (3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32.0%. (4) When augmenting the lexicon, linguistic constraints can provide simple inexpensive filters yielding significantly better precision, reducing error rates as much as 49.4%.',\n",
       "  'year': 1994,\n",
       "  'venue': 'ANLP'},\n",
       " {'paperId': '7eca2934825184908e57619b354c327a6eaaf7cc',\n",
       "  'title': 'Topic Modeling with Wasserstein Autoencoders',\n",
       "  'authorId': '144647318',\n",
       "  'authorName': 'Feng Nan',\n",
       "  'abstract': 'We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3e2e426de00a76fcbb03c4e0fb6492e0ff475919',\n",
       "  'title': 'Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task',\n",
       "  'authorId': '1814319',\n",
       "  'authorName': 'Omid Bakhshandeh',\n",
       "  'abstract': 'Understanding common entities and their attributes is a primary requirement for any system that comprehends natural language. In order to enable learning about common entities, we introduce a novel machine comprehension task, GuessTwo: given a short paragraph comparing different aspects of two real-world semantically-similar entities, a system should guess what those entities are. Accomplishing this task requires deep language understanding which enables inference, connecting each comparison paragraph to different levels of knowledge about world entities and their attributes. So far we have crowdsourced a dataset of more than 14K comparison paragraphs comparing entities from a variety of categories such as fruits and animals. We have designed two schemes for evaluation: open-ended, and binary-choice prediction. For benchmarking further progress in the task, we have collected a set of paragraphs as the test set on which human can accomplish the task with an accuracy of 94.2% on open-ended prediction. We have implemented various models for tackling the task, ranging from semantic-driven to neural models. The semantic-driven approach outperforms the neural models, however, the results indicate that the task is very challenging across the models.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '760df4fcde1c0207b87bea0bd9d30416612b548a',\n",
       "  'title': 'Text simplification using synchronous dependency grammars: Generalising automatically harvested rules',\n",
       "  'authorId': '2071811',\n",
       "  'authorName': 'Angrosh Mandya',\n",
       "  'abstract': 'We present an approach to text simplification based on synchronous dependency grammars. Our main contributions in this work are (a) a study of how automatically derived lexical simplification rules can be generalised to enable their application in new contexts without introducing errors, and (b) an evaluation of our hybrid system that combines a large set of automatically acquired rules with a small set of hand-crafted rules for common syntactic simplification. Our evaluation shows significant improvements over the state of the art, with scores comparable to human simplifications.',\n",
       "  'year': 2014,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': '986be91a820f32602b6a68794460f63ea3e7f5a9',\n",
       "  'title': 'Semi-supervised Domain Adaptation for Dependency Parsing with Dynamic Matching Network',\n",
       "  'authorId': '2154293431',\n",
       "  'authorName': 'Ying Li',\n",
       "  'abstract': 'Supervised parsing models have achieved impressive results on in-domain texts. However, their performances drop drastically on out-of-domain texts due to the data distribution shift. The shared-private model has shown its promising advantages for alleviating this problem via feature separation, whereas prior works pay more attention to enhance shared features but neglect the in-depth relevance of specific ones. To address this issue, we for the first time apply a dynamic matching network on the shared-private model for semi-supervised cross-domain dependency parsing. Meanwhile, considering the scarcity of target-domain labeled data, we leverage unlabeled data from two aspects, i.e., designing a new training strategy to improve the capability of the dynamic matching network and fine-tuning BERT to obtain domain-related contextualized representations. Experiments on benchmark datasets show that our proposed model consistently outperforms various baselines, leading to new state-of-the-art results on all domains. Detailed analysis on different matching strategies demonstrates that it is essential to learn suitable matching weights to emphasize useful features and ignore useless or even harmful ones. Besides, our proposed model can be directly extended to multi-source domain adaptation and achieves best performances among various baselines, further verifying the effectiveness and robustness.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1806b8f823fc2094b3f299d27de0257c32664f31',\n",
       "  'title': 'Improving Cross-Lingual Word Embeddings by Meeting in the Middle',\n",
       "  'authorId': '2624566',\n",
       "  'authorName': 'Yerai Doval',\n",
       "  'abstract': 'Cross-lingual word embeddings are becoming increasingly important in multilingual NLP. Recently, it has been shown that these embeddings can be effectively learned by aligning two disjoint monolingual vector spaces through linear transformations, using no more than a small bilingual dictionary as supervision. In this work, we propose to apply an additional transformation after the initial alignment step, which moves cross-lingual synonyms towards a middle point between them. By applying this transformation our aim is to obtain a better cross-lingual integration of the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces also improve by this transformation. This is in contrast to the original alignment, which is typically learned such that the structure of the monolingual spaces is preserved. Our experiments confirm that the resulting cross-lingual embeddings outperform state-of-the-art models in both monolingual and cross-lingual evaluation tasks.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b8a17190a1bf4e8297da71b99faf53448a8f5613',\n",
       "  'title': 'NILC_USP: An Improved Hybrid System for Sentiment Analysis in Twitter Messages',\n",
       "  'authorId': '27715110',\n",
       "  'authorName': 'P. B. Filho',\n",
       "  'abstract': 'This paper describes the NILC USP system that participated in SemEval-2014 Task 9: Sentiment Analysis in Twitter, a re-run of the SemEval 2013 task under the same name. Our system is an improved version of the system that participated in the 2013 task. This system adopts a hybrid classification process that uses three classification approaches: rule-based, lexiconbased and machine learning. We suggest a pipeline architecture that extracts the best characteristics from each classifier. In this work, we want to verify how this hybrid approach would improve with better classifiers. The improved system achieved an F-score of 65.39% in the Twitter message-level subtask for 2013 dataset (+ 9.08% of improvement) and 63.94% for 2014 dataset.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '20fd55830d7c33c6a12a769e9ef9b7b2cf3612d6',\n",
       "  'title': 'Making historical texts accessible to everybody',\n",
       "  'authorId': '2735405',\n",
       "  'authorName': 'C. Vertan',\n",
       "  'abstract': 'In this paper we discuss the degree of readability of historical texts for a broad public. We argue that text simplification methods can improve significantly this aspect and bring an added value to historical texts. We present a specific example, a genuine multilingual historical texts, which should be available at least to r e s a r h e r r m d i f r n t f i e l s a n d p r p o s a mechanism for simplifying the text.',\n",
       "  'year': 2014,\n",
       "  'venue': ''},\n",
       " {'paperId': 'dd2a5308d8e3374fb5ee603a09e22e2e9380a251',\n",
       "  'title': 'Towards Semantic Language Classification: Inducing and Clustering Semantic Association Networks from Europarl',\n",
       "  'authorId': '2620186',\n",
       "  'authorName': 'Steffen Eger',\n",
       "  'abstract': 'We induce semantic association networks from translation relations in parallel corpora. The resulting semantic spaces are encoded in a single reference language, which ensures cross-language comparability. As our main contribution, we cluster the obtained (crosslingually comparable) lexical semantic spaces. We find that, in our sample of languages, lexical semantic spaces largely coincide with genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'd130b7a1a99acb5329530420d17568d29dedc0d9',\n",
       "  'title': 'Passive-Aggressive Sequence Labeling with Discriminative Post-Editing for Recognising Person Entities in Tweets',\n",
       "  'authorId': '113320522',\n",
       "  'authorName': 'Leon Derczynski',\n",
       "  'abstract': 'Recognising entities in social media text is difficult. NER on newswire text is conventionally cast as a sequence labeling problem. This makes implicit assumptions regarding its textual structure. Social media text is rich in disfluency and often has poor or noisy structure, and intuitively does not always satisfy these assumptions. We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': 'cfc4f5ee223357e9b4b0173cdb4d96201ef1fce3',\n",
       "  'title': 'Project Underline - A Government Perspective',\n",
       "  'authorId': '35288491',\n",
       "  'authorName': 'Michael J. Chrzanowski',\n",
       "  'abstract': 'The purpose of the TIPSTER contract with Carnegie Group, Inc. (CGI) of Pittsburgh, PA is to promote and further develop automatic Text Summarization using a Maximal Marginal Relevance (MMR) metric to generate summaries of documents that are directly relevant to the information need of an individual user. CGI subcontracts with Carnegie Mellon University to perform most of its linguistic research.',\n",
       "  'year': 1998,\n",
       "  'venue': 'TIPSTER'},\n",
       " {'paperId': 'd073db916b5c04c59f4e58e4951159693c54dd80',\n",
       "  'title': 'Modeling the Influence of Verb Aspect on the Activation of Typical Event Locations with BERT',\n",
       "  'authorId': '2121567090',\n",
       "  'authorName': 'Won-Ik Cho',\n",
       "  'abstract': 'Prior studies on event knowledge in sentence comprehension have shown that the aspect of the main verb plays an important role in the processing of non-core semantic roles, such as locations: when the aspect of the main verb is imperfective, locations become more salient in the mental representation of the event and are easier for human comprehenders to process. In our study, we tested the popular language model BERT on two datasets derived from experimental studies to determine whether BERT’s predictions of prototypical event locations were also influenced by aspect. We found that, although BERT efficiently modelled the typicality of locations, it did so independently of the verb aspect. Even when the transformer was forced to focus on the verb phrase by masking the context words in the sentence, the typicality predictions were still accurate; in addition, we found aspect to have a stronger influence on the scores, with locations in the imperfective setting being associated with lower surprisal values.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '083d3082ae771d733c1b9390ee963c0a11cadb6c',\n",
       "  'title': 'Boosting-based Parse Reranking with Subtree Features',\n",
       "  'authorId': '1765329',\n",
       "  'authorName': 'Taku Kudo',\n",
       "  'abstract': 'This paper introduces a new application of boosting for parse reranking. Several parsers have been proposed that utilize the all-subtrees representation (e.g., tree kernel and data oriented parsing). This paper argues that such an all-subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees. We show how the boosting algorithm can be applied to the all-subtrees representation and how it selects a small and relevant feature set efficiently. Two experiments on parse reranking show that our method achieves comparable or even better performance than kernel methods and also improves the testing efficiency.',\n",
       "  'year': 2005,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '625eec49cddc4ae11c195e57c8cfb6edc8d244cd',\n",
       "  'title': 'EliXa: A Modular and Flexible ABSA Platform',\n",
       "  'authorId': '2247768',\n",
       "  'authorName': 'Iñaki San Vicente',\n",
       "  'abstract': 'This paper presents a supervised Aspect Based Sentiment Analysis (ABSA) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the Opinion Target Extraction (OTE) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classification (slot 3) is addressed by means of a multiclass SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '34ca2c54b21d5ad088e3138fb844dbbd496b7e3c',\n",
       "  'title': 'Hierarchical neural model with attention mechanisms for the classification of social media text related to mental health',\n",
       "  'authorId': '3456894',\n",
       "  'authorName': 'Julia Ive',\n",
       "  'abstract': 'Mental health problems represent a major public health challenge. Automated analysis of text related to mental health is aimed to help medical decision-making, public health policies and to improve health care. Such analysis may involve text classification. Traditionally, automated classification has been performed mainly using machine learning methods involving costly feature engineering. Recently, the performance of those methods has been dramatically improved by neural methods. However, mainly Convolutional neural networks (CNNs) have been explored. In this paper, we apply a hierarchical Recurrent neural network (RNN) architecture with an attention mechanism on social media data related to mental health. We show that this architecture improves overall classification results as compared to previously reported results on the same data. Benefitting from the attention mechanism, it can also efficiently select text elements crucial for classification decisions, which can also be used for in-depth analysis.',\n",
       "  'year': 2018,\n",
       "  'venue': 'CLPsych@NAACL-HTL'},\n",
       " {'paperId': 'cfd468bf8b138b1eed6b32ad262a1a794f9440b4',\n",
       "  'title': 'A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification',\n",
       "  'authorId': '3444092',\n",
       "  'authorName': 'Lianhui Qin',\n",
       "  'abstract': 'Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classification is the bottleneck for discourse parsing. Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred. This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '3cf51f5f36bac0cbafdb7581d8713979e0d1c17e',\n",
       "  'title': 'CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization',\n",
       "  'authorId': '6333082',\n",
       "  'authorName': 'Shuyang Cao',\n",
       "  'abstract': 'We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. We further design four types of strategies for creating negative samples, to resemble errors made commonly by two state-of-the-art models, BART and PEGASUS, found in our new human annotations of summary errors. Experiments on XSum and CNN/Daily Mail show that our contrastive learning framework is robust across datasets and models. It consistently produces more factual summaries than strong comparisons with post error correction, entailment-based reranking, and unlikelihood training, according to QA-based factuality evaluation. Human judges echo the observation and find that our model summaries correct more errors.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '05dec9ff0f48f45b09245ba0354748961f18ff77',\n",
       "  'title': 'Citation-Enhanced Keyphrase Extraction from Research Papers: A Supervised Approach',\n",
       "  'authorId': '1690656',\n",
       "  'authorName': 'Cornelia Caragea',\n",
       "  'abstract': 'Given the large amounts of online textual documents available these days, e.g., news articles, weblogs, and scientific papers, effective methods for extracting keyphrases, which provide a high-level topic description of a document, are greatly needed. In this paper, we propose a supervised model for keyphrase extraction from research papers, which are embedded in citation networks. To this end, we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'e29e43d9c0772d44cff53044484970599db30d5f',\n",
       "  'title': 'Domain Differential Adaptation for Neural Machine Translation',\n",
       "  'authorId': '14199369',\n",
       "  'authorName': 'Zi-Yi Dou',\n",
       "  'abstract': 'Neural networks are known to be data hungry and domain sensitive, but it is nearly impossible to obtain large quantities of labeled data for every domain we are interested in. This necessitates the use of domain adaptation strategies. One common strategy encourages generalization by aligning the global distribution statistics between source and target domains, but one drawback is that the statistics of different domains or tasks are inherently divergent, and smoothing over these differences can lead to sub-optimal performance. In this paper, we propose the framework of Domain Differential Adaptation (DDA), where instead of smoothing over these differences we embrace them, directly modeling the difference between domains using models in a related task. We then use these learned domain differentials to adapt models for the target task accordingly. Experimental results on domain adaptation for neural machine translation demonstrate the effectiveness of this strategy, achieving consistent improvements over other alternative adaptation strategies in multiple experimental settings.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a538a05864a23e2f80f9b003d5ecbdfb8025b954',\n",
       "  'title': 'USFD at SemEval-2016 Task 6: Any-Target Stance Detection on Twitter with Autoencoders',\n",
       "  'authorId': '1736067',\n",
       "  'authorName': 'Isabelle Augenstein',\n",
       "  'abstract': 'This paper describes the University of Sheffield’s submission to the SemEval 2016 Twitter Stance Detection weakly supervised task (SemEval 2016 Task 6, Subtask B). In stance detection, the goal is to classify the stance of a tweet towards a target as “favor”, “against”, or “none”. In Subtask B, the targets in the test data are different from the targets in the training data, thus rendering the task more challenging but also more realistic. To address the lack of target-specific training data, we use a large set of unlabelled tweets containing all targets and train a bag-of-words autoencoder to learn how to produce feature representations of tweets. These feature representations are then used to train a logistic regression classifier on labelled tweets, with additional features such as an indicator of whether the target is contained in the tweet. Our submitted run on the test data achieved an F1 of 0.3270.',\n",
       "  'year': 2016,\n",
       "  'venue': 'SemEval@NAACL-HLT'},\n",
       " {'paperId': '93556328d4a4cbc07fd3f0088a622483c4fe8206',\n",
       "  'title': 'Some Issues in Parsing and Natural Language Understanding',\n",
       "  'authorId': '2189985',\n",
       "  'authorName': 'R. Bobrow',\n",
       "  'abstract': 'Language is a system for encoding and transmitting ideas. A theory that seeks to explain linguistic phenomena in terms of this fact is a functional theory. One that does not misses the point. [10]',\n",
       "  'year': 1981,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '81aef3e695a659eac556e5f5433c0726ce3ae640',\n",
       "  'title': 'Inducing a lexicon of sociolinguistic variables from code-mixed text',\n",
       "  'authorId': '2132856',\n",
       "  'authorName': 'Philippa Shoemark',\n",
       "  'abstract': 'Sociolinguistics is often concerned with how variants of a linguistic item (e.g., nothing vs. nothin’) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (football, fitba) along with their linguistic code (football→British, fitba→Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70% for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NUT@EMNLP'},\n",
       " {'paperId': '14bd7f40226ce130611115ff4e18a4e1912dafb0',\n",
       "  'title': 'Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering',\n",
       "  'authorId': '46664096',\n",
       "  'authorName': 'Jinhyuk Lee',\n",
       "  'abstract': 'Recently, open-domain question answering (QA) has been combined with machine comprehension models to find answers in a large knowledge source. As open-domain QA requires retrieving relevant documents from text corpora to answer questions, its performance largely depends on the performance of document retrievers. However, since traditional information retrieval systems are not effective in obtaining documents with a high probability of containing answers, they lower the performance of QA systems. Simply extracting more documents increases the number of irrelevant documents, which also degrades the performance of QA systems. In this paper, we introduce Paragraph Ranker which ranks paragraphs of retrieved documents for a higher answer recall with less noise. We show that ranking paragraphs and aggregating answers using Paragraph Ranker improves performance of open-domain QA pipeline on the four open-domain QA datasets by 7.8% on average.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '11fdffe08eb1b08bfa8e3fdabe1accfd0e336be3',\n",
       "  'title': 'Learning bilingual word embeddings with (almost) no bilingual data',\n",
       "  'authorId': '2347956',\n",
       "  'authorName': 'Mikel Artetxe',\n",
       "  'abstract': 'Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '434d0d67ec033385e5179479d1be328021cff688',\n",
       "  'title': 'Leave-one-out Word Alignment without Garbage Collector Effects',\n",
       "  'authorId': '51101169',\n",
       "  'authorName': 'Xiaolin Wang',\n",
       "  'abstract': 'Expectation-maximization algorithms, such as those implemented in GIZA++ pervade the field of unsupervised word alignment. However, these algorithms have a problem of over-fitting, leading to “garbage collector effects,” where rare words tend to be erroneously aligned to untranslated words. This paper proposes a leave-one-out expectationmaximization algorithm for unsupervised word alignment to address this problem. The proposed method excludes information derived from the alignment of a sentence pair from the alignment models used to align it. This prevents erroneous alignments within a sentence pair from supporting themselves. Experimental results on Chinese-English and Japanese-English corpora show that the F1, precision and recall of alignment were consistently increased by 5.0% ‐ 17.2%, and BLEU scores of end-to-end translation were raised by 0.03 ‐ 1.30. The proposed method also outperformed l0-normalized GIZA++ and Kneser-Ney smoothed GIZA++.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '0b1a0b92b34a707ff84b5c41121713624a4ba140',\n",
       "  'title': 'String-to-Dependency Statistical Machine Translation',\n",
       "  'authorId': '39839719',\n",
       "  'authorName': 'Libin Shen',\n",
       "  'abstract': 'We propose a novel string-to-dependency algorithm for statistical machine translation. This algorithm employs a target dependency language model during decoding to exploit long distance word relations, which cannot be modeled with a traditional n-gram language model. Experiments show that the algorithm achieves significant improvement in MT performance over a state-of-the-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets.',\n",
       "  'year': 2010,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': 'b0cd945f73e0f28a47cacb53f16d394917eea3a8',\n",
       "  'title': 'Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking',\n",
       "  'authorId': '24905917',\n",
       "  'authorName': 'Masaru Isonuma',\n",
       "  'abstract': 'This paper focuses on the end-to-end abstractive summarization of a single product review without supervision. We assume that a review can be described as a discourse tree, in which the summary is the root, and the child sentences explain their parent in detail. By recursively estimating a parent from its children, our model learns the latent discourse tree without an external parser and generates a concise summary. We also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point. The experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches. In particular, for relatively long reviews, it achieves a competitive or better performance than supervised models. The induced tree shows that the child sentences provide additional information about their parent, and the generated summary abstracts the entire review.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'efd6540b6c3631127ae9d414a2dcfa6fe22e930e',\n",
       "  'title': 'Understanding who uses Reddit: Profiling individuals with a self-reported bipolar disorder diagnosis',\n",
       "  'authorId': '2322347',\n",
       "  'authorName': 'Glorianna Jagfeld',\n",
       "  'abstract': 'Recently, research on mental health conditions using public online data, including Reddit, has surged in NLP and health research but has not reported user characteristics, which are important to judge generalisability of findings. This paper shows how existing NLP methods can yield information on clinical, demographic, and identity characteristics of almost 20K Reddit users who self-report a bipolar disorder diagnosis. This population consists of slightly more feminine- than masculine-gendered mainly young or middle-aged US-based adults who often report additional mental health diagnoses, which is compared with general Reddit statistics and epidemiological studies. Additionally, this paper carefully evaluates all methods and discusses ethical issues.',\n",
       "  'year': 2021,\n",
       "  'venue': 'CLPSYCH'},\n",
       " {'paperId': '321b414065f76eab7898b8cd3b6d7782b8a02490',\n",
       "  'title': '“What Is Your Evidence?” A Study of Controversial Topics on Social Media',\n",
       "  'authorId': '3456325',\n",
       "  'authorName': 'Aseel Addawood',\n",
       "  'abstract': 'In recent years, social media has revolutionized how people communicate and share information. One function of social media, besides connecting with friends, is sharing opinions with others. Micro blogging sites, like Twitter, have often provided an online forum for social activism. When users debate about controversial topics on social media, they typically share different types of evidence to support their claims. Classifying these types of evidence can provide an estimate for how adequately the arguments have been supported. We first introduce a manually built gold standard dataset of 3000 tweets related to the recent FBI and Apple encryption debate. We develop a framework for automatically classifying six evidence types typically used on Twitter to discuss the debate. Our findings show that a Support Vector Machine (SVM) classifier trained with n-gram and additional features is capable of capturing the different forms of representing evidence on Twitter, and exhibits significant improvements over the unigram baseline, achieving a F1 macroaveraged of 82.8%.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ArgMining@ACL'},\n",
       " {'paperId': '6d3dff5242f731b43b48130e51f16e4795167095',\n",
       "  'title': 'Multi-input Recurrent Independent Mechanisms for leveraging knowledge sources: Case studies on sentiment analysis and health text mining',\n",
       "  'authorId': '3422115',\n",
       "  'authorName': 'Parsa Bagherzadeh',\n",
       "  'abstract': 'This paper presents a way to inject and leverage existing knowledge from external sources in a Deep Learning environment, extending the recently proposed Recurrent Independent Mechnisms (RIMs) architecture, which comprises a set of interacting yet independent modules. We show that this extension of the RIMs architecture is an effective framework with lower parameter implications compared to purely fine-tuned systems.',\n",
       "  'year': 2021,\n",
       "  'venue': 'DEELIO'},\n",
       " {'paperId': '3c9543bb659832f79132c0444ed91912ceb18836',\n",
       "  'title': 'Improving Chinese Word Segmentation with Wordhood Memory Networks',\n",
       "  'authorId': '151472012',\n",
       "  'authorName': 'Yuanhe Tian',\n",
       "  'abstract': 'Contextual features always play an important role in Chinese word segmentation (CWS). Wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters. However, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks. In this paper, we therefore propose a neural framework, WMSeg, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS. Experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps WMSeg achieve state-of-the-art performance on all those datasets. Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '76070f348c99153c2aa5df96ad2ad55e1b866fa0',\n",
       "  'title': 'LTRC-MT Simple & Effective Hindi-English Neural Machine Translation Systems at WAT 2019',\n",
       "  'authorId': '2055869370',\n",
       "  'authorName': 'Vikrant Goyal',\n",
       "  'abstract': 'This paper describes the Neural Machine Translation systems of IIIT-Hyderabad (LTRC-MT) for WAT 2019 Hindi-English shared task. We experimented with both Recurrent Neural Networks & Transformer architectures. We also show the results of our experiments of training NMT models using additional data via backtranslation.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'de82ac9c443ace8f3552ca3d2fcb7387d8199092',\n",
       "  'title': 'Towards Improving Neural Named Entity Recognition with Gazetteers',\n",
       "  'authorId': '1701889',\n",
       "  'authorName': 'Tianyu Liu',\n",
       "  'abstract': 'Most of the recently proposed neural models for named entity recognition have been purely data-driven, with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features. This could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a',\n",
       "  'title': 'Learning to Deceive with Attention-Based Explanations',\n",
       "  'authorId': '7880098',\n",
       "  'authorName': 'Danish Pruthi',\n",
       "  'abstract': 'Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'db621b0ba5577a4962ee08bd113e1dcabc911ddb',\n",
       "  'title': 'Deep Generalized Canonical Correlation Analysis',\n",
       "  'authorId': '145583569',\n",
       "  'authorName': 'Adrian Benton',\n",
       "  'abstract': 'We present Deep Generalized Canonical Correlation Analysis (DGCCA) – a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn and evaluate DGCCA representations for three downstream tasks: phonetic transcription from acoustic & articulatory measurements, recommending hashtags and recommending friends on a dataset of Twitter users.',\n",
       "  'year': 2017,\n",
       "  'venue': 'RepL4NLP@ACL'},\n",
       " {'paperId': '169473a009731c7969a65ed2599e43a03ed83e71',\n",
       "  'title': 'TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts',\n",
       "  'authorId': '119240402',\n",
       "  'authorName': 'Sajad Sotudeh',\n",
       "  'abstract': 'Recent models in developing summarization systems consist of millions of parameters and the model performance is highly dependent on the abundance of training data. While most existing summarization corpora contain data in the order of thousands to one million, generation of large-scale summarization datasets in order of couple of millions is yet to be explored. Practically, more data is better at generalizing the training patterns to unseen data. In this paper, we introduce TLDR9+ –a large-scale summarization dataset– containing over 9 million training instances extracted from Reddit discussion forum ([HTTP]). This dataset is specifically gathered to perform extreme summarization (i.e., generating one-sentence summary in high compression and abstraction) and is more than twice larger than the previously proposed dataset. We go one step further and with the help of human annotations, we distill a more fine-grained dataset by sampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We further pinpoint different state-of-the-art summarization models on our proposed datasets.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NEWSUM'},\n",
       " {'paperId': 'df931ac9d80844394b356b0d05a77a40f291f7b2',\n",
       "  'title': 'Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems',\n",
       "  'authorId': '2131709',\n",
       "  'authorName': 'Pei-hao Su',\n",
       "  'abstract': \"Statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users. However in the reinforcement learning paradigm the dialogue manager (agent) often requires significant time to explore the state-action space to learn to behave in a desirable manner. This is a critical issue when the system is trained on-line with real users where learning costs are expensive. Reward shaping is one promising technique for addressing these concerns. Here we examine three recurrent neural network (RNN) approaches for providing reward shaping information in addition to the primary (task-orientated) environmental feedback. These RNNs are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster. In both simulated and real user scenarios these RNNs are shown to increase policy learning speed. Importantly, they do not require prior knowledge of the user's goal.\",\n",
       "  'year': 2015,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '724c5ae8dc57d0d2380607dca6cd29503d45e778',\n",
       "  'title': 'A Multilingual Paradigm for Automatic Verb Classification',\n",
       "  'authorId': '143939590',\n",
       "  'authorName': 'Paola Merlo',\n",
       "  'abstract': 'We demonstrate the benefits of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpora in multiple languages. Our research incorporates two interrelated threads. In one, we exploit the similarities in the crosslinguistic classification of verbs, to extend work on English verb classification to a new language (Italian), and to new classes within that language, achieving an accuracy of 86.4% (baseline 33.9%). Our second strand of research exploits the differences across languages in the syntactic expression of semantic properties, to show that complementary information about English verbs can be extracted from their translations in a second language (Chinese). The use of multilingual features improves classification performance of the English verbs, achieving an accuracy of 83.5% (baseline 33.3%).',\n",
       "  'year': 2002,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1219ca643d9576dc683b41ed3f55733ce4e26189',\n",
       "  'title': 'Aravind K. Joshi',\n",
       "  'authorId': '1736049',\n",
       "  'authorName': 'B. Webber',\n",
       "  'abstract': 'It might surprise some young researchers that the first recipient of the ACL LifeTime Achievement award, Aravind Joshi, was so often compared to “Yoda,” one of the oldest and most powerful of the Jedi Masters in the Star Wars universe. But Aravind was also one of the kindest, wisest, and most justly celebrated people that one was fortunate to know. When Aravind received the award in 2002, at the age of 73, he said that he hoped his lifetime wasn’t over. Fortunately, it wasn’t: For the next 15 years, Aravind continued to enjoy time spent on research; advising students and younger colleagues; attending ACL conferences both at home in the United States and in far-flung places such as Sydney, Singapore, and Jeju Island; and enjoying the company of his extraordinary wife, the embryologist Susan Heyner, his daughters, Meera and Shyamala Joshi, and his grandchildren, Marco and Ava. Then on 31 December 2017, Aravind died peacefully at home in Philadelphia, sitting in his favorite chair, at the age of 88. Aravind Joshi was born in Pune, India, on 5 August, 1929. He sailed to the United States in 1954 to study electrical engineering (EE) at the University of Pennsylvania, after he was rejected by Harvard because his application, mailed from India, arrived a day late. While completing his M.Sc. in EE, he worked as an engineer at RCA (Camden, NJ), and then while completing his Ph.D. in EE, as a research assistant at the University of Pennsylvania’s Department of Linguistics. After being awarded his doctorate, Aravind joined the Penn faculty, remaining in EE until the brilliant and prescient Saul Gorn, who chaired Penn’s Graduate Group in Computer and Information Science, convinced the University to establish a new academic department of Computer and Information Science (CIS). Aravind joined this new department as a full professor and Chair, grateful that Saul Gorn had argued so forcefully that the new department should embrace the science of information as well as the practical study of computers and computing. Allowing for such broad intellectual content allowed the evolving CIS Department to constantly reach out to researchers in other disciplines—including those at Penn’s Wharton School of Business, as well as at the Departments of Linguistics, Psychology, Philosophy and Bioinformatics. Aravind remained Chair of CIS for an incredible 13 years, until 1985—continuing throughout this time to carry out cutting-edge research, to serve leadership roles in both ACL (as President in 1975 and then as Book Series Editor from 1982) and IJCAI',\n",
       "  'year': 2018,\n",
       "  'venue': 'Computational Linguistics'},\n",
       " {'paperId': 'a8e4580471908d17e279000d328f39654359bd6e',\n",
       "  'title': 'Beam Search Strategies for Neural Machine Translation',\n",
       "  'authorId': '35307070',\n",
       "  'authorName': 'Markus Freitag',\n",
       "  'abstract': 'The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German to English and Chinese to English without losing any translation quality.',\n",
       "  'year': 2017,\n",
       "  'venue': 'NMT@ACL'},\n",
       " {'paperId': 'd94e70037e62ac941565de4f85c9139ec4e08716',\n",
       "  'title': 'Igbo Diacritic Restoration using Embedding Models',\n",
       "  'authorId': '3452783',\n",
       "  'authorName': 'I. Ezeani',\n",
       "  'abstract': 'Igbo is a low-resource language spoken by approximately 30 million people worldwide. It is the native language of the Igbo people of south-eastern Nigeria. In Igbo language, diacritics - orthographic and tonal - play a huge role in the distinguishing the meaning and pronunciation of words. Omitting diacritics in texts often leads to lexical ambiguity. Diacritic restoration is a pre-processing task that replaces missing diacritics on words from which they have been removed. In this work, we applied embedding models to the diacritic restoration task and compared their performances to those of n-gram models. Although word embedding models have been successfully applied to various NLP tasks, it has not been used, to our knowledge, for diacritic restoration. Two classes of word embeddings models were used: those projected from the English embedding space; and those trained with Igbo bible corpus (≈ 1m). Our best result, 82.49%, is an improvement on the baseline n-gram models.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'd129efbdd493d1bb7ba01cd647b88764050d52cd',\n",
       "  'title': 'LynyrdSkynyrd at WNUT-2020 Task 2: Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets',\n",
       "  'authorId': '30053596',\n",
       "  'authorName': 'Abhilasha Sancheti',\n",
       "  'abstract': 'In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing model achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.',\n",
       "  'year': 2020,\n",
       "  'venue': 'WNUT'},\n",
       " {'paperId': 'dcb0249ba85140849a07e3cbae358ec3e9b89ac5',\n",
       "  'title': 'Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering',\n",
       "  'authorId': '143618944',\n",
       "  'authorName': 'Vikas Yadav',\n",
       "  'abstract': 'Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd7d7ec998ad3582e40bbbd432fd75bc10687a729',\n",
       "  'title': 'UEdin: Translating L1 Phrases in L2 Context using Context-Sensitive SMT',\n",
       "  'authorId': '2955842',\n",
       "  'authorName': 'E. Hasler',\n",
       "  'abstract': 'We describe our systems for the SemEval 2014 Task 5: L2 writing assistant where a system has to find appropriate translations of L1 segments in a given L2 context. We participated in three out of four possible language pairs (English-Spanish, FrenchEnglish and Dutch-English) and achieved the best performance for all our submitted systems according to word-based accuracy. Our models are based on phrasebased machine translation systems and combine topical context information and language model scoring.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '83ea24b9a2d0cd322a5a4928c2bf2e8f82a300f9',\n",
       "  'title': 'LIIR at SemEval-2021 task 6: Detection of Persuasion Techniques In Texts and Images using CLIP features',\n",
       "  'authorId': '52206536',\n",
       "  'authorName': 'Erfan Ghadery',\n",
       "  'abstract': 'We describe our approach for SemEval-2021 task 6 on detection of persuasion techniques in multimodal content (memes). Our system combines pretrained multimodal models (CLIP) and chained classifiers. Also, we propose to enrich the data by a data augmentation technique. Our submission achieves a rank of 8/16 in terms of F1-micro and 9/16 with F1-macro on the test set.',\n",
       "  'year': 2021,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': 'fff5f58691af8aa30e113ed5129cbdf7d1df9330',\n",
       "  'title': 'Simple Open Stance Classification for Rumour Analysis',\n",
       "  'authorId': '145970060',\n",
       "  'authorName': 'Ahmet Aker',\n",
       "  'abstract': 'Stance classification determines the attitude, or stance, in a (typically short) text. The task has powerful applications, such as the detection of fake news or the automatic extraction of attitudes toward entities or events in the media. This paper describes a surprisingly simple and efficient classification approach to open stance classification in Twitter, for rumour and veracity classification. The approach profits from a novel set of automatically identifiable problem-specific features, which significantly boost classifier accuracy and achieve above state-of-the-art results on recent benchmark datasets. This calls into question the value of using complex sophisticated models for stance classification without first doing informed feature extraction.',\n",
       "  'year': 2017,\n",
       "  'venue': 'RANLP'},\n",
       " {'paperId': '3de72d2b1ead0b9c7af5804252024128312b9cfe',\n",
       "  'title': 'Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models',\n",
       "  'authorId': '144412704',\n",
       "  'authorName': 'Tong Niu',\n",
       "  'abstract': 'We present two categories of model-agnostic adversarial strategies that reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change strategies that evaluate over-sensitivity to small and semantics-preserving edits, as well as Should-Change strategies that test if a model is over-stable against subtle yet semantics-changing modifications. We next perform adversarial training with each strategy, employing a max-margin approach for negative generative examples. This not only makes the target dialogue model more robust to the adversarial inputs, but also helps it perform significantly better on the original inputs. Moreover, training on all strategies combined achieves further improvements, achieving a new state-of-the-art performance on the original task (also verified via human evaluation). In addition to adversarial training, we also address the robustness task at the model-level, by feeding it subword units as both inputs and outputs, and show that the resulting model is equally competitive, requires only 1/4 of the original vocabulary size, and is robust to one of the adversarial strategies (to which the original model is vulnerable) even without adversarial training.',\n",
       "  'year': 2018,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': '5c4bd1bc0f56952ed9c7045fd172fdfa6b7ebb3d',\n",
       "  'title': 'Learning to Classify Events from Human Needs Category Descriptions',\n",
       "  'authorId': '47929135',\n",
       "  'authorName': 'Haibo Ding',\n",
       "  'abstract': 'We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to: (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LeaPI, a zero-shot learning method that first automatically generate weak labels by instantiating high-level concepts with prototypical instances and then trains a human needs classifier with the weakly labeled data. To filter noisy concepts, we design a reinforced selection algorithm to choose high-quality concepts for instantiation. Experimental results on the human needs categorization task show that our method outperforms baseline methods, producing substantially better precision.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP 2020'},\n",
       " {'paperId': '9abc08a7335700f803ec0ed0fe4f7c754a3b344d',\n",
       "  'title': 'Learning to Represent Review with Tensor Decomposition for Spam Detection',\n",
       "  'authorId': '2121265994',\n",
       "  'authorName': 'Xuepeng Wang',\n",
       "  'abstract': 'Review spam detection is a key task in opinion mining. To accomplish this type of detection, previous work has focused mainly on effectively representing fake and non-fake reviews with discriminative features, which are discovered or elaborately designed by experts or developers. This paper proposes a novel review spam detection method that learns the representation of reviews automatically instead of heavily relying on experts’ knowledge in a data-driven manner. More specifically, according to 11 relations (generated automatically from two basic patterns) between reviewers and products, we employ tensor decomposition to learn the embeddings of the reviewers and products in a vector space. We collect relations between any two entities (reviewers and products), which results in much useful and global information. We concatenate the review text, the embeddings of the reviewer and the reviewed product as the representation of a review. Based on such representations, the classifier could identify the opinion spam more precisely. Experimental results on an open Yelp dataset show that our method could effectively enhance the spam detection accuracy compared with the stateof-the-art methods.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '4a8f355f2e0037db623d36928111b7b3df7a9bcd',\n",
       "  'title': 'A Holistic Framework for Analyzing the COVID-19 Vaccine Debate',\n",
       "  'authorId': '32727925',\n",
       "  'authorName': 'Maria Leonor Pacheco',\n",
       "  'abstract': 'The Covid-19 pandemic has led to infodemic of low quality information leading to poor health decisions. Combating the outcomes of this infodemic is not only a question of identifying false claims, but also reasoning about the decisions individuals make.In this work we propose a holistic analysis framework connecting stance and reason analysis, and fine-grained entity level moral sentiment analysis. We study how to model the dependencies between the different level of analysis and incorporate human insights into the learning process. Experiments show that our framework provides reliable predictions even in the low-supervision settings.',\n",
       "  'year': 2022,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'b7fdd8a6b9b68997fd23948c0617147d79d22402',\n",
       "  'title': 'Generalized Agreement for Bidirectional Word Alignment',\n",
       "  'authorId': '2108013385',\n",
       "  'authorName': 'Chunyang Liu',\n",
       "  'abstract': 'While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '15b8a016a3f11152efd770e5e360f643306c8108',\n",
       "  'title': 'The Cornell TIPSTER Phase III Project',\n",
       "  'authorId': '35388740',\n",
       "  'authorName': 'F. Gee',\n",
       "  'abstract': 'The overall objective of the Cornell University TIPSTER Project was to improve end-user efficiency in information retrieval systems by reducing the amount of text that the user must process [1]. The project focuses on high precision IR, near-duplicate detection and context-dependent summarization. The two main foundations of the research are the latest version of the Smart system for information Retrieval and the Empire system for natural language processing. Smart is an implementation of the vector-space model of information retrieval (IR). Its earlier purpose was to provide a framework to conduct IR research but current developments will make the system easier to use by non-researcher. Empire is a research-oriented system that uses machine learning methods to quickly perform partial parsing of sentences.',\n",
       "  'year': 1998,\n",
       "  'venue': 'TIPSTER'},\n",
       " {'paperId': '27e441cfd261eb73074fd204261510577f2d434d',\n",
       "  'title': 'Syntax-driven Iterative Expansion Language Models for Controllable Text Generation',\n",
       "  'authorId': '1794731',\n",
       "  'authorName': 'Noe Casas',\n",
       "  'abstract': 'The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradigm for introducing a syntactic inductive bias into neural text generation, where the dependency parse tree is used to drive the Transformer model to generate sentences iteratively. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less than half their decoding steps, and its generation process allows direct control over the syntactic constructions of the generated text, enabling the induction of stylistic variations.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SPNLP'},\n",
       " {'paperId': '35ca0e30ba5882b76eaa200dfb9f7cf697dc2ad8',\n",
       "  'title': 'NEWS 2018 Whitepaper',\n",
       "  'authorId': '2185019',\n",
       "  'authorName': 'Nancy F. Chen',\n",
       "  'abstract': 'Transliteration is defined as phonetic translation of names across languages. Transliteration of Named Entities (NEs) is necessary in many applications, such as machine translation, corpus alignment, cross-language IR, information extraction and automatic lexicon acquisition. All such systems call for high-performance transliteration, which is the focus of shared task in the NEWS 2018 workshop. The objective of the shared task is to promote machine transliteration research by providing a common benchmarking platform for the community to evaluate the state-of-the-art technologies.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NEWS@ACL'},\n",
       " {'paperId': '7acd100552613fbc07aa03eeb58c1f1e800491ec',\n",
       "  'title': 'Porting Multilingual Morphological Resources to OntoLex-Lemon',\n",
       "  'authorId': '1743895',\n",
       "  'authorName': 'Thierry Declerck',\n",
       "  'abstract': 'We describe work consisting in porting various morphological resources to the OntoLex-Lemon model. A main objective of this work is to offer a uniform representation of different morphological data sets in order to be able to compare and interlink multilingual resources and to cross-check and interlink or merge the content of morphological resources of one and the same language. The results of our work will be published on the Linguistic Linked Open Data cloud.',\n",
       "  'year': 2019,\n",
       "  'venue': 'RANLP'},\n",
       " {'paperId': '125cfe04a7e27161721227cd934701b656efb8bf',\n",
       "  'title': 'Contextual Argument Component Classification for Class Discussions',\n",
       "  'authorId': '2075494631',\n",
       "  'authorName': 'Luca Lugini',\n",
       "  'abstract': 'Argument mining systems often consider contextual information, i.e. information outside of an argumentative discourse unit, when trained to accomplish tasks such as argument component identification, classification, and relation extraction. However, prior work has not carefully analyzed the utility of different contextual properties in context-aware models. In this work, we show how two different types of contextual information, local discourse context and speaker context, can be incorporated into a computational model for classifying argument components in multi-party classroom discussions. We find that both context types can improve performance, although the improvements are dependent on context size and position.',\n",
       "  'year': 2021,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '19dcc665dee21a8148f9bd07066332783d212126',\n",
       "  'title': 'Evaluating Explanation Methods for Neural Machine Translation',\n",
       "  'authorId': '9073069',\n",
       "  'authorName': 'Jierui Li',\n",
       "  'abstract': 'Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '618c1f2557f4b59a167865f5305a95699061184a',\n",
       "  'title': 'WarwickDCS: From Phrase-Based to Target-Specific Sentiment Recognition',\n",
       "  'authorId': '144898685',\n",
       "  'authorName': 'Richard Townsend',\n",
       "  'abstract': 'We present and evaluate several hybrid systems for sentiment identification for Twitter, both at the phrase and document (tweet) level. Our approach has been to use a novel combination of lexica, traditional NLP and deep learning features. We also analyse techniques based on syntactic parsing and tokenbased association to handle topic specific sentiment in subtask C. Our strategy has been to identify subphrases relevant to the designated topic/target and assign sentiment according to our subtask A classifier. Our submitted subtask A classifier ranked fourth in the SemEval official results while our BASELINEand µPARSE classifiers for subtask C would have ranked second.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'b7ed99696d8854d080152b0723483d7c956ed838',\n",
       "  'title': 'Bi-Directional Neural Machine Translation with Synthetic Parallel Data',\n",
       "  'authorId': '145523874',\n",
       "  'authorName': 'Xing Niu',\n",
       "  'abstract': 'Despite impressive progress in high-resource settings, Neural Machine Translation (NMT) still struggles in low-resource and out-of-domain scenarios, often failing to match the quality of phrase-based translation. We propose a novel technique that combines back-translation and multilingual NMT to improve performance in these difficult cases. Our technique trains a single model for both directions of a language pair, allowing us to back-translate source or target monolingual data without requiring an auxiliary model. We then continue training on the augmented parallel data, enabling a cycle of improvement for a single model that can incorporate any source, target, or parallel data to improve both translation directions. As a byproduct, these models can reduce training and deployment costs significantly compared to uni-directional models. Extensive experiments show that our technique outperforms standard back-translation in low-resource scenarios, improves quality on cross-domain tasks, and effectively reduces costs across the board.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NMT@ACL'},\n",
       " {'paperId': '699729d637e07447f2d1b9fd6322427bea78278a',\n",
       "  'title': 'Semantic Analysis of Japanese Noun Phrases - A New Approach to Dictionary-Based Understanding',\n",
       "  'authorId': '1795664',\n",
       "  'authorName': 'S. Kurohashi',\n",
       "  'abstract': 'This paper presents a new method of analyzing Japanese noun phrases of the form N1 no N2. The Japanese postposition no roughly corresponds to of, but it has much broader usage. The method exploits a definition of N2 in a dictionary. For example, rugby no coach can be interpreted as a person who teaches technique in rugby. We illustrate the effectiveness of the method by the analysis of 300 test noun phrases.',\n",
       "  'year': 1999,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'b57574945012772e86f152d710e11917b1d41b1a',\n",
       "  'title': 'A Quantitative Evaluation of Linguistic Tests for the Automatic Prediction of Semantic Markedness',\n",
       "  'authorId': '1799688',\n",
       "  'authorName': 'V. Hatzivassiloglou',\n",
       "  'abstract': 'We present a corpus-based study of methods that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjectives. Solutions to this problem are applicable to the more general task of selecting the positive term from the pair. Using automatically collected data, the accuracy and applicability of each method is quantified, and a statistical analysis of the significance of the results is performed. We show that some simple methods are indeed good indicators for the answer to the problem while other proposed methods fail to perform better than would be attributable to chance. In addition, one of the simplest methods, text frequency, dominates all others. We also apply two generic statistical learning methods for combining the indications of the individual methods, and compare their performance to the simple methods. The most sophisticated complex learning method offers a small, but statistically significant, improvement over the original tests.',\n",
       "  'year': 1995,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3c9a3419d2eeefa8a4d0390839cff773dbc27fca',\n",
       "  'title': 'Learning to Generate Examples for Semantic Processing Tasks',\n",
       "  'authorId': '1784172',\n",
       "  'authorName': 'D. Croce',\n",
       "  'abstract': 'Even if recent Transformer-based architectures, such as BERT, achieved impressive results in semantic processing tasks, their fine-tuning stage still requires large scale training resources. Usually, Data Augmentation (DA) techniques can help to deal with low resource settings. In Text Classification tasks, the objective of DA is the generation of well-formed sentences that i) represent the desired task category and ii) are novel with respect to existing sentences. In this paper, we propose a neural approach to automatically learn to generate new examples using a pre-trained sequence-to-sequence model. We first learn a task-oriented similarity function that we use to pair similar examples. Then, we use these example pairs to train a model to generate examples. Experiments in low resource settings show that augmenting the training material with the proposed strategy systematically improves the results on text classification and natural language inference tasks by up to 10% accuracy, outperforming existing DA approaches.',\n",
       "  'year': 2022,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'dd0f388c57ede2aa86b8a40fee018a288c81b821',\n",
       "  'title': 'Automated Concatenation of Embeddings for Structured Prediction',\n",
       "  'authorId': '47120498',\n",
       "  'authorName': 'Xinyu Wang',\n",
       "  'abstract': 'Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '2365525857ebb4d55791867b5bff37cbef65b84d',\n",
       "  'title': 'Linguistic Cues to Deception and Perceived Deception in Interview Dialogues',\n",
       "  'authorId': '2350571',\n",
       "  'authorName': 'Sarah Ita Levitan',\n",
       "  'abstract': 'We explore deception detection in interview dialogues. We analyze a set of linguistic features in both truthful and deceptive responses to interview questions. We also study the perception of deception, identifying characteristics of statements that are perceived as truthful or deceptive by interviewers. Our analysis show significant differences between truthful and deceptive question responses, as well as variations in deception patterns across gender and native language. This analysis motivated our selection of features for machine learning experiments aimed at classifying globally deceptive speech. Our best classification performance is 72.74% F1-Score (about 17% better than human performance), which is achieved using a combination of linguistic features and individual traits.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '9ada3dd906fe5051c80e618a53e39993b457d2b2',\n",
       "  'title': 'UWM: Applying an Existing Trainable Semantic Parser to Parse Robotic Spatial Commands',\n",
       "  'authorId': '2108641',\n",
       "  'authorName': 'Rohit J. Kate',\n",
       "  'abstract': 'This paper describes Team UWM’s system for the Task 6 of SemEval 2014 for doing supervised semantic parsing of robotic spatial commands. An existing semantic parser, KRISP, was trained using the provided training data of natural language robotic spatial commands paired with their meaning representations in the formal robot command language. The entire process required very little manual effort. Without using the additional annotations of word-aligned semantic trees, the trained parser was able to exactly parse new commands into their meaning representations with 51.18% best F-measure at 72.67% precision and 39.49% recall. Results show that the parser was particularly accurate for short sentences.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '2b7523738c788fd3dfcab205501d1c46eb6b19bb',\n",
       "  'title': 'Efficient construction of metadata-enhanced web corpora',\n",
       "  'authorId': '2551614',\n",
       "  'authorName': 'A. Barbaresi',\n",
       "  'abstract': 'Metadata extraction is known to be a problem in general-purpose Web corpora, and so is extensive crawling with little yield. The contributions of this paper are threefold: a method to find and download large numbers of WordPress pages; a targeted extraction of content featuring much needed metadata; and an analysis of the documents in the corpus with insights of actual blog uses. The study focuses on a publishing software (WordPress), which allows for reliable extraction of structural elements such as metadata, posts, and comments. The download of about 9 million documents in the course of two experiments leads after processing to 2.7 billion tokens with usable metadata. This comparatively high yield is a step towards more efficiency with respect to machine power and \" Hi-Fi \" web corpora. The resulting corpus complies with formal requirements on metadata-enhanced corpora and on weblogs considered as a series of dated entries. However, existing typologies on Web texts have to be revised in the light of this hybrid genre.',\n",
       "  'year': 2016,\n",
       "  'venue': 'WAC@ACL'},\n",
       " {'paperId': 'f7344ae8941998e2a883d92f2a9e626c6a67afb6',\n",
       "  'title': 'VERTa: a Linguistically-motivated Metric at the WMT15 Metrics Task',\n",
       "  'authorId': '69049001',\n",
       "  'authorName': 'Elisabet Comelles',\n",
       "  'abstract': 'This paper describes VERTa’s submission to the 2015 EMNLP Workshop on Statistical Machine Translation. VERTa is a linguistically-motivated metric that combines linguistic features at different levels. In this paper, VERTa is described briefly, as well as the three versions submitted to the workshop: VERTa70Adeq30Flu, VERTa-EQ and VERTaW. Finally, the experiments conducted with the WMT14 data are reported and some conclusions are drawn.',\n",
       "  'year': 2015,\n",
       "  'venue': ''},\n",
       " {'paperId': '8bb783ca5db8a8091345bbceea26fee435b86fe8',\n",
       "  'title': 'Speechformer: Reducing Information Loss in Direct Speech Translation',\n",
       "  'authorId': '2006601535',\n",
       "  'authorName': 'Sara Papi',\n",
       "  'abstract': 'Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including speech translation. However, Transformer’s quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful linguistic information is not accessible to higher-level layers in the architecture. To solve this issue, we propose Speechformer, an architecture that, thanks to reduced memory usage in the attention layers, avoids the initial lossy compression and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (en→de/es/nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '25cb940ebd1dcd4bdee30c9ddb24aa62b0951549',\n",
       "  'title': 'GrammarSHAP: An Efficient Model-Agnostic and Structure-Aware NLP Explainer',\n",
       "  'authorId': '1786389',\n",
       "  'authorName': 'E. Mosca',\n",
       "  'abstract': 'Interpreting NLP models is fundamental for their development as it can shed light on hidden properties and unexpected behaviors. However, while transformer architectures exploit contextual information to enhance their predictive capabilities, most of the available methods to explain such predictions only provide importance scores at the word level. This work addresses the lack of feature attribution approaches that also take into account the sentence structure. We extend the SHAP framework by proposing GrammarSHAP—a model-agnostic explainer leveraging the sentence’s constituency parsing to generate hierarchical importance scores.',\n",
       "  'year': 2022,\n",
       "  'venue': 'LNLS'},\n",
       " {'paperId': '2f79aa196381d86ba938bf7c6503b3bf71a524dd',\n",
       "  'title': 'Supertagging the Long Tail with Tree-Structured Decoding of Complex Categories',\n",
       "  'authorId': '2126975',\n",
       "  'authorName': 'Jakob Prange',\n",
       "  'abstract': 'Abstract Although current CCG supertaggers achieve high accuracy on the standard WSJ test set, few systems make use of the categories’ internal structure that will drive the syntactic derivation during parsing. The tagset is traditionally truncated, discarding the many rare and complex category types in the long tail. However, supertags are themselves trees. Rather than give up on rare tags, we investigate constructive models that account for their internal structure, including novel methods for tree-structured prediction. Our best tagger is capable of recovering a sizeable fraction of the long-tail supertags and even generates CCG categories that have never been seen in training, while approximating the prior state of the art in overall tag accuracy with fewer parameters. We further investigate how well different approaches generalize to out-of-domain evaluation sets.',\n",
       "  'year': 2020,\n",
       "  'venue': 'Transactions of the Association for Computational Linguistics'},\n",
       " {'paperId': '62cb0c9bff4075277b27004d6fdec65ccab31c8b',\n",
       "  'title': 'Exact Decoding with Multi Bottom-Up Tree Transducers',\n",
       "  'authorId': '2612046',\n",
       "  'authorName': 'Daniel Quernheim',\n",
       "  'abstract': 'We present an experimental statistical tree-to-tree machine translation system based on the multi-bottom up tree transducer including rule extraction, tuning and decoding. Thanks to input parse forests and a “no pruning” strategy during decoding, the obtained translations are competitive. The drawbacks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass.',\n",
       "  'year': 2015,\n",
       "  'venue': 'WMT@EMNLP'},\n",
       " {'paperId': 'e962c301df1d33bc12d8115f4c82093103c94eeb',\n",
       "  'title': 'Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision',\n",
       "  'authorId': '22313325',\n",
       "  'authorName': 'A. Obamuyide',\n",
       "  'abstract': 'In this paper we frame the task of supervised relation classification as an instance of meta-learning. We propose a model-agnostic meta-learning protocol for training relation classifiers to achieve enhanced predictive performance in limited supervision settings. During training, we aim to not only learn good parameters for classifying relations with sufficient supervision, but also learn model parameters that can be fine-tuned to enhance predictive performance for relations with limited supervision. In experiments conducted on two relation classification datasets, we demonstrate that the proposed meta-learning approach improves the predictive performance of two state-of-the-art supervised relation classification models.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '6eee69031d2e11aa03a5a8fcb219cff4562863be',\n",
       "  'title': 'ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning',\n",
       "  'authorId': '51518773',\n",
       "  'authorName': 'Rujun Han',\n",
       "  'abstract': 'While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This **E**ffective **CON**tinual pre-training framework for **E**vent **T**emporal reasoning (ECONET) improves the PTLMs’ fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'f64a276170521b695c59261347f02d4698889929',\n",
       "  'title': 'Reverse-engineering Language: A Study on the Semantic Compositionality of German Compounds',\n",
       "  'authorId': '2436364',\n",
       "  'authorName': 'C. Dima',\n",
       "  'abstract': 'In this paper we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpusestimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'c80606ae5e96dc55b9c41ecb63ef1d013011548b',\n",
       "  'title': 'Semantic Alignment with Calibrated Similarity for Multilingual Sentence Embedding',\n",
       "  'authorId': '48691049',\n",
       "  'authorName': 'Jiyeon Ham',\n",
       "  'abstract': 'Measuring the similarity score between a pair of sentences in different languages is the es-sential requisite for multilingual sentence embedding methods. Predicting the similarity score consists of two sub-tasks, which are monolingual similarity evaluation and multilingual sentence retrieval. However, conven-tional methods have mainly tackled only one of the sub-tasks and therefore showed biased performances. In this paper, we suggest a novel and strong method for multilingual sentence embedding, which shows performance improvement on both sub-tasks, consequently resulting in robust predictions of multilingual similarity scores. The suggested method consists of two parts: to learn semantic similarity of sentences in the pivot language and then to extend the learned semantic structure to different languages. To align semantic structures across different languages, we introduce a teacher-student network. The teacher network distills the knowledge of the pivot language to different languages of the student network. During the distillation, the parameters of the teacher network are updated with the slow-moving average. Together with the distillation and the parameter updating, the semantic structure of the student network can be directly aligned across different languages while preserving the ability to measure the semantic similarity. Thus, the multilingual training method drives performance improvement on multilingual similarity evaluation. The suggested model achieves the state-of-the-art performance on extended STS 2017 multilingual similarity evaluation as well as two sub-tasks, which are extended STS 2017 monolingual similarity evaluation and Tatoeba multilingual retrieval in 14 languages.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '2d9402130a22fb2d2c66fa2e3f64a9cacf146630',\n",
       "  'title': 'Exploring Adequacy Errors in Neural Machine Translation with the Help of Cross-Language Aligned Word Embeddings',\n",
       "  'authorId': '104539408',\n",
       "  'authorName': 'M. Ustaszewski',\n",
       "  'abstract': 'Neural machine translation (NMT) was shown to produce more fluent output than phrase-based statistical (PBMT) and rule-based machine translation (RBMT). However, improved fluency makes it more difficult for post editors to identify and correct adequacy errors, because unlike RBMT and SMT, in NMT adequacy errors are frequently not anticipated by fluency errors. Omissions and additions of content in otherwise flawlessly fluent NMT output are the most prominent types of such adequacy errors, which can only be detected with reference to source texts. This contribution explores the degree of semantic similarity between source texts, NMT output and post edited output. In this way, computational semantic similarity scores (cosine similarity) are related to human quality judgments. The analyses are based on publicly available NMT post editing data annotated for errors in three language pairs (EN-DE, EN-LV, EN-HR) with the Multidimensional Quality Metrics (MQM). Methodologically, this contribution tests whether cross-language aligned word embeddings as the sole source of semantic information mirror human error annotation.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Second Workshop Human-Informed Translation and Interpreting Technology associated with RANLP 2019'},\n",
       " {'paperId': '66316a4f2a0e5d6606582eb50f6b1d3ecec6a2cd',\n",
       "  'title': 'A Comparative Study on Vocabulary Reduction for Phrase Table Smoothing',\n",
       "  'authorId': '3179080',\n",
       "  'authorName': 'Yunsu Kim',\n",
       "  'abstract': 'This work systematically analyzes the smoothing effect of vocabulary reduction for phrase translation models. We extensively compare various word-level vocabularies to show that the performance of smoothing is not significantly affected by the choice of vocabulary. This result provides empirical evidence that the standard phrase translation model is extremely sparse. Our experiments also reveal that vocabulary reduction is more effective for smoothing large-scale phrase tables.',\n",
       "  'year': 2019,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '72c0e5b7365ae4981db13cfa15ca808a8eb3a8a1',\n",
       "  'title': 'Head-Driven Phrase Structure Grammar Parsing on Penn Treebank',\n",
       "  'authorId': '30887404',\n",
       "  'authorName': 'Junru Zhou',\n",
       "  'abstract': 'Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 95.84 F1 of constituent parsing and 97.00% UAS of dependency parsing on PTB.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'cc760dd9f909db13a1725e3c1f4975f47be358ef',\n",
       "  'title': 'Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data',\n",
       "  'authorId': '28130078',\n",
       "  'authorName': 'Paul Pu Liang',\n",
       "  'abstract': 'Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders. One promising data source to help monitor human behavior is daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes. In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors. Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood. However, we find that models trained to predict mood often also capture private user identities in their intermediate representations. To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive. By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ab85274e96b44e5d8324f4a622b3bf9dda50b33b',\n",
       "  'title': 'Textual Expertise in Word Experts: An Approach to Text Parsing Based on Topic/Comment Monitoring',\n",
       "  'authorId': '1744669',\n",
       "  'authorName': 'U. Hahn',\n",
       "  'abstract': 'In this paper prototype versions of two word experts for text analysis are dealt with which demonstrate that word experts are a feasible tool for parsing texts on the level of text cohesion as well as text coherence. The analysis is based on two major knowledge sources: context information is modelled in terms of a frame knowledge base, while the co-text keeps record of the linear sequencing of text analysis. The result of text parsing consists of a text graph reflecting the thematic organization of topics in a text.',\n",
       "  'year': 1984,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'fc1171568daa6c46c470eaaf128b14a777a6ceda',\n",
       "  'title': 'Tree-Adjoining Grammars Are Not Closed Under Strong Lexicalization',\n",
       "  'authorId': '40462390',\n",
       "  'authorName': 'Marco Kuhlmann',\n",
       "  'abstract': 'A lexicalized tree-adjoining grammar is a tree-adjoining grammar where each elementary tree contains some overt lexical item. Such grammars are being used to give lexical accounts of syntactic phenomena, where an elementary tree defines the domain of locality of the syntactic and semantic dependencies of its lexical items. It has been claimed in the literature that for every tree-adjoining grammar, one can construct a strongly equivalent lexicalized version. We show that such a procedure does not exist: Tree-adjoining grammars are not closed under strong lexicalization.',\n",
       "  'year': 2012,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': '4158285df6a2b0f5a0de567573d995328ccbbb38',\n",
       "  'title': 'Probing for Semantic Classes: Diagnosing the Meaning Content of Word Embeddings',\n",
       "  'authorId': '3261470',\n",
       "  'authorName': 'Yadollah Yaghoobzadeh',\n",
       "  'abstract': 'Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and word senses, where word senses from different words are related by semantic classes. This is the basis for novel diagnostic tests for an embedding’s content: we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes. Our main findings are: (i) Information about a sense is generally represented well in a single-vector embedding – if the sense is frequent. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3171ec184b5fec0bc7b47356ad74d8598e858ddc',\n",
       "  'title': 'Leveraging Linguistic Structure For Open Domain Information Extraction',\n",
       "  'authorId': '32301760',\n",
       "  'authorName': 'Gabor Angeli',\n",
       "  'abstract': 'Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, inference, and other IE tasks. Traditionally these are extracted using a large set of patterns; however, this approach is brittle on out-of-domain text and long-range dependencies, and gives no insight into the substructure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences. We then run natural logic inference over these short clauses to determine the maximally specific arguments for each candidate triple. We show that our approach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '35749accd78395fa0fe8f231311d0cb2f525f97b',\n",
       "  'title': 'Hybrid Enhanced Universal Dependencies Parsing',\n",
       "  'authorId': '1686239',\n",
       "  'authorName': 'Johannes Heinecke',\n",
       "  'abstract': 'This paper describes our system to predict enhanced dependencies for Universal Dependencies (UD) treebanks, which ranked 2nd in the Shared Task on Enhanced Dependency Parsing with an average ELAS of 82.60%. Our system uses a hybrid two-step approach. First, we use a graph-based parser to extract a basic syntactic dependency tree. Then, we use a set of linguistic rules which generate the enhanced dependencies for the syntactic tree. The application of these rules is optimized using a classifier which predicts their suitability in the given context. A key advantage of this approach is its language independence, as rules rely solely on dependency trees and UPOS tags which are shared across all languages.',\n",
       "  'year': 2020,\n",
       "  'venue': 'IWPT'},\n",
       " {'paperId': 'a20b6b16dd92c195d8e8611a962584f5da0cd94e',\n",
       "  'title': 'Chinese Word Segmentation as LMR Tagging',\n",
       "  'authorId': '1702849',\n",
       "  'authorName': 'Nianwen Xue',\n",
       "  'abstract': 'In this paper we present Chinese word segmentation algorithms based on the so-called LMR tagging. Our LMR taggers are implemented with the Maximum Entropy Markov Model and we then use Transformation-Based Learning to combine the results of the two LMR taggers that scan the input in opposite directions. Our system achieves F-scores of 95.9% and 91.6% on the Academia Sinica corpus and the Hong Kong City University corpus respectively.',\n",
       "  'year': 2003,\n",
       "  'venue': 'SIGHAN'},\n",
       " {'paperId': '18b5e764e2158dc6ff7d19b0496dafcd3dd3b719',\n",
       "  'title': 'Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models — Is Single-Corpus Evaluation Enough?',\n",
       "  'authorId': '35643168',\n",
       "  'authorName': 'Masato Mita',\n",
       "  'abstract': 'This study explores the necessity of performing cross-corpora evaluation for grammatical error correction (GEC) models. GEC models have been previously evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark. However, the evaluation remains incomplete because the task difficulty varies depending on the test corpus and conditions such as the proficiency levels of the writers and essay topics. To overcome this limitation, we evaluate the performance of several GEC models, including NMT-based (LSTM, CNN, and transformer) and an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models’ rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '2af8a3517e26bf2ca77fbfeddad28d9403bca4c1',\n",
       "  'title': 'Advances in Debating Technologies: Building AI That Can Debate Humans',\n",
       "  'authorId': '1693525',\n",
       "  'authorName': 'Roy Bar-Haim',\n",
       "  'abstract': 'The tutorial focuses on Debating Technologies, a sub-field of computational argumentation defined as “computational technologies developed directly to enhance, support, and engage with human debating” (Gurevych et al., 2016). A recent milestone in this field is Project Debater, which was revealed in 2019 as the first AI system that can debate human experts on complex topics. Project Debater is the third in the series of IBM Research AI’s grand challenges, following Deep Blue and Watson. It has been developed for over six years by a large team of researchers and engineers, and its live demonstration in February 2019 received massive media attention. This research effort has resulted in more than 50 scientific papers to date, and many datasets freely available for research purposes. We discuss the scientific challenges that arise when building such a system, including argument mining, argument quality assessment, stance classification, principled argument detection, narrative generation, and rebutting a human opponent. Many of the underlying capabilities of Project Debater have been made freely available for academic research, and the tutorial will include a detailed explanation of how to use and leverage these tools. In addition to discussing individual components, the tutorial also provides a holistic view of a debating system. Such a view is largely missing in the academic literature, where each paper typically addresses a specific problem in isolation. We present a complete pipeline of a debating system, and discuss the information flow and the interaction between the various components. Finally, we discuss practical applications and future challenges of debating technologies.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'beb12fc82cdc04169214c89059b3c849fc0655f1',\n",
       "  'title': 'Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance',\n",
       "  'authorId': '50493332',\n",
       "  'authorName': 'Billy Chiu',\n",
       "  'abstract': 'The quality of word representations is frequently assessed using correlation with human judgements of word similarity. Here, we question whether such intrinsic evaluation can predict the merits of the representations for downstream tasks. We study the correlation between results on ten word similarity benchmarks and tagger performance on three standard sequence labeling tasks using a variety of word vectors induced from an unannotated corpus of 3.8 billion words, and demonstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study.',\n",
       "  'year': 2016,\n",
       "  'venue': 'RepEval@ACL'},\n",
       " {'paperId': '7850326fc472b525fe5118fa3f97da4e3e95204d',\n",
       "  'title': 'Bridging Information-Seeking Human Gaze and Machine Reading Comprehension',\n",
       "  'authorId': '3274291',\n",
       "  'authorName': 'J. Malmaud',\n",
       "  'abstract': 'In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking dataset with a large number of participants engaging in a multiple choice reading comprehension task. Our analysis of this data reveals increased fixation times over parts of the text that are most relevant for answering the question. Motivated by this finding, we propose making automated reading comprehension more human-like by mimicking human information-seeking reading behavior during reading comprehension. We demonstrate that this approach leads to performance gains on multiple choice question answering in English for a state-of-the-art reading comprehension model.',\n",
       "  'year': 2020,\n",
       "  'venue': 'CONLL'},\n",
       " {'paperId': '3068d60cd0aa24ad27b1cd7e430482360349aafb',\n",
       "  'title': 'ParFDA for Instance Selection for Statistical Machine Translation',\n",
       "  'authorId': '2237884',\n",
       "  'authorName': 'Ergun Biçici',\n",
       "  'abstract': 'We build parallel feature decay algorithms (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the translation task at the first conference on statistical machine translation (Bojar et al., 2016a) (WMT16). ParFDA obtains results close to the top constrained phrase-based SMT with an average of 2.52 BLEU points difference using significantly less computation for building SMT systems than the computation that would be spent using all available corpora. We obtain BLEU bounds based on target coverage and show that ParFDA results can be improved by 12.6 BLEU points on average. Similar bounds show that top constrained SMT results at WMT16 can be improved by 8 BLEU points on average while German to English and Romanian to English translations results are already close to the bounds. 1 ParFDA',\n",
       "  'year': 2016,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': 'fba28688404c090661d29cd8b90acf1cb74d959c',\n",
       "  'title': 'Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric',\n",
       "  'authorId': '2182290',\n",
       "  'authorName': 'N. Moosavi',\n",
       "  'abstract': 'Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identification effect in the B3, CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '0d74e4f02e74e4d81e6e8a47d7112e5182c4afa8',\n",
       "  'title': 'How (Non-)Optimal is the Lexicon?',\n",
       "  'authorId': '1388571351',\n",
       "  'authorName': 'Tiago Pimentel',\n",
       "  'abstract': 'The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf’s law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world’s languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon’s optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes—as measured by code length.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '55e3daa20b556c90afdc73b130fc43bec1ea6b83',\n",
       "  'title': 'Factoring Recursion and Dependencies: an Aspect of Tree Adjoining Grammars (Tag) and a Comparison of Some Formal Properties of Tags, GPSGs, Plgs, and LPGS',\n",
       "  'authorId': '1714374',\n",
       "  'authorName': 'A. Joshi',\n",
       "  'abstract': 'D u r i n g t h e l a s t few y e a r s t h e r e i s v i g o r o u s activity In constructing highly constrained grammatical systems by eliminating the transformational component either totally or partially. There is increasing recognition of t he f a c t t h a t the e n t i r e r a n g e o f d e p e n d e n c i e s t h a t t r a n s f o r m a t i o n a l g r a m m a r s i n t h e i r v a r i o u s i n c a r n a t i o n s have t r i e d t o a c c o u n t f o r c a n be satisfactorily captured by classes of rules that are non-transformational and at the same Clme highly constrlaned in terms of the classes of g r a m m a r s and l a n g u a g e s t h a t t h e y de f i n e .',\n",
       "  'year': 1983,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ea3928baba12de2fba9ce76e6804bfe50fe1cef3',\n",
       "  'title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation',\n",
       "  'authorId': '2621022',\n",
       "  'authorName': 'Ioannis Konstas',\n",
       "  'abstract': 'Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'bdf9d6ad58b11efbd337265b0c9b7819431fb198',\n",
       "  'title': 'LexNet: A Graphical Environment for Graph-Based NLP',\n",
       "  'authorId': '9215251',\n",
       "  'authorName': 'Dragomir R. Radev',\n",
       "  'abstract': 'This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan. LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification). All tools in the collection are based on random walks on lexical graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment.',\n",
       "  'year': 2006,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '8cf0a6d4890d73453f65b777fa96806aef039f17',\n",
       "  'title': 'Tricolor DAGs for Machine Translation',\n",
       "  'authorId': '2874038',\n",
       "  'authorName': 'Koichi Takeda',\n",
       "  'abstract': 'Machine translation (MT) has recently been formulated in terms of constraint-based knowledge representation and unification theories, but it is becoming more and more evident that it is not possible to design a practical MT system without an adequate method of handling mismatches between semantic representations in the source and target languages. In this paper, we introduce the idea of \"information-based\" MT, which is considerably more flexible than interlingual MT or the conventional transfer-based MT.',\n",
       "  'year': 1994,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'dcf34a415f95024bec0825048a515927e0ecb97a',\n",
       "  'title': 'Inferring Narrative Causality between Event Pairs in Films',\n",
       "  'authorId': '2111296747',\n",
       "  'authorName': 'Zhichao Hu',\n",
       "  'abstract': 'To understand narrative, humans draw inferences about the underlying relations between narrative events. Cognitive theories of narrative understanding define these inferences as four different types of causality, that include pairs of events A, B where A physically causes B (X drop, X break), to pairs of events where A causes emotional state B (Y saw X, Y felt fear). Previous work on learning narrative relations from text has either focused on “strict” physical causality, or has been vague about what relation is being learned. This paper learns pairs of causal events from a corpus of film scene descriptions which are action rich and tend to be told in chronological order. We show that event pairs induced using our methods are of high quality and are judged to have a stronger causal relation than event pairs from Rel-Grams.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '6ef94216e9845b0031cdbd81084e613acd673578',\n",
       "  'title': 'Modeling Document-Level Context for Event Detection via Important Context Selection',\n",
       "  'authorId': '3460489',\n",
       "  'authorName': 'Amir Pouran Ben Veyseh',\n",
       "  'abstract': 'The task of Event Detection (ED) in Information Extraction aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g., BERT) as a critical component in state-of-the-art models for ED. However, the length limit for input texts is a barrier for such ED models as they cannot encode long-range document-level context that has been shown to be beneficial for ED. To address this issue, we propose a novel method to model document-level context for ED that dynamically selects relevant sentences in the document for the event prediction of the target sentence. The target sentence will be then augmented with the selected sentences and consumed entirely by transformer-based language models for improved representation learning for ED. To this end, the REINFORCE algorithm is employed to train the relevant sentence selection for ED. Several information types are then introduced to form the reward function for the training process, including ED performance, sentence similarity, and discourse relations. Our extensive experiments on multiple benchmark datasets reveal the effectiveness of the proposed model, leading to new state-of-the-art performance.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '8520e718783fc940d32d1a73211d916d7f71b0d9',\n",
       "  'title': 'The First Komi-Zyrian Universal Dependencies Treebanks',\n",
       "  'authorId': '11267515',\n",
       "  'authorName': 'N. Partanen',\n",
       "  'abstract': 'Two Komi-Zyrian treebanks were included in the Universal Dependencies 2.2 release. This article contextualizes the treebanks, discusses the process through which they were created, and outlines the future plans and timeline for the next improvements. Special attention is paid to the possibilities of using UD in the documentation and description of endangered languages.',\n",
       "  'year': 2018,\n",
       "  'venue': 'UDW@EMNLP'},\n",
       " {'paperId': 'aa8349a06c21b6687fd607caac2b67920983fb99',\n",
       "  'title': 'Exploring the Relative Role of Bottom-up and Top-down Information in Phoneme Learning',\n",
       "  'authorId': '2676559',\n",
       "  'authorName': 'Abdellah Fourtassi',\n",
       "  'abstract': 'We test both bottom-up and top-down approaches in learning the phonemic status of the sounds of English and Japanese. We used large corpora of spontaneous speech to provide the learner with an input that models both the linguistic properties and statistical regularities of each language. We found both approaches to help discriminate between allophonic and phonemic contrasts with a high degree of accuracy, although top-down cues proved to be effective only on an interesting subset of the data.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca',\n",
       "  'title': 'Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections',\n",
       "  'authorId': '51011000',\n",
       "  'authorName': 'Ruiqi Zhong',\n",
       "  'abstract': 'Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can “prompt\" the LM with the review and the label description “Does the user like this movie?\", and ask whether the next word is “Yes\" or “No\". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by finetuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a samesized QA model and the previous SOTA zeroshot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-thebox might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '28a4b1dcfc8d375d9a22b320392505d3e185ff9f',\n",
       "  'title': 'Bag of What? Simple Noun Phrase Extraction for Text Analysis',\n",
       "  'authorId': '3422919',\n",
       "  'authorName': 'Abram Handler',\n",
       "  'abstract': 'Social scientists who do not have specialized natural language processing training often use a unigram bag-of-words (BOW) representation when analyzing text corpora. We offer a new phrase-based method, NPFST, for enriching a unigram BOW. NPFST uses a partof-speech tagger and a finite state transducer to extract multiword phrases to be added to a unigram BOW. We compare NPFST to both ngram and parsing methods in terms of yield, recall, and efficiency. We then demonstrate how to use NPFST for exploratory analyses; it performs well, without configuration, on many different kinds of English text. Finally, we present a case study using NPFST to analyze a new corpus of U.S. congressional bills.',\n",
       "  'year': 2016,\n",
       "  'venue': 'NLP+CSS@EMNLP'},\n",
       " {'paperId': '999b4b988180b9168d4fd4bdceaf421cd7f17096',\n",
       "  'title': 'MuST-C: a Multilingual Speech Translation Corpus',\n",
       "  'authorId': '39640268',\n",
       "  'authorName': 'Mattia Antonino Di Gangi',\n",
       "  'abstract': 'Current research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation. To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with a state-of-the-art approach on each language direction.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '157e646ce1e3bc9873db9d1c50a117100c1e7e47',\n",
       "  'title': 'Analyzing the Semantics of patient data to rank records of literature retrieval',\n",
       "  'authorId': '2075038',\n",
       "  'authorName': 'E. Mendonça',\n",
       "  'abstract': \"We describe the use of clinical data present in the medical record to determine the relevance of research evidence from literature databases. We studied the effect of using automated knowledge approaches as compared to physician's selection of articles, when using a traditional information retrieval system. Three methods were evaluated. The first method identified terms and their semantics and relationships in the patient's record to build a map of the record, which was represented in conceptual graph notation. This approach was applied to data in an individual's medical record and used to score citations retrieved using a graph matching algorithm. The second method identified associations between terms in the medical record, assigning them semantic types and weights based on the co-occurrence of these associations in citations of biomedical literature. The method was applied to data in an individual's medical record and used to score citations. The last method combined the first two. The results showed that physicians agreed better with each other than with the automated methods. However, we found a significant positive relation between physicians' selection of abstracts and two of the methods. We believe the results encourage the use of clinical data to determine the relevance of medical literature to the care of individual patients.\",\n",
       "  'year': 2002,\n",
       "  'venue': 'ACL Workshop on Natural Language Processing in the Biomedical Domain'},\n",
       " {'paperId': '448c377dfd28ceb9c1e55dbdc55872fb6e34ba57',\n",
       "  'title': 'Learning to Solve NLP Tasks in an Incremental Number of Languages',\n",
       "  'authorId': '3285152',\n",
       "  'authorName': 'Giuseppe Castellucci',\n",
       "  'abstract': 'In real scenarios, a multilingual model trained to solve NLP tasks on a set of languages can be required to support new languages over time. Unfortunately, the straightforward retraining on a dataset containing annotated examples for all the languages is both expensive and time-consuming, especially when the number of target languages grows. Moreover, the original annotated material may no longer be available due to storage or business constraints. Re-training only with the new language data will inevitably result in Catastrophic Forgetting of previously acquired knowledge. We propose a Continual Learning strategy that updates a model to support new languages over time, while maintaining consistent results on previously learned languages. We define a Teacher-Student framework where the existing model “teaches” to a student model its knowledge about the languages it supports, while the student is also trained on a new language. We report an experimental evaluation in several tasks including Sentence Classification, Relational Learning and Sequence Labeling.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '775b71d6667afc9e0ad29bcdad02239b15843693',\n",
       "  'title': 'UTMN at SemEval-2020 Task 11: A Kitchen Solution to Automatic Propaganda Detection',\n",
       "  'authorId': '14432985',\n",
       "  'authorName': 'Elena Mikhalkova',\n",
       "  'abstract': 'The article describes a fast solution to propaganda detection at SemEval-2020 Task 11, based on feature adjustment. We use per-token vectorization of features and a simple Logistic Regression classifier to quickly test different hypotheses about our data. We come up with what seems to us the best solution, however, we are unable to align it with the result of the metric suggested by the organizers of the task. We test how our system handles class and feature imbalance by varying the number of samples of two classes (Propaganda and None) in the training set, the size of a context window in which a token is vectorized and combination of vectorization means. The result of our system at SemEval2020 Task 11 is F-score=0.37.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': '2caa021d85d4878d3369000e0068f617576d6cca',\n",
       "  'title': 'Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog',\n",
       "  'authorId': '2150275',\n",
       "  'authorName': 'Satwik Kottur',\n",
       "  'abstract': 'A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6dbbade359d2830cf86691fd188e285c7571b41c',\n",
       "  'title': 'NICT-5’s Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages',\n",
       "  'authorId': '3209719',\n",
       "  'authorName': 'Raj Dabre',\n",
       "  'abstract': 'In this paper we describe our submission to the multilingual Indic language translation wtask “MultiIndicMT” under the team name “NICT-5”. This task involves translation from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.',\n",
       "  'year': 2021,\n",
       "  'venue': 'WAT'},\n",
       " {'paperId': '03098b52e3e60e137c75fcc464346bad9fe25dd4',\n",
       "  'title': 'Inducing Implicit Arguments from Comparable Texts: A Framework and Its Applications',\n",
       "  'authorId': '46617131',\n",
       "  'authorName': 'Michael Roth',\n",
       "  'abstract': 'In this article, we investigate aspects of sentential meaning that are not expressed in local predicate–argument structures. In particular, we examine instances of semantic arguments that are only inferable from discourse context. The goal of this work is to automatically acquire and process such instances, which we also refer to as implicit arguments, to improve computational models of language. As contributions towards this goal, we establish an effective framework for the difficult task of inducing implicit arguments and their antecedents in discourse and empirically demonstrate the importance of modeling this phenomenon in discourse-level tasks.Our framework builds upon a novel projection approach that allows for the accurate detection of implicit arguments by aligning and comparing predicate–argument structures across pairs of comparable texts. As part of this framework, we develop a graph-based model for predicate alignment that significantly outperforms previous approaches. Based on such alignments, we show that implicit argument instances can be automatically induced and applied to improve a current model of linking implicit arguments in discourse. We further validate that decisions on argument realization, although being a subtle phenomenon most of the time, can considerably affect the perceived coherence of a text. Our experiments reveal that previous models of coherence are not able to predict this impact. Consequently, we develop a novel coherence model, which learns to accurately predict argument realization based on automatically aligned pairs of implicit and explicit arguments.',\n",
       "  'year': 2015,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': '6ab4fbc78d6d20a8cf6d876b50fbfbffc17d6584',\n",
       "  'title': 'A Dynamic Speaker Model for Conversational Interactions',\n",
       "  'authorId': '47413820',\n",
       "  'authorName': 'Hao Cheng',\n",
       "  'abstract': 'Individual differences in speakers are reflected in their language use as well as in their interests and opinions. Characterizing these differences can be useful in human-computer interaction, as well as analysis of human-human conversations. In this work, we introduce a neural model for learning a dynamically updated speaker embedding in a conversational context. Initial model training is unsupervised, using context-sensitive language generation as an objective, with the context being the conversation history. Further fine-tuning can leverage task-dependent supervised training. The learned neural representation of speakers is shown to be useful for content ranking in a socialbot and dialog act prediction in human-human conversations.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '59ee2e2ed78da75c356ba063c0a88466ffb62391',\n",
       "  'title': 'MultiDM-GCN: Aspect-Guided Response Generation in Multi-Domain Multi-Modal Dialogue System using Graph Convolution Network',\n",
       "  'authorId': '40195882',\n",
       "  'authorName': 'Mauajama Firdaus',\n",
       "  'abstract': 'In the recent past, dialogue systems have gained immense popularity and have become ubiquitous. During conversations, humans not only rely on languages but seek contextual information through visual contents as well. In every task-oriented dialogue system, the user is guided by the different aspects of a product or service that regulates the conversation towards selecting the product or service. In this work, we present a multi-modal conversational framework for a task-oriented dialogue setup that generates the responses following the different aspects of a product or service to cater to the user’s needs. We show that the responses guided by the aspect information provide more interactive and informative responses for better communication between the agent and the user. We first create a Multi-domain Multi-modal Dialogue (MDMMD) dataset having conversations involving both text and images belonging to the three different domains, such as restaurants, electronics, and furniture. We implement a Graph Convolutional Network (GCN) based framework that generates appropriate textual responses from the multi-modal inputs. The multi-modal information having both textual and image representation is fed to the decoder and the aspect information for generating aspect guided responses. Quantitative and qualitative analyses show that the proposed methodology outperforms several baselines for the proposed task of aspect-guided response generation.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '907065d735cfa2c2f169dd01c668c00302c64297',\n",
       "  'title': 'Aligning Chinese-English Parallel Parse Trees: Is it Feasible?',\n",
       "  'authorId': '121137142',\n",
       "  'authorName': 'D. Deng',\n",
       "  'abstract': 'We investigate the feasibility of aligning Chinese and English parse trees by examining cases of incompatibility between Chinese-English parallel parse trees. This work is done in the context of an annotation project wherewe construct a parallel treebank by doingword and phrase alignments simultaneously. We discuss the most common incompatibility patterns identified within VPs and NPs and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself. This suggests that in principle it is feasible to align the parallel parse trees with somemodification of existing syntactic annotation guidelines. We believe this has implications for the use of parallel parse trees as an important resource for Machine Translation models.',\n",
       "  'year': 2014,\n",
       "  'venue': 'LAW@COLING'},\n",
       " {'paperId': '3d5eabf859c29670e3b91adf6f0193a8d7bc4981',\n",
       "  'title': 'Using an On-Line Dictionary to Find Rhyming Words and Pronunciations for Unknown Words',\n",
       "  'authorId': '11178191',\n",
       "  'authorName': 'R. Byrd',\n",
       "  'abstract': 'Humans know a great deal about relationships among words. This paper discusses relationships among word pronunciations. We describe a computer system which models human judgement of rhyme by assigning specific roles to the location of primary stress, the similarity of phonetic segments, and other factors. By using the model as an experimental tool, we expect to improve our understanding of rhyme. A related computer model will attempt to generate pronunciations for unknown words by analogy with those for known words. The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words. As in the case of rhyme, the computer model will be an important tool for improving our understanding of these processes. Both models serve as the basis for functions in the WordSmith automated dictionary system.',\n",
       "  'year': 1985,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '328d027a999efc9d9e315367f5e01096ef4e7255',\n",
       "  'title': 'COVID-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation',\n",
       "  'authorId': '1786863',\n",
       "  'authorName': 'Qingyun Wang',\n",
       "  'abstract': 'To combat COVID-19, both clinicians and scientists need to digest the vast amount of relevant biomedical knowledge in literature to understand the disease mechanism and the related biological functions. We have developed a novel and comprehensive knowledge discovery framework, COVID-KG to extract fine-grained multimedia knowledge elements (entities, relations and events) from scientific literature. We then exploit the constructed multimedia knowledge graphs (KGs) for question answering and report generation, using drug repurposing as a case study. Our framework also provides detailed contextual sentences, subfigures, and knowledge subgraphs as evidence. All of the data, KGs, reports.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '8db80de081890299c1bd6a6134cf1d7750fc80cf',\n",
       "  'title': 'Multilingual Agreement for Multilingual Neural Machine Translation',\n",
       "  'authorId': '2118801449',\n",
       "  'authorName': 'Jian Yang',\n",
       "  'abstract': 'Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '71bb7201cc2597cad880d9324a463aa1804854f4',\n",
       "  'title': 'TTCB System Description to a Shared Task on Implicit and Underspecified Language 2021',\n",
       "  'authorId': '2862582',\n",
       "  'authorName': 'Peratham Wiriyathammabhum',\n",
       "  'abstract': 'In this report, we describe our transformers for text classification baseline (TTCB) submissions to a shared task on implicit and underspecified language 2021. We cast the task of predicting revision requirements in collaboratively edited instructions as text classification. We considered transformer-based models which are the current state-of-the-art methods for text classification. We explored different training schemes, loss functions, and data augmentations. Our best result of 68.45% test accuracy (68.84% validation accuracy), however, consists of an XLNet model with a linear annealing scheduler and a cross-entropy loss. We do not observe any significant gain on any validation metric based on our various design choices except the MiniLM which has a higher validation F1 score and is faster to train by a half but also a lower validation accuracy score.',\n",
       "  'year': 2021,\n",
       "  'venue': 'UNIMPLICIT'},\n",
       " {'paperId': '20b847537d3b7b9661733c1770c5faab3c0e2215',\n",
       "  'title': 'Biomedical Named Entity Recognition with Multilingual BERT',\n",
       "  'authorId': '2216545',\n",
       "  'authorName': 'K. Hakala',\n",
       "  'abstract': 'We present the approach of the Turku NLP group to the PharmaCoNER task on Spanish biomedical named entity recognition. We apply a CRF-based baseline approach and multilingual BERT to the task, achieving an F-score of 88% on the development data and 87% on the test set with BERT. Our approach reflects a straightforward application of a state-of-the-art multilingual model that is not specifically tailored to either the language nor the application domain. The source code is available at: https://github.com/chaanim/pharmaconer',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '596c5016b4c6541419c20d72b7e7d4d238402bfe',\n",
       "  'title': 'AutoML Strategy Based on Grammatical Evolution: A Case Study about Knowledge Discovery from Text',\n",
       "  'authorId': '1390037280',\n",
       "  'authorName': 'Suilan Estévez-Velarde',\n",
       "  'abstract': 'The process of extracting knowledge from natural language text poses a complex problem that requires both a combination of machine learning techniques and proper feature selection. Recent advances in Automatic Machine Learning (AutoML) provide effective tools to explore large sets of algorithms, hyper-parameters and features to find out the most suitable combination of them. This paper proposes a novel AutoML strategy based on probabilistic grammatical evolution, which is evaluated on the health domain by facing the knowledge discovery challenge in Spanish text documents. Our approach achieves state-of-the-art results and provides interesting insights into the best combination of parameters and algorithms to use when dealing with this challenge. Source code is provided for the research community.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e6b0e3c2d42534b422e410a543fcd2a2d0726763',\n",
       "  'title': 'Correcting Grammatical Verb Errors',\n",
       "  'authorId': '2271568',\n",
       "  'authorName': 'A. Rozovskaya',\n",
       "  'abstract': 'Verb errors are some of the most common mistakes made by non-native writers of English but some of the least studied. The reason is that dealing with verb errors requires a new paradigm; essentially all research done on correcting grammatical errors assumes a closed set of triggers ‐ e.g., correcting the use of prepositions or articles ‐ but identifying mistakes in verbs necessitates identifying potentially ambiguous triggers first, and then determining the type of mistake made and correcting it. Moreover, once the verb is identified, modeling verb errors is challenging because verbs fulfill many grammatical functions, resulting in a variety of mistakes. Consequently, the little earlier work done on verb errors assumed that the error type is known in advance. We propose a linguistically-motivated approach to verb error correction that makes use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': 'bd6bca48d9440d49acbd26f2efff0629a2886c63',\n",
       "  'title': 'Unsupervised Domain Adaptation in Cross-corpora Abusive Language Detection',\n",
       "  'authorId': '1956965',\n",
       "  'authorName': 'Tulika Bose',\n",
       "  'abstract': 'The state-of-the-art abusive language detection models report great in-corpus performance, but underperform when evaluated on abusive comments that differ from the training scenario. As human annotation involves substantial time and effort, models that can adapt to newly collected comments can prove to be useful. In this paper, we investigate the effectiveness of several Unsupervised Domain Adaptation (UDA) approaches for the task of cross-corpora abusive language detection. In comparison, we adapt a variant of the BERT model, trained on large-scale abusive comments, using Masked Language Model (MLM) fine-tuning. Our evaluation shows that the UDA approaches result in sub-optimal performance, while the MLM fine-tuning does better in the cross-corpora setting. Detailed analysis reveals the limitations of the UDA approaches and emphasizes the need to build efficient adaptation methods for this task.',\n",
       "  'year': 2021,\n",
       "  'venue': 'SOCIALNLP'},\n",
       " {'paperId': '17972be63441aff7ff2dc6e941d6ca7c935bb339',\n",
       "  'title': 'Domain-Aware Dependency Parsing for Questions',\n",
       "  'authorId': '31099365',\n",
       "  'authorName': 'Aparna Garimella',\n",
       "  'abstract': 'Parsing natural language questions in specific domains is crucial to a wide range of applications from question-answering to dialog systems. Pre-trained parsers are usually trained on corpora dominated by non-questions, and thus perform poorly on domain-specific questions. Retraining parsers with domain-specific questions labeled with syntactic parse trees is expensive, as these annotations require linguistic expertise. In this paper, we propose an automatic labeled domain question generation framework by leveraging domain knowledge and seed domain questions. We evaluate our approach in two domains, and release the generated question datasets. Our experimental results demonstrate that auto-generated labeled questions indeed lead to significant (4.9% − 9%) increase in the accuracy of state-of-the-art (SoTA) parsers on domain questions.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '88dbde378e9ac5c25fc7d78f5da147223e8d34d4',\n",
       "  'title': 'Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention',\n",
       "  'authorId': '38253388',\n",
       "  'authorName': 'Pengcheng Yin',\n",
       "  'abstract': 'We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'a5fa8ba210af198c447464f62aec91b1357442ba',\n",
       "  'title': 'A Weakly Supervised Approach to Train Temporal Relation Classifiers and Acquire Regular Event Pairs Simultaneously',\n",
       "  'authorId': '9185405',\n",
       "  'authorName': 'Wenlin Yao',\n",
       "  'abstract': 'Capabilities of detecting temporal and causal relations between two events can benefit many applications. Most of existing temporal relation classifiers were trained in a supervised manner. Instead, we explore the observation that regular event pairs show a consistent temporal relation despite of their various contexts and these rich contexts can be used to train a contextual temporal relation classifier, which can further recognize new temporal relation contexts and identify new regular event pairs. We focus on detecting after and before temporal relations and design a weakly supervised learning approach that extracts thousands of regular event pairs and learns a contextual temporal relation classifier simultaneously. Evaluation shows that the acquired regular event pairs are of high quality and contain rich commonsense knowledge and domain specific knowledge. In addition, the weakly supervised trained temporal relation classifier achieves comparable performance with the state-of-the-art supervised systems.',\n",
       "  'year': 2017,\n",
       "  'venue': 'RANLP'},\n",
       " {'paperId': 'fd669888fba73b8d6340a349a802cdd6fb6cd7e0',\n",
       "  'title': 'TeamZ: Measuring Semantic Textual Similarity for Spanish Using an Overlap-Based Approach',\n",
       "  'authorId': '2110761762',\n",
       "  'authorName': 'Anubhav Gupta',\n",
       "  'abstract': 'This paper presents an overlap-based approach using bag of words and the Spanish WordNet to solve the STS-Spanish subtask (STS-Es) of SemEval-2014 Task 10. Since bag of words is the most commonly used method to ascertain similarity, the performance is modest.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '5387fd57d093ba95675453c38988fb6427bf0a5d',\n",
       "  'title': 'Understanding of unknown medical words',\n",
       "  'authorId': '1732198',\n",
       "  'authorName': 'N. Grabar',\n",
       "  'abstract': 'We assume that unknown words with internal structure (affixed words or compounds) can provide speakers with linguistic cues as for their meaning, and thus help their decoding and understanding. To verify this hypothesis, we propose to work with a set of French medical words. These words are annotated by five annotators. Then, two kinds of analysis are performed: analysis of the evolution of understandable and non-understandable words (globally and according to some suffixes) and anal- \\nysis of clusters created with unsupervised algorithms on basis of linguistic and extra-linguistic features of the studied words. Our results suggest that, according to linguistic sensitivity of annotators, technical words can be decoded and become understandable. As for the clusters, some of \\nthem distinguish between understandable and non-understandable words. Resources built in this work will be made freely available for the research purposes.',\n",
       "  'year': 2017,\n",
       "  'venue': 'BiomedicalNLP@RANLP'},\n",
       " {'paperId': '3292c859d9e8ad23e638199b4b3673a97c6de955',\n",
       "  'title': 'Thematic fit evaluation: an aspect of selectional preferences',\n",
       "  'authorId': '145017437',\n",
       "  'authorName': 'A. Sayeed',\n",
       "  'abstract': 'In this paper, we discuss the human thematic fit judgement correlation task in the context of real-valued vector space word representations. Thematic fit is the extent to which an argument fulfils the selectional preference of a verb given a role: for example, how well “cake” fulfils the patient role of “cut”. In recent work, systems have been evaluated on this task by finding the correlations of their output judgements with human-collected judgement data. This task is a representationindependent way of evaluating models that can be applied whenever a system score can be generated, and it is applicable wherever predicate-argument relations are significant to performance in end-user tasks. Significant progress has been made on this cognitive modeling task, leaving considerable space for future, more comprehensive types of evaluation.',\n",
       "  'year': 2016,\n",
       "  'venue': 'RepEval@ACL'},\n",
       " {'paperId': 'af2bfdedece5305d4e7bfa31b33d1fc7f7f271b2',\n",
       "  'title': 'TALN-UPF: Taxonomy Learning Exploiting CRF-Based Hypernym Extraction on Encyclopedic Definitions',\n",
       "  'authorId': '2254466',\n",
       "  'authorName': 'Luis Espinosa Anke',\n",
       "  'abstract': 'This paper describes the system submitted by the TALN-UPF team to SEMEVAL Task 17 (Taxonomy Extraction Evaluation). We present a method for automatically learning a taxonomy from a flat terminology, which benefits from a definition corpus obtained by querying the BabelNet semantic network. Then, we combine a machine-learning algorithm for term-hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '46b962cf0ada07fe7894c9f160c8a9b2289eb0c2',\n",
       "  'title': 'POS-tagging of Tunisian Dialect Using Standard Arabic Resources and Tools',\n",
       "  'authorId': '2924500',\n",
       "  'authorName': 'Ahmed Hamdi',\n",
       "  'abstract': 'Developing natural language processing tools usually requires a large number of resources (lexica, annotated corpora, etc.), which often do not exist for less-resourced languages. One way to overcome the problem of lack of resources is to devote substantial efforts to build new ones from scratch. Another approach is to exploit existing resources of closely related languages. In this paper, we focus on developing a part-of-speech tagger for the Tunisian Arabic dialect (TUN), a low-resource language, by exploiting its close-ness to Modern Standard Arabic (MSA), which has many state-of-the-art resources and tools. Our system achieved an accuracy of 89% (∼20% absolute improvement over an MSA tagger baseline).',\n",
       "  'year': 2015,\n",
       "  'venue': 'ANLP@ACL'},\n",
       " {'paperId': '484e13110733514f159f5950633cda66b07e8733',\n",
       "  'title': 'Automatic disambiguation of English puns',\n",
       "  'authorId': '1818919',\n",
       "  'authorName': 'Tristan Miller',\n",
       "  'abstract': 'Traditional approaches to word sense disambiguation (WSD) rest on the assumption that there exists a single, unambiguous communicative intention underlying every word in a document. However, writers sometimes intend for a word to be interpreted as simultaneously carrying multiple distinct meanings. This deliberate use of lexical ambiguity---i.e., punning---is a particularly common source of humour. In this paper we describe how traditional, language-agnostic WSD approaches can be adapted to \"disambiguate\" puns, or rather to identify their double meanings. We evaluate several such approaches on a manually sense-annotated corpus of English puns and observe performance exceeding that of some knowledge-based and supervised baselines.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '2b4b852dba463878235f13529248d2efd25902f7',\n",
       "  'title': 'A Task-based Framework to Evaluate Evaluative Arguments',\n",
       "  'authorId': '1825424',\n",
       "  'authorName': 'G. Carenini',\n",
       "  'abstract': 'We present an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users. The framework is based on the task-efficacy evaluation method. An evaluative argument is presented in the context of a decision task and measures related to its effectiveness are assessed. Within this framework, we are currently running a formal experiment to verify whether argument effectiveness can be increased by tailoring the argument to the user and by varying the degree of argument conciseness.',\n",
       "  'year': 2000,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': 'ec9de84c0ea2deb72eaba7115d8101ccec071af9',\n",
       "  'title': 'Continuous Space Representations of Linguistic Typology and their Application to Phylogenetic Inference',\n",
       "  'authorId': '2606962',\n",
       "  'authorName': 'Yugo Murawaki',\n",
       "  'abstract': 'For phylogenetic inference, linguistic typology is a promising alternative to lexical evidence because it allows us to compare an arbitrary pair of languages. A challenging problem with typology-based phylogenetic inference is that the changes of typological features over time are less intuitive than those of lexical features. In this paper, we work on reconstructing typologically natural ancestors To do this, we leverage dependencies among typological features. We first represent each language by continuous latent components that capture feature dependencies. We then combine them with a typology evaluator that distinguishes typologically natural languages from other possible combinations of features. We perform phylogenetic inference in the continuous space and use the evaluator to ensure the typological naturalness of inferred ancestors. We show that the proposed method reconstructs known language families more accurately than baseline methods. Lastly, assuming the monogenesis hypothesis, we attempt to reconstruct a common ancestor of the world’s languages.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'c6dbd97f458c76925363a6b8f6c5e6198163e54e',\n",
       "  'title': 'The Grammar-Learning Trajectories of Neural Language Models',\n",
       "  'authorId': '41019330',\n",
       "  'authorName': 'Leshem Choshen',\n",
       "  'abstract': 'The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models’ learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent “developmental” stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '8e82dd83df5023df86868c59a03fd7872fb5931e',\n",
       "  'title': 'Attention Is (not) All You Need for Commonsense Reasoning',\n",
       "  'authorId': '35660331',\n",
       "  'authorName': 'T. Klein',\n",
       "  'abstract': 'The recently introduced BERT model exhibits strong performance on several language understanding benchmarks. In this paper, we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a225aab16b852dd2cc12cf72749ff9609656389b',\n",
       "  'title': 'Improving Multilingual Neural Machine Translation with Auxiliary Source Languages',\n",
       "  'authorId': '47210642',\n",
       "  'authorName': 'Weijia Xu',\n",
       "  'abstract': 'Multilingual neural machine translation models typically handle one source language at a time. However, prior work has shown that translating from multiple source languages improves translation quality. Different from existing approaches on multi-source translation that are limited to the test scenario where parallel source sentences from multiple languages are available at inference time, we propose to improve multilingual translation in a more common scenario by exploiting synthetic source sentences from auxiliary languages. We train our model on synthetic multi-source corpora and apply random masking to enable flexible inference with single-source or bi-source inputs. Extensive experiments on Chinese/English→Japanese and a large-scale multilingual translation benchmark show that our model outperforms the multilingual baseline significantly by up to +4.0 BLEU with the largest improvements on low-resource or distant language pairs.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'de646c58e6ea552646639c9357352bb93083eb0c',\n",
       "  'title': 'Graph-Based Collective Lexical Selection for Statistical Machine Translation',\n",
       "  'authorId': '34739384',\n",
       "  'authorName': 'Jinsong Su',\n",
       "  'abstract': 'Lexical selection is of great importance to statistical machine translation. In this paper, we propose a graph-based framework for collective lexical selection. The framework is established on a translation graph that captures not only local associations between source-side content words and their target translations but also targetside global dependencies in terms of relatedness among target items. We also introduce a random walk style algorithm to collectively identify translations of sourceside content words that are strongly related in translation graph. We validate the effectiveness of our lexical selection framework on Chinese-English translation. Experiment results with large-scale training data show that our approach significantly improves lexical selection.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '4302e981e3ec118b68e0b3fcf1820b3f6ecfa988',\n",
       "  'title': 'Argumentation Quality Assessment: Theory vs. Practice',\n",
       "  'authorId': '2626599',\n",
       "  'authorName': 'Henning Wachsmuth',\n",
       "  'abstract': 'Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'b05f33c5db8a0a36830629e5b285785a58fbfbb2',\n",
       "  'title': 'Classifying ReachOut posts with a radial basis function SVM',\n",
       "  'authorId': '1895292',\n",
       "  'authorName': 'Chris Brew',\n",
       "  'abstract': 'The ReachOut clinical psychology shared task challenge addresses the problem of providing an automatic triage for posts to a support forum for people with a history of mental health issues. Posts are classified into green, amber, red and crisis. The non-green categories correspond to increasing levels of urgency for some form of intervention. The Thomson Reuters submissions arose from an idea about self-training and ensemble learning. The available labeled training set is small (947 examples) and the class distribution unbalanced. It was therefore hoped to develop a method that would make use of the larger dataset of unlabeled posts provided by the organisers. This did not work, but the performance of a radial basis function SVM intended as a baseline was relatively good. Therefore, the report focuses on the latter, aiming to understand the reasons for its performance.',\n",
       "  'year': 2016,\n",
       "  'venue': 'CLPsych@HLT-NAACL'},\n",
       " {'paperId': '6d981d56d029b42822be5cd2af6e7a08433240c9',\n",
       "  'title': 'Noise Pollution in Hospital Readmission Prediction: Long Document Classification with Reinforcement Learning',\n",
       "  'authorId': '47775705',\n",
       "  'authorName': 'Liyan Xu',\n",
       "  'abstract': 'This paper presents a reinforcement learning approach to extract noise in long clinical documents for the task of readmission prediction after kidney transplant. We face the challenges of developing robust models on a small dataset where each document may consist of over 10K tokens with full of noise including tabular text and task-irrelevant sentences. We first experiment four types of encoders to empirically decide the best document representation, and then apply reinforcement learning to remove noisy text from the long documents, which models the noise extraction process as a sequential decision problem. Our results show that the old bag-of-words encoder outperforms deep learning-based encoders on this task, and reinforcement learning is able to improve upon baseline while pruning out 25% text segments. Our analysis depicts that reinforcement learning is able to identify both typical noisy tokens and task-specific noisy text.',\n",
       "  'year': 2020,\n",
       "  'venue': 'BIONLP'},\n",
       " {'paperId': '0ae3a217443ccbe9041d7295a6833cb6258f4c74',\n",
       "  'title': 'TeamDL at SemEval-2018 Task 8: Cybersecurity Text Analysis using Convolutional Neural Network and Conditional Random Fields',\n",
       "  'authorId': '147577950',\n",
       "  'authorName': 'Manikandan Ravikiran',\n",
       "  'abstract': 'In this work we present our participation to SemEval-2018 Task 8 subtasks 1 & 2 respectively. We developed Convolution Neural Network system for malware sentence classification (subtask 1) and Conditional Random Fields system for malware token label prediction (subtask 2). We experimented with couple of word embedding strategies, feature sets and achieved competitive performance across the two subtasks. For subtask 1 We experimented with two category of word embeddings namely native embeddings and task specific embedding using Word2vec and Glove algorithms. 1. Native Embeddings: All words including the unknown ones that are randomly initialized use embeddings from original Word2vec/Glove models. 2. Task specific : The embeddings are generated by training Word2vec/Glove algorithms on sentences from MalwareTextDB We found that glove outperforms rest of embeddings for subtask 1. For subtask 2, we used N-grams of size 6, previous, next tokens and labels, features giving disjunctions of words anywhere in the left or right, word shape features, word lemma of current, previous and next words, word-tag pair features, POS tags, prefix and suffixes.',\n",
       "  'year': 2018,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '9992c25cd9c9de0cc8e11c079d669239915571bb',\n",
       "  'title': 'Generating Feedback for English Foreign Language Exercises',\n",
       "  'authorId': '2582384',\n",
       "  'authorName': 'Björn Rudzewitz',\n",
       "  'abstract': 'While immediate feedback on learner language is often discussed in the Second Language Acquisition literature (e.g., Mackey 2006), few systems used in real-life educational settings provide helpful, metalinguistic feedback to learners. In this paper, we present a novel approach leveraging task information to generate the expected range of well-formed and ill-formed variability in learner answers along with the required diagnosis and feedback. We combine this offline generation approach with an online component that matches the actual student answers against the pre-computed hypotheses. The results obtained for a set of 33 thousand answers of 7th grade German high school students learning English show that the approach successfully covers frequent answer patterns. At the same time, paraphrases and content errors require a more flexible alignment approach, for which we are planning to complement the method with the CoMiC approach successfully used for the analysis of reading comprehension answers (Meurers et al., 2011).',\n",
       "  'year': 2018,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': '638b60fc1ba22bea6c9d6523d3b8dd8204057423',\n",
       "  'title': 'Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants',\n",
       "  'authorId': '153408953',\n",
       "  'authorName': 'Max Bartolo',\n",
       "  'abstract': 'In Dynamic Adversarial Data Collection (DADC), human annotators are tasked with finding examples that models struggle to predict correctly. Models trained on DADC-collected training data have been shown to be more robust in adversarial and out-of-domain settings, and are considerably harder for humans to fool. However, DADC is more time-consuming than traditional data collection and thus more costly per annotated example. In this work, we examine whether we can maintain the advantages of DADC, without incurring the additional cost. To that end, we introduce Generative Annotation Assistants (GAAs), generator-in-the-loop models that provide real-time suggestions that annotators can either approve, modify, or reject entirely. We collect training datasets in twenty experimental settings and perform a detailed analysis of this approach for the task of extractive question answering (QA) for both standard and adversarial data collection. We demonstrate that GAAs provide significant efficiency benefits with over a 30% annotation speed-up, while leading to over a 5x improvement in model fooling rates. In addition, we find that using GAA-assisted training data leads to higher downstream model performance on a variety of question answering tasks over adversarial data collection.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '2e2147ca34e726f568fb28089424d2d97cb8b7f5',\n",
       "  'title': 'Learning to Rank Visual Stories From Human Ranking Data',\n",
       "  'authorId': '48162772',\n",
       "  'authorName': 'Chi-Yang Hsu',\n",
       "  'abstract': 'Visual storytelling (VIST) is a typical vision and language task that has seen extensive development in the natural language generation research domain. However, it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST. In this paper, we present the VHED (VIST Human Evaluation Data) dataset, which first re-purposes human evaluation results for automatic evaluation; hence we develop Vrank (VIST Ranker), a novel reference-free VIST metric for story evaluation. We first show that the results from commonly adopted automatic metrics for text generation have little correlation with those obtained from human evaluation, which motivates us to directly utilize human evaluation results to learn the automatic evaluation model. In the experiments, we evaluate the generated texts to predict story ranks using our model as well as other reference-based and reference-free metrics. Results show that Vrank prediction is significantly more aligned to human evaluation than other metrics with almost 30% higher accuracy when ranking story pairs. Moreover, we demonstrate that only Vrank shows human-like behavior in its strong ability to find better stories when the quality gap between two stories is high. Finally, we show the superiority of Vrank by its generalizability to pure textual stories, and conclude that this reuse of human evaluation results puts Vrank in a strong position for continued future advances.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'b46e09643335ed519d411edfc68ad4f44d000f2c',\n",
       "  'title': 'fairseq Sˆ2: A Scalable and Integrable Speech Synthesis Toolkit',\n",
       "  'authorId': '20132361',\n",
       "  'authorName': 'Changhan Wang',\n",
       "  'abstract': 'This paper presents fairseq Sˆ2, a fairseq extension for speech synthesis. We implement a number of autoregressive (AR) and non-AR text-to-speech models, and their multi-speaker variants. To enable training speech synthesis models with less curated data, a number of preprocessing tools are built and their importance is shown empirically. To facilitate faster iteration of development and analysis, a suite of automatic metrics is included. Apart from the features added specifically for this extension, fairseq Sˆ2 also benefits from the scalability offered by fairseq and can be easily integrated with other state-of-the-art systems provided in this framework. The code, documentation, and pre-trained models will be made available at https://github.com/pytorch/fairseq/tree/master/examples/speech_synthesis.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b44c87051f0b53fbe101933007b4328cdeccac12',\n",
       "  'title': 'Care Episode Retrieval',\n",
       "  'authorId': '40633376',\n",
       "  'authorName': 'Hans Moen',\n",
       "  'abstract': 'The documentation of a care episode consists of clinical notes concerning patient care, concluded with a discharge summary. Care episodes are stored electronically and used throughout the health care sector by patients, administrators and professionals from different areas, primarily for clinical purposes, but also for secondary purposes such as decision support and research. A common use case is, given a – possibly unfinished – care episode, to retrieve the most similar care episodes among the records. This paper presents several methods for information retrieval, focusing on care episode retrieval, based on textual similarity, where similarity is measured through domain-specific modelling of the distributional semantics of words. Models include variants of random indexing and a semantic neural network model called word2vec. A novel method is introduced that utilizes the ICD-10 codes attached to care episodes to better induce domain-specificity in the semantic model. We report on an experimental evaluation of care episode retrieval that circumvents the lack of human judgements regarding episode relevance by exploiting (1) ICD10 codes of care episodes and (2) semantic similarity between their discharge summaries. Results suggest that several of the methods proposed outperform a state-ofthe art search engine (Lucene) on the retrieval task.',\n",
       "  'year': 2014,\n",
       "  'venue': 'Louhi@EACL'},\n",
       " {'paperId': '82f2b83658a315fd1002b0721dda74d04c994e6a',\n",
       "  'title': 'Unfolding and Shrinking Neural Machine Translation Ensembles',\n",
       "  'authorId': '48404632',\n",
       "  'authorName': 'Felix Stahlberg',\n",
       "  'abstract': 'Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '4e6dc15cbcf9b175a8e294dd51ec7c6fe82020b3',\n",
       "  'title': 'Question Generation from SQL Queries Improves Neural Semantic Parsing',\n",
       "  'authorId': '51223794',\n",
       "  'authorName': 'Daya Guo',\n",
       "  'abstract': 'In this paper, we study how to learn a semantic parser of state-of-the-art accuracy with less supervised training data. We conduct our study on WikiSQL, the largest hand-annotated semantic parsing dataset to date. First, we demonstrate that question generation is an effective method that empowers us to learn a state-of-the-art neural network based semantic parser with thirty percent of the supervised training data. Second, we show that applying question generation to the full supervised training data further improves the state-of-the-art model. In addition, we observe that there is a logarithmic relationship between the accuracy of a semantic parser and the amount of training data.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'e7a2f80446d82b757c1bd994c4f40cfc6f3a53fc',\n",
       "  'title': 'MetaWeighting: Learning to Weight Tasks in Multi-Task Learning',\n",
       "  'authorId': '1753922779',\n",
       "  'authorName': 'Yuren Mao',\n",
       "  'abstract': 'Task weighting, which assigns weights on the including tasks during training, significantly matters the performance of Multi-task Learning (MTL); thus, recently, there has been an explosive interest in it. However, existing task weighting methods assign weights only based on the training loss, while ignoring the gap between the training loss and generalization loss. It degenerates MTL’s performance. To address this issue, the present paper proposes a novel task weighting algorithm, which automatically weights the tasks via a learning-to-learn paradigm, referred to as MetaWeighting. Extensive experiments are conducted to validate the superiority of our proposed method in multi-task text classification.',\n",
       "  'year': 2022,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'f86f7fc6af6f15329e0867f0513f20063673adf8',\n",
       "  'title': 'GCPG: A General Framework for Controllable Paraphrase Generation',\n",
       "  'authorId': '2004672900',\n",
       "  'authorName': 'Kexin Yang',\n",
       "  'abstract': 'Controllable paraphrase generation (CPG) incorporates various external conditions to obtain desirable paraphrases. However, existing works only highlight a special condition under two indispensable aspects of CPG (i.e., lexically and syntactically CPG) individually, lacking a unified circumstance to explore and analyze their effectiveness. In this paper, we propose a general controllable paraphrase generation framework (GCPG), which represents both lexical and syntactical conditions as text sequences and uniformly processes them in an encoder-decoder paradigm. Under GCPG, we reconstruct commonly adopted lexical condition (i.e., Keywords) and syntactical conditions (i.e., Part-Of-Speech sequence, Constituent Tree, Masked Template and Sentential Exemplar) and study the combination of the two types. In particular, for Sentential Exemplar condition, we propose a novel exemplar construction method — Syntax-Similarity based Exemplar (SSE). SSE retrieves a syntactically similar but lexically different sentence as the exemplar for each target sentence, avoiding exemplar-side words copying problem. Extensive experiments demonstrate that GCPG with SSE achieves state-of-the-art performance on two popular benchmarks. In addition, the combination of lexical and syntactical conditions shows the significant controllable ability of paraphrase generation, and these empirical results could provide novel insight to user-oriented paraphrasing.',\n",
       "  'year': 2022,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '4d5faafe1b839a2b3f280555ddbb6288b02e01ae',\n",
       "  'title': 'Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning',\n",
       "  'authorId': '2715551',\n",
       "  'authorName': 'Zheng-Yu Niu',\n",
       "  'abstract': 'Shortage of manually sense-tagged data is an obstacle to supervised word sense disambiguation methods. In this paper we investigate a label propagation based semi-supervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping.',\n",
       "  'year': 2005,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '912ba9757feb259b02a2b96d6e0e5a0863bc8a53',\n",
       "  'title': 'A Large-scale Evaluation of Neural Machine Transliteration for Indic Languages',\n",
       "  'authorId': '1711973',\n",
       "  'authorName': 'Anoop Kunchukuttan',\n",
       "  'abstract': 'We take up the task of large-scale evaluation of neural machine transliteration between English and Indic languages, with a focus on multilingual transliteration to utilize orthographic similarity between Indian languages. We create a corpus of 600K word pairs mined from parallel translation corpora and monolingual corpora, which is the largest transliteration corpora for Indian languages mined from public sources. We perform a detailed analysis of multilingual transliteration and propose an improved multilingual training recipe for Indic languages. We analyze various factors affecting transliteration quality like language family, transliteration direction and word origin.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': 'f7d6f0306f363231f307c9ab28b38d53ec1019e7',\n",
       "  'title': 'TTCS^{\\\\mathcal{E}}: a Vectorial Resource for Computing Conceptual Similarity',\n",
       "  'authorId': '3328466',\n",
       "  'authorName': 'E. Mensa',\n",
       "  'abstract': 'In this paper we introduce the TTCSE , a linguistic resource that relies on BabelNet, NASARI and ConceptNet, that has now been used to compute the conceptual similarity between concept pairs. The conceptual representation herein provides uniform access to concepts based on BabelNet synset IDs, and consists of a vectorbased semantic representation which is compliant with the Conceptual Spaces, a geometric framework for common-sense knowledge representation and reasoning. The TTCSE has been evaluated in a preliminary experimentation on a conceptual',\n",
       "  'year': 2017,\n",
       "  'venue': ''},\n",
       " {'paperId': 'b75329489baf067e6f7bbb74f16ffd49fba80dfa',\n",
       "  'title': 'Freebase QA: Information Extraction or Semantic Parsing?',\n",
       "  'authorId': '33219438',\n",
       "  'authorName': 'Xuchen Yao',\n",
       "  'abstract': 'We contrast two seemingly distinct approaches to the task of question answering (QA) using Freebase: one based on information extraction techniques, the other on semantic parsing. Results over the same test-set were collected from two state-ofthe-art, open-source systems, then analyzed in consultation with those systems’ creators. We conclude that the differences between these technologies, both in task performance, and in how they get there, is not significant. This suggests that the semantic parsing community should target answering more compositional open-domain questions that are beyond the reach of more direct information extraction methods.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL 2014'},\n",
       " {'paperId': 'e889e2dd1647fdfc28edb9abe08a863b704f08be',\n",
       "  'title': 'Using Intermediate Representations to Solve Math Word Problems',\n",
       "  'authorId': '2110406322',\n",
       "  'authorName': 'Danqing Huang',\n",
       "  'abstract': 'To solve math word problems, previous statistical approaches attempt at learning a direct mapping from a problem description to its corresponding equation system. However, such mappings do not include the information of a few higher-order operations that cannot be explicitly represented in equations but are required to solve the problem. The gap between natural language and equations makes it difficult for a learned model to generalize from limited data. In this work we present an intermediate meaning representation scheme that tries to reduce this gap. We use a sequence-to-sequence model with a novel attention regularization term to generate the intermediate forms, then execute them to obtain the final answers. Since the intermediate forms are latent, we propose an iterative labeling framework for learning by leveraging supervision signals from both equations and answers. Our experiments show using intermediate forms outperforms directly predicting equations.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '55ff1999a059ddc90e5f797dbbbb802989971ba9',\n",
       "  'title': 'Finding Friends and Flipping Frenemies: Automatic Paraphrase Dataset Augmentation Using Graph Theory',\n",
       "  'authorId': '1876074899',\n",
       "  'authorName': 'Hannah Chen',\n",
       "  'abstract': 'Most NLP datasets are manually labeled, so suffer from inconsistent labeling or limited size. We propose methods for automatically improving datasets by viewing them as graphs with expected semantic properties. We construct a paraphrase graph from the provided sentence pair labels, and create an augmented dataset by directly inferring labels from the original sentence pairs using a transitivity property. We use structural balance theory to identify likely mislabelings in the graph, and flip their labels. We evaluate our methods on paraphrase models trained using these datasets starting from a pretrained BERT model, and find that the automatically-enhanced training sets result in more accurate models.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '57e75eb6ae68d550e684d99a2ecd09786c0d563d',\n",
       "  'title': 'Simultaneous Translation and Paraphrase for Language Education',\n",
       "  'authorId': '2153374',\n",
       "  'authorName': 'Stephen Mayhew',\n",
       "  'abstract': 'We present the task of Simultaneous Translation and Paraphrasing for Language Education (STAPLE). Given a prompt in one language, the goal is to generate a diverse set of correct translations that language learners are likely to produce. This is motivated by the need to create and maintain large, high-quality sets of acceptable translations for exercises in a language-learning application, and synthesizes work spanning machine translation, MT evaluation, automatic paraphrasing, and language education technology. We developed a novel corpus with unique properties for five languages (Hungarian, Japanese, Korean, Portuguese, and Vietnamese), and report on the results of a shared task challenge which attracted 20 teams to solve the task. In our meta-analysis, we focus on three aspects of the resulting systems: external training corpus selection, model architecture and training decisions, and decoding and filtering strategies. We find that strong systems start with a large amount of generic training data, and then fine-tune with in-domain data, sampled according to our provided learner response frequencies.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NGT'},\n",
       " {'paperId': '76037594f29a663fbd2799de2e5c7463c02a8a1d',\n",
       "  'title': 'Discourse-Aware Neural Extractive Text Summarization',\n",
       "  'authorId': '34837371',\n",
       "  'authorName': 'Jiacheng Xu',\n",
       "  'abstract': 'Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1479f952bcc1aea9f4c5edfeca69c18274b68dc0',\n",
       "  'title': 'Modeling Morphological Typology for Unsupervised Learning of Language Morphology',\n",
       "  'authorId': '49506999',\n",
       "  'authorName': 'Hongzhi Xu',\n",
       "  'abstract': 'This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology. By modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology. The system is tested on nine typologically and genetically diverse languages, and shows superior performance over leading systems. We also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd9b824dbecbe3a1f0b1489f9e4521a532a63818d',\n",
       "  'title': 'Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning',\n",
       "  'authorId': '1452684657',\n",
       "  'authorName': 'Mitchell A. Gordon',\n",
       "  'abstract': 'Pre-trained universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.',\n",
       "  'year': 2020,\n",
       "  'venue': 'REPL4NLP'},\n",
       " {'paperId': '8d3feb186556ead77590979b42c42374549a1166',\n",
       "  'title': 'Earth Mover’s Distance Minimization for Unsupervised Bilingual Lexicon Induction',\n",
       "  'authorId': '48985275',\n",
       "  'authorName': 'Meng Zhang',\n",
       "  'abstract': 'Cross-lingual natural language processing hinges on the premise that there exists invariance across languages. At the word level, researchers have identified such invariance in the word embedding semantic spaces of different languages. However, in order to connect the separate spaces, cross-lingual supervision encoded in parallel data is typically required. In this paper, we attempt to establish the cross-lingual connection without relying on any cross-lingual supervision. By viewing word embedding spaces as distributions, we propose to minimize their earth mover’s distance, a measure of divergence between distributions. We demonstrate the success on the unsupervised bilingual lexicon induction task. In addition, we reveal an interesting finding that the earth mover’s distance shows potential as a measure of language difference.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '99a2a9e956b24c62222a20d63644d1c1edb838a5',\n",
       "  'title': 'NLM at MEDIQA 2021: Transfer Learning-based Approaches for Consumer Question and Multi-Answer Summarization',\n",
       "  'authorId': '46782621',\n",
       "  'authorName': 'S. Yadav',\n",
       "  'abstract': 'The quest for seeking health information has swamped the web with consumers’ healthrelated questions, which makes the need for efficient and reliable question answering systems more pressing. The consumers’ questions, however, are very descriptive and contain several peripheral information (like patient’s medical history, demographic information, etc.), that are often not required for answering the question. Furthermore, it contributes to the challenges of understanding natural language questions for automatic answer retrieval. Also, it is crucial to provide the consumers with the exact and relevant answers, rather than the entire pool of answer documents to their question. One of the cardinal tasks in achieving robust consumer health question answering systems is the question summarization and multi-document answer summarization. This paper describes the participation of the U.S. National Library of Medicine (NLM) in Consumer Question and Multi-Answer Summarization tasks of the MEDIQA 2021 challenge at NAACL-BioNLP workshop. In this work, we exploited the capabilities of pre-trained transformer models and introduced a transfer learning approach for the abstractive Question Summarization and extractive Multi-Answer Summarization tasks by first pre-training our model on a task-specific summarization dataset followed by fine-tuning it for both the tasks via incorporating medical entities. We achieved the second, sixth and the fourth position for the Question Summarization task in terms ROUGE-1, ROUGE-2 and ROUGE-L scores respectively.',\n",
       "  'year': 2021,\n",
       "  'venue': 'BIONLP'},\n",
       " {'paperId': '30893ab43187e55941b8bbff1de903546b1dc459',\n",
       "  'title': 'Do Neural Network Cross-Modal Mappings Really Bridge Modalities?',\n",
       "  'authorId': '144481186',\n",
       "  'authorName': 'Guillem Collell',\n",
       "  'abstract': 'Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '889feabe31ba0d24c093ac94d54a06eecb87e3f4',\n",
       "  'title': 'Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding',\n",
       "  'authorId': '46217681',\n",
       "  'authorName': 'Nouha Dziri',\n",
       "  'abstract': 'Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'cb13a26968aa60c27a9336620f34214fa0c1f2d3',\n",
       "  'title': 'Improving Named Entity Recognition with Attentive Ensemble of Syntactic Information',\n",
       "  'authorId': '121334441',\n",
       "  'authorName': 'Yuyang Nie',\n",
       "  'abstract': 'Named entity recognition (NER) is highly sensitive to sentential syntactic and semantic properties where entities may be extracted according to how they are used and placed in the running text. To model such properties, one could rely on existing resources to providing helpful knowledge to the NER task; some existing studies proved the effectiveness of doing so, and yet are limited in appropriately leveraging the knowledge such as distinguishing the important ones for particular context. In this paper, we improve NER by leveraging different types of syntactic information through attentive ensemble, which functionalizes by the proposed key-value memory networks, syntax attention, and the gate mechanism for encoding, weighting and aggregating such syntactic information, respectively. Experimental results on six English and Chinese benchmark datasets suggest the effectiveness of the proposed model and show that it outperforms previous studies on all experiment datasets.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '44a590466bf95545eb47daf8ce2a468fa8818751',\n",
       "  'title': 'On Hapax Legomena and Morphological Productivity',\n",
       "  'authorId': '1970864',\n",
       "  'authorName': 'J. Pierrehumbert',\n",
       "  'abstract': 'Quantifying and predicting morphological productivity is a long-standing challenge in corpus linguistics and psycholinguistics. The same challenge reappears in natural language processing in the context of handling words that were not seen in the training set (out-of-vocabulary, or OOV, words). Prior research showed that a good indicator of the productivity of a morpheme is the number of words involving it that occur exactly once (the hapax legomena). A technical connection was adduced between this result and Good-Turing smoothing, which assigns probability mass to unseen events on the basis of the simplifying assumption that word frequencies are stationary. In a large-scale study of 133 affixes in Wikipedia, we develop evidence that success in fact depends on tapping the frequency range in which the assumptions of Good-Turing are violated.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': '819dd861d0b404373797c347686c2178363a0048',\n",
       "  'title': 'Studying Generalisability across Abusive Language Detection Datasets',\n",
       "  'authorId': '148147782',\n",
       "  'authorName': 'Steve Durairaj Swamy',\n",
       "  'abstract': 'Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-dataset training and testing, the paper reveals that the preconceived notion of including more non-abusive samples in a dataset (to emulate reality) may have a detrimental effect on the generalisability of a model trained on that data. Hence a hierarchical annotation model is utilised here to reveal redundancies in existing datasets and to help reduce redundancy in future efforts.',\n",
       "  'year': 2019,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': 'cfd09dd798f434cfaa8bc9d94ecb9837bd7de155',\n",
       "  'title': 'A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit',\n",
       "  'authorId': '2504795',\n",
       "  'authorName': 'Yin-Wen Chang',\n",
       "  'abstract': 'Decoding of phrase-based translation models in the general case is known to be NP-complete, by a reduction from the traveling salesman problem (Knight, 1999). In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during translation. However, the impact on complexity after imposing such a constraint is not well studied. In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The runtime of the algorithm is O(nd!lhd+1) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The algorithm makes use of a novel representation that gives a new perspective on decoding of phrase-based models.',\n",
       "  'year': 2017,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '4521f5e30024dee07de088288aa5607bdeb38ad5',\n",
       "  'title': 'Combining Distant and Partial Supervision for Relation Extraction',\n",
       "  'authorId': '32301760',\n",
       "  'authorName': 'Gabor Angeli',\n",
       "  'abstract': 'Broad-coverage relation extraction either requires expensive supervised training data, or suffers from drawbacks inherent to distant supervision. We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples. We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative. In this way, we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus. Our approach gives a substantial increase of 3.9% endto-end F1 on the 2013 KBP Slot Filling evaluation, yielding a net F1 of 37.7%.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '82517ab6bc2ba14099d4f9bacb0e946a4ba05e3e',\n",
       "  'title': 'X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension',\n",
       "  'authorId': '30671790',\n",
       "  'authorName': 'Mostafa Abdou',\n",
       "  'abstract': 'Although the vast majority of knowledge bases (KBs) are heavily biased towards English, Wikipedias do cover very different topics in different languages. Exploiting this, we introduce a new multilingual dataset (X-WikiRE), framing relation extraction as a multilingual machine reading problem. We show that by leveraging this resource it is possible to robustly transfer models cross-lingually and that multilingual support significantly improves (zero-shot) relation extraction, enabling the population of low-resourced KBs from their well-populated counterparts.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '100f55b495e6ea9a7d85cfebbc583751e6b31683',\n",
       "  'title': 'SHARP: Search-Based Adversarial Attack for Structured Prediction',\n",
       "  'authorId': '2107964289',\n",
       "  'authorName': 'Liwen Zhang',\n",
       "  'abstract': 'Adversarial attack of structured prediction models faces various challenges such as the difﬁculty of perturbing discrete words, the sentence quality issue, and the sensitivity of outputs to small perturbations. In this work, we in-troduce SHARP, a new attack method that for-mulates the black-box adversarial attack as a search-based optimization problem with a specially designed objective function considering sentence ﬂuency, meaning preservation and attacking effectiveness. Additionally, three different searching strategies are analyzed and compared, i.e ., Beam Search, Metropolis-Hastings Sampling, and Hybrid Search. We demonstrate the effectiveness of our attacking strategies on two challenging structured prediction tasks: part-of-speech (POS) tagging and dependency parsing. Through automatic and human evaluations, we show that our method performs a more potent attack compared with pioneer arts. Moreover, the generated adversarial examples can be used to successfully boost the robustness and performance of the victim model via adversarial training.',\n",
       "  'year': 2022,\n",
       "  'venue': 'NAACL-HLT'},\n",
       " {'paperId': 'b22b6c5a6a7a7ad003619960227641dc8dd65c97',\n",
       "  'title': 'BME-HAS System for CoNLL–SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection',\n",
       "  'authorId': '2285758',\n",
       "  'authorName': 'Judit Ács',\n",
       "  'abstract': 'This paper presents an encoder-decoder neural network based solution for both subtasks of the CoNLL–SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection. All of our models are sequence-to-sequence neural networks with multiple encoders and a single decoder.',\n",
       "  'year': 2018,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': 'c442375c9bccffdbb9c3ee25f03c13e472318cdc',\n",
       "  'title': 'Learning a Cost-Effective Annotation Policy for Question Answering',\n",
       "  'authorId': '2208488',\n",
       "  'authorName': 'Bernhard Kratzwald',\n",
       "  'abstract': 'State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our framework against traditional manual annotations in an extensive set of experiments. We find that our approach can reduce up to 21.1% of the annotation cost.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a748b9756e8c2df94861d3191848bbadaa80559e',\n",
       "  'title': 'Coreference and Focus in Reading Times',\n",
       "  'authorId': '39854114',\n",
       "  'authorName': 'Evan Jaffe',\n",
       "  'abstract': 'This paper presents evidence of a linguistic focus effect on coreference resolution in broad-coverage human sentence processing. While previous work has explored the role of prominence in coreference resolution (Almor, 1999; Foraker and McElree, 2007), these studies use constructed stimuli with specific syntactic patterns (e.g. cleft constructions) which could have idiosyncratic frequency confounds. This paper explores the generalizability of this effect on coreference resolution in a broad-coverage analysis. In particular, the current work proposes several new estimators of prominence appropriate for broadcoverage sentence processing and evaluates them as predictors of reading behavior in the Natural Stories corpus (Futrell, Gibson, Tily, Vishnevetsky, Piantadosi, and Fedorenko, in prep), a collection of “constructed-natural” narratives read by a large number of subjects. Results show a strong facilitation effect for one of these predictors on exploratory data and confirm that it generalizes to held-out data. These results provide broad-coverage support for the hypothesis that coreference resolution is easier when the target entity is focused by discourse properties, resulting in faster reading times.',\n",
       "  'year': 2018,\n",
       "  'venue': 'CMCL'},\n",
       " {'paperId': '7321ae1b6dc03f4575b9f1acee6c69b1072e45f8',\n",
       "  'title': 'GATE, a General Architecture for Text Engineering',\n",
       "  'authorId': '145435830',\n",
       "  'authorName': 'H. Cunningham',\n",
       "  'abstract': 'This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering.GATE lies at the intersection of human language computation and software engineering, and constitutes aninfrastructural system supporting research and development of languageprocessing software.',\n",
       "  'year': 1997,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '05ecf19ab393561d31e239865bb0072c781171de',\n",
       "  'title': 'Development of the Multilingual Semantic Annotation System',\n",
       "  'authorId': '5483672',\n",
       "  'authorName': 'S. Piao',\n",
       "  'abstract': 'This paper reports on our research to generate multilingual semantic lexical resources and develop multilingual semantic annotation software, which assigns each word in running text to a semantic category based on a lexical semantic classification scheme. Such tools have an important role in developing intelligent multilingual NLP, text mining and ICT systems. In this work, we aim to extend an existing English semantic annotation tool to cover a range of languages, namely Italian, Chinese and Brazilian Portuguese, by bootstrapping new semantic lexical resources via automatically translating existing English semantic lexicons into these languages. We used a set of bilingual dictionaries and word lists for this purpose. In our experiment, with minor manual improvement of the automatically generated semantic lexicons, the prototype tools based on the new lexicons achieved an average lexical coverage of 79.86% and an average annotation precision of 71.42% (if only precise annotations are considered) or 84.64% (if partially correct annotations are included) on the three languages. Our experiment demonstrates that it is feasible to rapidly develop prototype semantic annotation tools for new languages by automatically bootstrapping new semantic lexicons based on existing ones.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6',\n",
       "  'title': 'TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages',\n",
       "  'authorId': '144797264',\n",
       "  'authorName': 'J. Clark',\n",
       "  'abstract': 'Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA—a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology—the set of linguistic features each language expresses—such that we expect models performing well on this set to generalize across a large number of the world’s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don’t know the answer yet, and the data is collected directly in each language without the use of translation.',\n",
       "  'year': 2020,\n",
       "  'venue': 'Transactions of the Association for Computational Linguistics'},\n",
       " {'paperId': '75c590f18e60a727d8fb32aeba00de6c81779f16',\n",
       "  'title': 'SINAI at SemEval-2021 Task 5: Combining Embeddings in a BiLSTM-CRF model for Toxic Spans Detection',\n",
       "  'authorId': '3455118',\n",
       "  'authorName': 'Flor Miriam Plaza del Arco',\n",
       "  'abstract': 'This paper describes the participation of SINAI team at Task 5: Toxic Spans Detection which consists of identifying spans that make a text toxic. Although several resources and systems have been developed so far in the context of offensive language, both annotation and tasks have mainly focused on classifying whether a text is offensive or not. However, detecting toxic spans is crucial to identify why a text is toxic and can assist human moderators to locate this type of content on social media. In order to accomplish the task, we follow a deep learning-based approach using a Bidirectional variant of a Long Short Term Memory network along with a stacked Conditional Random Field decoding layer (BiLSTM-CRF). Specifically, we test the performance of the combination of different pre-trained word embeddings for recognizing toxic entities in text. The results show that the combination of word embeddings helps in detecting offensive content. Our team ranks 29th out of 91 participants.',\n",
       "  'year': 2021,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': 'edfcc873bca0c9a06cbe876c8e89888d1ad7c238',\n",
       "  'title': 'Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks',\n",
       "  'authorId': '1411458885',\n",
       "  'authorName': 'Anton Chernyavskiy',\n",
       "  'abstract': 'The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP).Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks.We introduce and study a number of variations in the calculation of the loss as well as in the overall training procedure; in particular, we find that a special data shuffling can be quite important.Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression.Finally, we offer detailed analysis and discussion, which should be useful for researchers aiming to explore the utility of contrastive loss in NLP.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '2d6cd38755e2cf2bd43e75c532a201e20ede128b',\n",
       "  'title': 'Inferring Logical Forms From Denotations',\n",
       "  'authorId': '2616463',\n",
       "  'authorName': 'Panupong Pasupat',\n",
       "  'abstract': 'A core problem in learning semantic parsers from denotations is picking out consistent logical forms--those that yield the correct denotation--from a combinatorially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more expressive class of logical forms, and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance. To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms. On the WikiTableQuestions dataset, we increase the coverage of answerable questions from 53.5% to 76%, and the additional crowdsourced supervision lets us rule out 92.1% of spurious logical forms.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'b85ec2b911b7b5e4bc5519d503d265b6c9c9d0c2',\n",
       "  'title': 'Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation',\n",
       "  'authorId': '144653901',\n",
       "  'authorName': 'Daniel Loureiro',\n",
       "  'abstract': 'Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of WordNet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'cfb78a526e916d9ae4b933eb9186bd9e75fe3e50',\n",
       "  'title': 'Supervised and Unsupervised Minimalist Quality Estimators: Vicomtech’s Participation in the WMT 2018 Quality Estimation Task',\n",
       "  'authorId': '3038583',\n",
       "  'authorName': 'T. Etchegoyhen',\n",
       "  'abstract': 'We describe Vicomtech’s participation in the WMT 2018 shared task on quality estimation, for which we submitted minimalist quality estimators. The core of our approach is based on two simple features: lexical translation overlaps and language model cross-entropy scores. These features are exploited in two system variants: uMQE is an unsupervised system, where the final quality score is obtained by averaging individual feature scores; sMQE is a supervised variant, where the final score is estimated by a Support Vector Regressor trained on the available annotated datasets. The main goal of our minimalist approach to quality estimation is to provide reliable estimators that require minimal deployment effort, few resources, and, in the case of uMQE, do not depend on costly data annotation or post-editing. Our approach was applied to all language pairs in sentence quality estimation, obtaining competitive results across the board.',\n",
       "  'year': 2018,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '6db2b93a2d4007371030644173f1001c959214d2',\n",
       "  'title': 'Learning to Write with Cooperative Discriminators',\n",
       "  'authorId': '14487640',\n",
       "  'authorName': 'Ari Holtzman',\n",
       "  'abstract': 'Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice’s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '00ca3e88d783794efd1b5f5dee44ce9dc07c700b',\n",
       "  'title': 'A Two-Stage Approach for Computing Associative Responses to a Set of Stimulus Words',\n",
       "  'authorId': '2062744387',\n",
       "  'authorName': 'Urmi Ghosh',\n",
       "  'abstract': 'This paper describes the system submitted by the IIIT-H team for the CogALex-2014 shared task on multiword association. The task involves generating a ranked list of responses to a set of stimulus words. The two-stage approach combines the strength of neural network based word embeddings and frequency based association measures. The system achieves an accuracy of 34.9% over the test set.',\n",
       "  'year': 2014,\n",
       "  'venue': 'CogALex@COLING'},\n",
       " {'paperId': '49cc2b9ce2df9d0e125189a9b0e3801fda2388bb',\n",
       "  'title': 'Tutorial on Multimodal Machine Learning',\n",
       "  'authorId': '49933077',\n",
       "  'authorName': 'Louis-Philippe Morency',\n",
       "  'abstract': 'Multimodal machine learning involves integrating and modeling information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, HCI, and healthcare. This tutorial, building upon a new edition of a survey paper on multimodal ML as well as previously-given tutorials and academic courses, will describe an updated taxonomy on multimodal machine learning synthesizing its core technical challenges and major directions for future research.',\n",
       "  'year': 2022,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '47ba27ff507db9962948bc52b4c7e58a409e22ab',\n",
       "  'title': 'Using Machine Learning and Deep Learning Methods to Find Mentions of Adverse Drug Reactions in Social Media',\n",
       "  'authorId': '1738669042',\n",
       "  'authorName': 'Pilar López Úbeda',\n",
       "  'abstract': 'Over time the use of social networks is becoming very popular platforms for sharing health related information. Social Media Mining for Health Applications (SMM4H) provides tasks such as those described in this document to help manage information in the health domain. This document shows the first participation of the SINAI group. We study approaches based on machine learning and deep learning to extract adverse drug reaction mentions from Twitter. The results obtained in the tasks are encouraging, we are close to the average of all participants and even above in some cases.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task'},\n",
       " {'paperId': '3d29fb20a388e16a31fe36726b8fd5afcd490b93',\n",
       "  'title': 'Dependency Tree Annotation with Mechanical Turk',\n",
       "  'authorId': '3139108',\n",
       "  'authorName': 'S. Tratz',\n",
       "  'abstract': 'Crowdsourcing is frequently employed to quickly and inexpensively obtain valuable linguistic annotations but is rarely used for parsing, likely due to the perceived difficulty of the task and the limited training of the available workers. This paper presents what is, to the best of our knowledge, the first published use of Mechanical Turk (or similar platform) to crowdsource parse trees. We pay Turkers to construct unlabeled dependency trees for 500 English sentences using an interactive graphical dependency tree editor, collecting 10 annotations per sentence. Despite not requiring any training, several of the more prolific workers meet or exceed 90% attachment agreement with the Penn Treebank (PTB) portion of our data, and, furthermore, for 72% of these PTB sentences, at least one Turker produces a perfect parse. Thus, we find that, supported with a simple graphical interface, people with presumably no prior experience can achieve surprisingly high degrees of accuracy on this task. To facilitate research into aggregation techniques for complex crowdsourced annotations, we publicly release our annotated corpus.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '2f6070f88ecca9a4d20167571a856d6884d6cb1a',\n",
       "  'title': 'Is it Harder to Parse Chinese, or the Chinese Treebank?',\n",
       "  'authorId': '143643017',\n",
       "  'authorName': 'R. Levy',\n",
       "  'abstract': 'We present a detailed investigation of the challenges posed when applying parsing models developed against English corpora to Chinese. We develop a factored-model statistical parser for the Penn Chinese Treebank, showing the implications of gross statistical differences between WSJ and Chinese Tree-banks for the most general methods of parser adaptation. We then provide a detailed analysis of the major sources of statistical parse errors for this corpus, showing their causes and relative frequencies, and show that while some types of errors are due to difficult ambiguities inherent in Chinese grammar, others arise due to treebank annotation practices. We show how each type of error can be addressed with simple, targeted changes to the independence assumptions of the maximum likelihood-estimated PCFG factor of the parsing model, which raises our F1 from 80.7% to 82.6% on our development set, and achieves parse accuracy close to the best published figures for Chinese parsing.',\n",
       "  'year': 2003,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '6d26049c3ed8cec7bc11a02e1236e4df6b8a4e32',\n",
       "  'title': 'Word-Level Uncertainty Estimation for Black-Box Text Classifiers using RNNs',\n",
       "  'authorId': '3459629',\n",
       "  'authorName': 'J. S. Andersen',\n",
       "  'abstract': 'Estimating uncertainties of Neural Network predictions paves the way towards more reliable and trustful text classifications. However, common uncertainty estimation approaches remain as black-boxes without explaining which features have led to the uncertainty of a prediction. This hinders users from understanding the cause of unreliable model behaviour. We introduce an approach to decompose and visualize the uncertainty of text classifiers at the level of words. Our approach builds on top of Recurrent Neural Networks and Bayesian modelling in order to provide detailed explanations of uncertainties, enabling a deeper reasoning about unreliable model behaviours. We conduct a preliminary experiment to check the impact and correctness of our approach. By explaining and investigating the predictive uncertainties of a sentiment analysis task, we argue that our approach is able to provide a more profound understanding of artificial decision making.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': 'd6d737ea3b57c5bbfcb37e9a4accc6f5501305bc',\n",
       "  'title': 'Controlled Semi-automatic Annotation of Classical Ethiopic',\n",
       "  'authorId': '2735405',\n",
       "  'authorName': 'C. Vertan',\n",
       "  'abstract': 'Preservation of the cultural heritage by means of digital methods became extremely popular during last years. After intensive digitization campaigns the focus moves slowly from the genuine preservation (i.e digital archiving together with standard search mechanisms) to research-oriented usage of materials available electronically. This usage is intended to go far beyond simple reading of digitized materials; researchers should be able to gain new insigts in materials, discover new facts by means of tools relying on innovative algorithms. In this article we will describe the workflow necessary for the annotation of a dichronic corpus of classical Ethiopic, language of essential importance for the study of Early Christianity',\n",
       "  'year': 2019,\n",
       "  'venue': ''},\n",
       " {'paperId': '28c938d64d360cdf97c3584d0452a088b56e85ca',\n",
       "  'title': 'Touch-Based Pre-Post-Editing of Machine Translation Output',\n",
       "  'authorId': '35150510',\n",
       "  'authorName': 'Benjamin Marie',\n",
       "  'abstract': 'We introduce pre-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, confirming the promising prospects of the novel translation scenarios offered by our approach.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '39b8c3a864b00e42921ec10b043f54aae5da1cd8',\n",
       "  'title': 'Toward Diverse Precondition Generation',\n",
       "  'authorId': '2050205',\n",
       "  'authorName': 'Heeyoung Kwon',\n",
       "  'abstract': 'A typical goal for language understanding is to logically connect the events of a discourse, but often connective events are not described due to their commonsense nature. In order to address this deficit, we focus here on generating precondition events. Precondition generation can be framed as a sequence-to-sequence problem: given a target event, generate a possible precondition. However, in most real-world scenarios, an event can have several preconditions, which is not always suitable for standard seq2seq frameworks. We propose DiP, the Diverse Precondition generation system that can generate unique and diverse preconditions. DiP consists of three stages of the generative process – an event sampler, a candidate generator, and a post-processor. The event sampler provides control codes (precondition triggers) which the candidate generator uses to focus its generation. Post-processing further improves the results through re-ranking and filtering. Unlike other conditional generation systems, DiP automatically generates control codes without training on diverse examples. Analysis reveals that DiP improves the diversity of preconditions significantly compared to a beam search baseline. Also, manual evaluation shows that DiP generates more preconditions than a strong nucleus sampling baseline.',\n",
       "  'year': 2021,\n",
       "  'venue': 'STARSEM'},\n",
       " {'paperId': '24d687d8e1617b5e9682b2d054c8d7afc8d0551f',\n",
       "  'title': 'Learning Verb Classes in an Incremental Model',\n",
       "  'authorId': '2602766',\n",
       "  'authorName': 'Libby Barak',\n",
       "  'abstract': 'The ability of children to generalize over the linguistic input they receive is key to acquiring productive knowledge of verbs. Such generalizations help children extend their learned knowledge of constructions to a novel verb, and use it appropriately in syntactic patterns previously unobserved for that verb—a key factor in language productivity. Computational models can help shed light on the gradual development of more abstract knowledge during verb acquisition. We present an incremental Bayesian model that simultaneously and incrementally learns argument structure constructions and verb classes given naturalistic language input. We show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes.',\n",
       "  'year': 2014,\n",
       "  'venue': 'CMCL@ACL'},\n",
       " {'paperId': 'b74937a4408fc1cd2d01cdeef633373923bfe991',\n",
       "  'title': 'Explicit Semantic Decomposition for Definition Generation',\n",
       "  'authorId': '2108959287',\n",
       "  'authorName': 'Jiahuan Li',\n",
       "  'abstract': 'Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. However, previous works hardly consider explicitly modeling the “components” of definitions, leading to under-specific generation results. In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation. Experimental results show that achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd948b3c061185f5d9d2c407cf1e851e43495b473',\n",
       "  'title': 'Identification of Ambiguous Multiword Expressions Using Sequence Models and Lexical Resources',\n",
       "  'authorId': '31858684',\n",
       "  'authorName': 'Manon Scholivet',\n",
       "  'abstract': 'We present a simple and efficient tagger capable of identifying highly ambiguous multiword expressions (MWEs) in French texts. It is based on conditional random fields (CRF), using local context information as features. We show that this approach can obtain results that, in some cases, approach more sophisticated parser-based MWE identification methods without requiring syntactic trees from a treebank. Moreover, we study how well the CRF can take into account external information coming from a lexicon.',\n",
       "  'year': 2017,\n",
       "  'venue': 'MWE@EACL'},\n",
       " {'paperId': '0477ef522fa3ec861a9ff5ba35c96cc08ba3697f',\n",
       "  'title': 'Representations for Question Answering from Documents with Tables and Text',\n",
       "  'authorId': '143689491',\n",
       "  'authorName': 'V. Zayats',\n",
       "  'abstract': 'Tables in web documents are pervasive and can be directly used to answer many of the queries searched on the web, motivating their integration in question answering. Very often information presented in tables is succinct and hard to interpret with standard language representations. On the other hand, tables often appear within textual context, such as an article describing the table. Using the information from an article as additional context can potentially enrich table representations. In this work we aim to improve question answering from tables by refining table representations based on information from surrounding text. We also present an effective method to combine text and table-based predictions for question answering from full documents, obtaining significant improvements on the Natural Questions dataset (Kwiatkowski et al., 2019).',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '8d6a67a5f4192280a35ccaaf4e660c1772c54a63',\n",
       "  'title': 'Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies',\n",
       "  'authorId': '2368067',\n",
       "  'authorName': 'Barret Zoph',\n",
       "  'abstract': 'We present a simple algorithm to efficiently train language models with noise-contrastive estimation (NCE) on graphics processing units (GPUs). Our NCE-trained language models achieve significantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in Chelba et al. (2013). When incorporated into a strong Arabic-English machine translation system they give a strong boost in translation quality. We release a toolkit so that others may also train large-scale, large vocabulary LSTM language models with NCE, parallelizing computation across multiple GPUs.',\n",
       "  'year': 2016,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'e31cb285b3f02dc79e2032a85a211832f3e51a39',\n",
       "  'title': 'A Neural Model of Adaptation in Reading',\n",
       "  'authorId': '3220165',\n",
       "  'authorName': 'Marten van Schijndel',\n",
       "  'abstract': 'It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context. We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model. We analyze the performance of the model on controlled materials from psycholinguistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '43ccd087369ec2158d885ec120cb896825c522a0',\n",
       "  'title': 'Combining Outputs of Multiple Japanese Named Entity Chunkers by Stacking',\n",
       "  'authorId': '1732417',\n",
       "  'authorName': 'T. Utsuro',\n",
       "  'abstract': 'In this paper, we propose a method for learning a classifier which combines outputs of more than one Japanese named entity extractors. The proposed combination method belongs to the family of stacked generalizers, which is in principle a technique of combining outputs of several classifiers at the first stage by learning a second stage classifier to combine those outputs at the first stage. Individual models to be combined are based on maximum entropy models, one of which always considers surrounding contexts of a fixed length, while the other considers those of variable lengths according to the number of constituent morphemes of named entities. As an algorithm for learning the second stage classifier, we employ a decision list learning method. Experimental evaluation shows that the proposed method achieves improvement over the best known results with Japanese named entity extractors based on maximum entropy models.',\n",
       "  'year': 2002,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '677daf92e768e65900988352fa7cbbdef5e37b91',\n",
       "  'title': 'Semi-supervised Interactive Intent Labeling',\n",
       "  'authorId': '38531701',\n",
       "  'authorName': 'Saurav Sahay',\n",
       "  'abstract': 'Building the Natural Language Understanding (NLU) modules of task-oriented Spoken Dialogue Systems (SDS) involves a definition of intents and entities, collection of task-relevant data, annotating the data with intents and entities, and then repeating the same process over and over again for adding any functionality/enhancement to the SDS. In this work, we showcase an Intent Bulk Labeling system where SDS developers can interactively label and augment training data from unlabeled utterance corpora using advanced clustering and visual labeling methods. We extend the Deep Aligned Clustering work with a better backbone BERT model, explore techniques to select the seed data for labeling, and develop a data balancing method using an oversampling technique that utilizes paraphrasing models. We also look at the effect of data augmentation on the clustering process. Our results show that we can achieve over 10% gain in clustering accuracy on some datasets using the combination of the above techniques. Finally, we extract utterance embeddings from the clustering model and plot the data to interactively bulk label the samples, reducing the time and effort for data labeling of the whole dataset significantly.',\n",
       "  'year': 2021,\n",
       "  'venue': 'DASH'},\n",
       " {'paperId': '3d03bf891bfb5988f5a6861caa82022d60d537ed',\n",
       "  'title': 'Simplified Abugidas',\n",
       "  'authorId': '2126835',\n",
       "  'authorName': 'Chenchen Ding',\n",
       "  'abstract': 'An abugida is a writing system where the consonant letters represent syllables with a default vowel and other vowels are denoted by diacritics. We investigate the feasibility of recovering the original text written in an abugida after omitting subordinate diacritics and merging consonant letters with similar phonetic values. This is crucial for developing more efficient input methods by reducing the complexity in abugidas. Four abugidas in the southern Brahmic family, i.e., Thai, Burmese, Khmer, and Lao, were studied using a newswire 20,000-sentence dataset. We compared the recovery performance of a support vector machine and an LSTM-based recurrent neural network, finding that the abugida graphemes could be recovered with 94% - 97% accuracy at the top-1 level and 98% - 99% at the top-4 level, even after omitting most diacritics (10 - 30 types) and merging the remaining 30 - 50 characters into 21 graphemes.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a3e2d3ce897a28fc35be86ec73fab9d5591f10d4',\n",
       "  'title': 'Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration',\n",
       "  'authorId': '2107904041',\n",
       "  'authorName': 'Changsong Liu',\n",
       "  'abstract': 'To enable language-based communication and collaboration with cognitive robots, this paper presents an approach where an agent can learn task models jointly from language instruction and visual demonstration using an And-Or Graph (AoG) representation. The learned AoG captures a hierarchical task structure where linguistic labels (for language communication) are grounded to corresponding state changes from the physical environment (for perception and action). Our empirical results on a cloth-folding domain have shown that, although state detection through visual processing is full of uncertainties and error prone, by a tight integration with language the agent is able to learn an effective AoG for task representation. The learned AoG can be further applied to infer and interpret on-going actions from new visual demonstration using linguistic labels at different levels of granularity.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '48fbdf1be70221ac8a6b22079245030ab6158760',\n",
       "  'title': 'Findings of the 2018 Conference on Machine Translation (WMT18)',\n",
       "  'authorId': '143832874',\n",
       "  'authorName': 'Ondrej Bojar',\n",
       "  'abstract': 'This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2018. Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation.',\n",
       "  'year': 2018,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '58834a447c749758e7f57498c6dd88a281af41a0',\n",
       "  'title': 'Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing',\n",
       "  'authorId': '47070750',\n",
       "  'authorName': 'Daniel Fried',\n",
       "  'abstract': 'Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser’s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '85476334d2c3c359dfbe1ec869bdacbf25ff1309',\n",
       "  'title': 'Asymmetry in Parsing and Generating with Unification Grammars: Case Studies from ELU',\n",
       "  'authorId': '46898028',\n",
       "  'authorName': 'G. Russell',\n",
       "  'abstract': 'Recent developments in generation algorithms have enabled work in unification-based computational linguistics to approach more closely the ideal of grammars as declarative statements of linguistic facts, neutral between analysis and synthesis. From this perspective, however, the situation is still far from perfect; all known methods of generation impose constraints on the grammars they assume.We briefly consider a number of proposals for generation, outlining their consequences for the form of grammars, and then report on experience arising from the addition of a generator to an existing unification environment. The algorithm in question (based on that of Shieber et al. (1989)), though among the most permissive currently available, excludes certain classes of parsable analyses.',\n",
       "  'year': 1990,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'bcb9c929b34e569d8d492721e83981c4086d4d79',\n",
       "  'title': 'Style versus Content: A distinction without a (learnable) difference?',\n",
       "  'authorId': '51111055',\n",
       "  'authorName': 'Somayeh Jafaritazehjani',\n",
       "  'abstract': 'Textual style transfer involves modifying the style of a text while preserving its content. This assumes that it is possible to separate style from content. This paper investigates whether this separation is possible. We use sentiment transfer as our case study for style transfer analysis. Our experimental methodology frames style transfer as a multi-objective problem, balancing style shift with content preservation and fluency. Due to the lack of parallel data for style transfer we employ a variety of adversarial encoder-decoder networks in our experiments. Also, we use of a probing methodology to analyse how these models encode style-related features in their latent spaces. The results of our experiments which are further confirmed by a human evaluation reveal the inherent trade-off between the multiple style transfer objectives which indicates that style cannot be usefully separated from content within these style-transfer systems.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': 'adcfebbe2a1960e5a23243d1a5f3837832109ff1',\n",
       "  'title': 'Distributional vectors encode referential attributes',\n",
       "  'authorId': '2116391511',\n",
       "  'authorName': 'Abhijeet Gupta',\n",
       "  'abstract': 'Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany). In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants). In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes. We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more firmly in the external world in measurable ways.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'd0c47c4de7ff56150c32e3b68fb53f7f898fc57b',\n",
       "  'title': 'Investigating Public Health Surveillance using Twitter',\n",
       "  'authorId': '1399097376',\n",
       "  'authorName': 'Antonio Jimeno-Yepes',\n",
       "  'abstract': 'Microblog services such as Twitter are an attractive source of data for public health surveillance, as they avoid the legal and technical obstacles to accessing the more obvious and targeted sources of health information. Only a tiny fraction of tweets may contain useful public health information but in Twitter this is oset by the sheer volume of tweets posted. We present a system which can identify medical named entities in a real-time stream of Twitter posts and determine their geographic locations, as well as preliminary experiments in using this information for health surveillance purposes.',\n",
       "  'year': 2015,\n",
       "  'venue': 'BioNLP@IJCNLP'},\n",
       " {'paperId': '0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78',\n",
       "  'title': 'Using Convolutional Neural Networks to Classify Hate-Speech',\n",
       "  'authorId': '1887897',\n",
       "  'authorName': 'Björn Gambäck',\n",
       "  'abstract': 'The paper introduces a deep learning-based Twitter hate-speech text classification system. The classifier assigns each tweet to one of four predefined categories: racism, sexism, both (racism and sexism) and non-hate-speech. Four Convolutional Neural Network models were trained on resp. character 4-grams, word vectors based on semantic information built using word2vec, randomly generated word vectors, and word vectors combined with character n-grams. The feature set was down-sized in the networks by max-pooling, and a softmax function used to classify tweets. Tested by 10-fold cross-validation, the model based on word2vec embeddings performed best, with higher precision than recall, and a 78.3% F-score.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ALW@ACL'},\n",
       " {'paperId': 'bd04b4193cb4717cb8b0403e7284b6c88bffd7e5',\n",
       "  'title': 'Modeling Entity Knowledge for Fact Verification',\n",
       "  'authorId': '2152797401',\n",
       "  'authorName': 'Yang Liu',\n",
       "  'abstract': 'Fact verification is a challenging task of identifying the truthfulness of given claims based on the retrieval of relevant evidence texts. Many claims require understanding and reasoning over external entity information for precise verification. In this paper, we propose a novel fact verification model using entity knowledge to enhance its performance. We retrieve descriptive text from Wikipedia for each entity, and then encode these descriptions by a smaller lightweight network to be fed into the main verification model. Furthermore, we boost model performance by adopting and predicting the relatedness between the claim and each evidence as additional signals. We demonstrate experimentally on a large-scale benchmark dataset FEVER that our framework achieves competitive results with a FEVER score of 72.89% on the test set.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FEVER'},\n",
       " {'paperId': '3106ec03b628bbf5209764437a3f48ded4fbed77',\n",
       "  'title': 'An Unsupervised System for Parallel Corpus Filtering',\n",
       "  'authorId': '2113189',\n",
       "  'authorName': 'Viktor Hangya',\n",
       "  'abstract': 'In this paper we describe LMU Munich’s submission for the WMT 2018 Parallel Corpus Filtering shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of machine translation systems, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our unsupervised system achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.',\n",
       "  'year': 2018,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': 'c5a24b38c099ac55291adac2acf5d039fa7d05bc',\n",
       "  'title': 'Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets',\n",
       "  'authorId': '35470059',\n",
       "  'authorName': 'Michael Kranzlein',\n",
       "  'abstract': 'For interpreting the behavior of a probabilistic model, it is useful to measure a model’s calibration —the extent to which the model produces reliable conﬁdence scores. We address the open problem of calibration for tagging models with sparse tagsets , and recommend strategies to measure and reduce calibration error (CE) in such models. We show that several post-hoc re calibration techniques all reduce calibration error across the marginal distribution for two existing sequence taggers. Moreover, we propose tag frequency grouping (TFG) as a way to measure calibration error in different frequency bands. Further, recalibrat-ing each group separately promotes a more eq-uitable reduction of calibration error across the tag frequency spectrum.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6053f37173a211a7d7e1a7db2ad46fc092650530',\n",
       "  'title': 'Identification of Languages in Algerian Arabic Multilingual Documents',\n",
       "  'authorId': '3449178',\n",
       "  'authorName': 'Wafia Adouane',\n",
       "  'abstract': 'This paper presents a language identification system designed to detect the language of each word, in its context, in a multilingual documents as generated in social media by bilingual/multilingual communities, in our case speakers of Algerian Arabic. We frame the task as a sequence tagging problem and use supervised machine learning with standard methods like HMM and Ngram classification tagging. We also experiment with a lexicon-based method. Combining all the methods in a fall-back mechanism and introducing some linguistic rules, to deal with unseen tokens and ambiguous words, gives an overall accuracy of 93.14%. Finally, we introduced rules for language identification from sequences of recognised words.',\n",
       "  'year': 2017,\n",
       "  'venue': 'WANLP@EACL'},\n",
       " {'paperId': '5165989a092a7e2be6f3cae420380cbe8024269a',\n",
       "  'title': 'What Determines Inter-Coder Agreement in Manual Annotations? A Meta-Analytic Investigation',\n",
       "  'authorId': '3338514',\n",
       "  'authorName': 'P. Bayerl',\n",
       "  'abstract': 'Recent discussions of annotator agreement have mostly centered around its calculation and interpretation, and the correct choice of indices. Although these discussions are important, they only consider the “back-end” of the story, namely, what to do once the data are collected. Just as important in our opinion is to know how agreement is reached in the first place and what factors influence coder agreement as part of the annotation process or setting, as this knowledge can provide concrete guidelines for the planning and set-up of annotation projects. To investigate whether there are factors that consistently impact annotator agreement we conducted a meta-analytic investigation of annotation studies reporting agreement percentages. Our meta-analysis synthesized factors reported in 96 annotation studies from three domains (word-sense disambiguation, prosodic transcriptions, and phonetic transcriptions) and was based on a total of 346 agreement indices. Our analysis identified seven factors that influence reported agreement values: annotation domain, number of categories in a coding scheme, number of annotators in a project, whether annotators received training, the intensity of annotator training, the annotation purpose, and the method used for the calculation of percentage agreements. Based on our results we develop practical recommendations for the assessment, interpretation, calculation, and reporting of coder agreement. We also briefly discuss theoretical implications for the concept of annotation quality.',\n",
       "  'year': 2011,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': 'f7c14e79d3eb1d24b4184d106244be1672113ce2',\n",
       "  'title': 'WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER',\n",
       "  'authorId': '2140370472',\n",
       "  'authorName': 'Simone Tedeschi',\n",
       "  'abstract': 'Multilingual Named Entity Recognition (NER) is a key intermediate task which is needed in many areas of NLP. In this paper, we address the well-known issue of data scarcity in NER, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of multilingual silver data for the task. We exploit the texts of Wikipedia and introduce a new methodology based on the effective combination of knowledge-based approaches and neural models, together with a novel domain adaptation technique, to produce high-quality training corpora for NER. We evaluate our datasets extensively on standard benchmarks for NER, yielding substantial improvements of up to 6 span-based F1-score points over previous state-of-the-art systems for data creation.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'f0580cb3e89633800bc103913567d5e8893acfc3',\n",
       "  'title': 'Past, Present, Future: A Computational Investigation of the Typology of Tense in 1000 Languages',\n",
       "  'authorId': '2412559',\n",
       "  'authorName': 'Ehsaneddin Asgari',\n",
       "  'abstract': 'We present SuperPivot, an analysis method for low-resource languages that occur in a superparallel corpus, i.e., in a corpus that contains an order of magnitude more languages than parallel corpora currently in use. We show that SuperPivot performs well for the crosslingual analysis of the linguistic phenomenon of tense. We produce analysis results for more than 1000 languages, conducting – to the best of our knowledge – the largest crosslingual computational study performed to date. We extend existing methodology for leveraging parallel corpora for typological analysis by overcoming a limiting assumption of earlier work: We only require that a linguistic feature is overtly marked in a few of thousands of languages as opposed to requiring that it be marked in all languages under investigation.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5a94aaa3ad624608e46a75de49452a03de568a07',\n",
       "  'title': 'Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding',\n",
       "  'authorId': '31702389',\n",
       "  'authorName': 'Alexander Ku',\n",
       "  'abstract': 'We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations. We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in simulated, photo-realistic environments.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '7600bc8922c225b299658417f83d54e450a28642',\n",
       "  'title': 'Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation',\n",
       "  'authorId': '48335426',\n",
       "  'authorName': 'Biao Zhang',\n",
       "  'abstract': 'Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c15c767ee88118b8428f8f25a7070452494cb252',\n",
       "  'title': 'PRHLT-UPV at SemEval-2020 Task 12: BERT for Multilingual Offensive Language Detection',\n",
       "  'authorId': '25179126',\n",
       "  'authorName': 'Gretel Liz De la Peña Sarracén',\n",
       "  'abstract': 'The present paper describes the system submitted by the PRHLT-UPV team for the task 12 of SemEval-2020: OffensEval 2020. The official title of the task is Multilingual Offensive Language Identification in Social Media, and aims to identify offensive language in texts. The languages included in the task are English, Arabic, Danish, Greek and Turkish. We propose a model based on the BERT architecture for the analysis of texts in English. The approach leverages knowledge within a pre-trained model and performs fine-tuning for the particular task. In the analysis of the other languages the Multilingual BERT is used, which has been pre-trained for a large number of languages. In the experiments, the proposed method for English texts is compared with other approaches to analyze the relevance of the architecture used. Furthermore, simple models for the other languages are evaluated to compare them with the proposed one. The experimental results show that the model based on BERT outperforms other approaches. The main contribution of this work lies in this study, despite not obtaining the first positions in most cases of the competition ranking.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': 'd97bf89fbdfe69e530c3a6fb0870bf9f8d799f0f',\n",
       "  'title': 'SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment',\n",
       "  'authorId': '2472657',\n",
       "  'authorName': 'Goran Glavas',\n",
       "  'abstract': 'Lexical entailment (LE) is a fundamental asymmetric lexico-semantic relation, supporting the hierarchies in lexical resources (e.g., WordNet, ConceptNet) and applications like natural language inference and taxonomy induction. Multilingual and cross-lingual NLP applications warrant models for LE detection that go beyond language boundaries. As part of SemEval 2020, we carried out a shared task (Task 2) on multilingual and cross-lingual LE. The shared task spans three dimensions: (1) monolingual vs. cross-lingual LE, (2) binary vs. graded LE, and (3) a set of 6 diverse languages (and 15 corresponding language pairs). We offered two different evaluation tracks: (a) Dist: for unsupervised, fully distributional models that capture LE solely on the basis of unannotated corpora, and (b) Any: for externally informed models, allowed to leverage any resources, including lexico-semantic networks (e.g., WordNet or BabelNet). In the Any track, we recieved runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': '0388a7e782e8c9930e92ad90211cb935ab01c2ec',\n",
       "  'title': 'Natural Language Analysis of Patent Claims',\n",
       "  'authorId': '2287325',\n",
       "  'authorName': 'S. Sheremetyeva',\n",
       "  'abstract': 'We propose a NLP methodology for analyzing patent claims that combines symbolic grammar formalisms with data-intensive methods while enhancing analysis robustness. The output of our analyzer is a shallow interlingual representation that captures both the structure and content of a claim text. The methodology can be used in any patent-related application, such as machine translation, improving readability of patent claims, information retrieval, extraction, summarization, generation, etc. The methodology should be universal in the sense that it could be applied to any language, other parts of patent documentation and text as such.',\n",
       "  'year': 2003,\n",
       "  'venue': 'ACL 2003'},\n",
       " {'paperId': '6e88f1df50d99185432a0fa5306df30b5ca28025',\n",
       "  'title': 'Complaint Identification in Social Media with Transformer Networks',\n",
       "  'authorId': '1999667117',\n",
       "  'authorName': 'Mali Jin',\n",
       "  'abstract': 'Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '56bf47172aa2ecc5d34cacd081befff03288b5d8',\n",
       "  'title': 'Exploring Stylistic Variation with Age and Income on Twitter',\n",
       "  'authorId': '2192277',\n",
       "  'authorName': 'Lucie Flekova',\n",
       "  'abstract': 'Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors’ age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '0ec10fa2a336ddca05373ecf2fb74251e52b3aef',\n",
       "  'title': 'Some Points in a Time',\n",
       "  'authorId': '145848824',\n",
       "  'authorName': 'Karen Spärck Jones',\n",
       "  'abstract': 'This article offers a personal perspective on the development of language and information processing over the last half century, focusing on the use of statistical methods. Introduced, with computers, in the 1950s, these have not always been highly regarded, but were revived in the 1990s. They have proved effective in more ways than might have been expected, and encourage new thinking about what language and information processing involve.',\n",
       "  'year': 2005,\n",
       "  'venue': 'Computational Linguistics'},\n",
       " {'paperId': '1c238c9cf76b26e9c2cd7e3e23c47576667a4425',\n",
       "  'title': 'Enriching Language Models with Visually-grounded Word Vectors and the Lancaster Sensorimotor Norms',\n",
       "  'authorId': '2061232',\n",
       "  'authorName': 'C. Kennington',\n",
       "  'abstract': 'Language models are trained only on text despite the fact that humans learn their first language in a highly interactive and multimodal environment where the first set of learned words are largely concrete, denoting physical entities and embodied states. To enrich language models with some of this missing experience, we leverage two sources of information: (1) the Lancaster Sensorimotor norms, which provide ratings (means and standard deviations) for over 40,000 English words along several dimensions of embodiment, and which capture the extent to which something is experienced across 11 different sensory modalities, and (2) vectors from coefficients of binary classifiers trained on images for the BERT vocabulary. We pre-trained the ELECTRA model and fine-tuned the RoBERTa model with these two sources of information then evaluate using the established GLUE benchmark and the Visual Dialog benchmark. We find that enriching language models with the Lancaster norms and image vectors improves results in both tasks, with some implications for robust language models that capture holistic linguistic meaning in a language learning context.',\n",
       "  'year': 2021,\n",
       "  'venue': 'CONLL'},\n",
       " {'paperId': '5ea878c8b1b3b412f74e84301c78c35fff5638ca',\n",
       "  'title': 'Semi-Supervised Learning and Domain Adaptation in Natural Language Processing Anders Søgaard University of Copenhagen Morgan & Claypool (Synthesis Lectures on Human Language Technologies, edited by Graeme Hirst, volume 21), 2013, x+93 pp; paperbound, ISBN 978-1-60845-985-8, $40.00; e-book, ISBN 978-',\n",
       "  'authorId': '2458308',\n",
       "  'authorName': 'George F. Foster',\n",
       "  'abstract': 'Classical machine learning makes at least two assumptions that are at odds with its application to natural language. First, it implicitly assumes there are enough data. This is rarely the case in NLP, where sparse data is the norm, especially for intermediate tasks like parsing that require artificial labeling. Second, it assumes that all examples are drawn from the same distribution. Language is of course not like this: rather than being an ordered landscape, it is a wildly varying one, rich with strange growths and prone to sudden monstrous blooms like micro-blogging. Both these traits can cause a classically trained NLP system to behave poorly. To cope with data sparsity, a common strategy is semi-supervised learning, in which a small labeled data set is augmented by a larger amount of (typically more abundant) unlabeled data. To cope with domain differences between training and test data, adaptation techniques can be used to mitigate training data bias by exploiting whatever is known of the test domain. The link between these two topics is that what is known of the test domain often comes in the form of an unlabeled sample, and hence semi-supervised techniques constitute an important class of adaptation strategies.',\n",
       "  'year': 2014,\n",
       "  'venue': 'Computational Linguistics'},\n",
       " {'paperId': '8de470d16044429085b590f03216c0a8992e364c',\n",
       "  'title': 'How to Train good Word Embeddings for Biomedical NLP',\n",
       "  'authorId': '50493332',\n",
       "  'authorName': 'Billy Chiu',\n",
       "  'abstract': 'The quality of word embeddings depends on the input corpora, model architectures, and hyper-parameter settings. Using the state-of-the-art neural embedding tool word2vec and both intrinsic and extrinsic evaluations, we present a comprehensive study of how the quality of embeddings changes according to these features. Apart from identifying the most influential hyper-parameters, we also observe one that creates contradictory results between intrinsic and extrinsic evaluations. Furthermore, we find that bigger corpora do not necessarily produce better biomedical domain word embeddings. We make our evaluation tools and resources as well as the created state-of-the-art word embeddings available under open licenses from https://github.com/ cambridgeltl/BioNLP-2016.',\n",
       "  'year': 2016,\n",
       "  'venue': 'BioNLP@ACL'},\n",
       " {'paperId': '668f42a4d4094f0a66d402a16087e14269b31a1f',\n",
       "  'title': 'Analysis Methods in Neural Language Processing: A Survey',\n",
       "  'authorId': '2083259',\n",
       "  'authorName': 'Yonatan Belinkov',\n",
       "  'abstract': 'The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.',\n",
       "  'year': 2018,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '74ffab9da62788160681e86d3d36ae578df7c977',\n",
       "  'title': 'End-to-End Argument Mining as Biaffine Dependency Parsing',\n",
       "  'authorId': '47107411',\n",
       "  'authorName': 'Yuxiao Ye',\n",
       "  'abstract': 'Non-neural approaches to argument mining (AM) are often pipelined and require heavy feature-engineering. In this paper, we propose a neural end-to-end approach to AM which is based on dependency parsing, in contrast to the current state-of-the-art which relies on relation extraction. Our biaffine AM dependency parser significantly outperforms the state-of-the-art, performing at F1 = 73.5% for component identification and F1 = 46.4% for relation identification. One of the advantages of treating AM as biaffine dependency parsing is the simple neural architecture that results. The idea of treating AM as dependency parsing is not new, but has previously been abandoned as it was lagging far behind the state-of-the-art. In a thorough analysis, we investigate the factors that contribute to the success of our model: the biaffine model itself, our representation for the dependency structure of arguments, different encoders in the biaffine model, and syntactic information additionally fed to the model. Our work demonstrates that dependency parsing for AM, an overlooked idea from the past, deserves more attention in the future.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': 'eea16dfc29f0521dd547e67a84af4ff95a9c5529',\n",
       "  'title': 'Visually Grounded Reasoning across Languages and Cultures',\n",
       "  'authorId': '144097210',\n",
       "  'authorName': 'Fangyu Liu',\n",
       "  'abstract': 'The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'be8fa9b67bdb2c3c41087f0b811bf33db9949519',\n",
       "  'title': 'Compound or Term Features? Analyzing Salience in Predicting the Difficulty of German Noun Compounds across Domains',\n",
       "  'authorId': '3449046',\n",
       "  'authorName': 'Anna Hätty',\n",
       "  'abstract': 'Predicting the difficulty of domain-specific vocabulary is an important task towards a better understanding of a domain, and to enhance the communication between lay people and experts. We investigate German closed noun compounds and focus on the interaction of compound-based lexical features (such as frequency and productivity) and terminology-based features (contrasting domain-specific and general language) across word representations and classifiers. Our prediction experiments complement insights from classification using (a) manually designed features to characterise termhood and compound formation and (b) compound and constituent word embeddings. We find that for a broad binary distinction into ‘easy’ vs. ‘difficult’ general-language compound frequency is sufficient, but for a more fine-grained four-class distinction it is crucial to include contrastive termhood features and compound and constituent features.',\n",
       "  'year': 2021,\n",
       "  'venue': 'STARSEM'},\n",
       " {'paperId': '843f58b5f34b45c29577cc17ebd28efea97fade7',\n",
       "  'title': 'POS Tagging for Improving Code-Switching Identification in Arabic',\n",
       "  'authorId': '144643791',\n",
       "  'authorName': 'Mohammed A. Attia',\n",
       "  'abstract': 'When speakers code-switch between their native language and a second language or language variant, they follow a syntactic pattern where words and phrases from the embedded language are inserted into the matrix language. This paper explores the possibility of utilizing this pattern in improving code-switching identification between Modern Standard Arabic (MSA) and Egyptian Arabic (EA). We try to answer the question of how strong is the POS signal in word-level code-switching identification. We build a deep learning model enriched with linguistic features (including POS tags) that outperforms the state-of-the-art results by 1.9% on the development set and 1.0% on the test set. We also show that in intra-sentential code-switching, the selection of lexical items is constrained by POS categories, where function words tend to come more often from the dialectal language while the majority of content words come from the standard language.',\n",
       "  'year': 2019,\n",
       "  'venue': 'WANLP@ACL 2019'},\n",
       " {'paperId': 'cd322080df586195060fef85975ae231f1b9b6a2',\n",
       "  'title': 'Factoring Ambiguity out of the Prediction of Compositionality for German Multi-Word Expressions',\n",
       "  'authorId': '2736946',\n",
       "  'authorName': 'Stefan Bott',\n",
       "  'abstract': 'Ambiguity represents an obstacle for distributional semantic models(DSMs), which typically subsume the contexts of all word senses within one vector. While individual vector space approaches have been concerned with sense discrimination (e.g., Schütze 1998, Erk 2009, Erk and Pado 2010), such discrimination has rarely been integrated into DSMs across semantic tasks. This paper presents a soft-clustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs.',\n",
       "  'year': 2017,\n",
       "  'venue': 'MWE@EACL'},\n",
       " {'paperId': 'aaf5e3afd61d0df6c483ca32faf8e7a9198b1557',\n",
       "  'title': 'Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning',\n",
       "  'authorId': '9610143',\n",
       "  'authorName': 'Zhihao Fan',\n",
       "  'abstract': 'Image Captioning aims at generating a short description for an image. Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space. They suffer from generating high frequent n-gram with irrelevant words. To tackle this problem, we propose to construct an image-grounded vocabulary, based on which, captions are generated with limitation and guidance. In specific, a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words. For generation, we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly. Reinforce algorithm is employed to train the generator using constraint vocabulary as action space. Experimental results on MS COCO and Flickr30k show the effectiveness of our framework compared to some state-of-the-art models.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '45d98f7a834cd1351b6d9ee0da38a839eab3b31c',\n",
       "  'title': 'Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity',\n",
       "  'authorId': '2115829571',\n",
       "  'authorName': 'Yuxia Wang',\n",
       "  'abstract': 'In this paper, we apply pre-trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low-resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS 1 demonstrate substantial improvements, validating the utility of the proposed methods.',\n",
       "  'year': 2020,\n",
       "  'venue': 'BIONLP'},\n",
       " {'paperId': 'ba7d8b907f5693564bc3589c909ccf55a7209355',\n",
       "  'title': 'Continuous Learning in Neural Machine Translation using Bilingual Dictionaries',\n",
       "  'authorId': '2920247',\n",
       "  'authorName': 'J. Niehues',\n",
       "  'abstract': 'While recent advances in deep learning led to significant improvements in machine translation, neural machine translation is often still not able to continuously adapt to the environment. For humans, as well as for machine translation, bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges: The system needs to be able to perform one-shot learning as well as model the morphology of source and target language. In this work, we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. We integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30% to up to 70%. The correct lemma is even generated by more than 90%.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': 'bb567cf793a6c4df1a652489c5ce866fe044f0e5',\n",
       "  'title': 'Supervised and Unsupervised Transfer Learning for Question Answering',\n",
       "  'authorId': '2815804',\n",
       "  'authorName': 'Yu-An Chung',\n",
       "  'abstract': 'Although transfer learning has been shown to be successful for tasks like object and speech recognition, its applicability to question answering (QA) has yet to be well-studied. In this paper, we conduct extensive experiments to investigate the transferability of knowledge learned from a source QA dataset to a target dataset using two QA models. The performance of both models on a TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) is significantly improved via a simple transfer learning technique from MovieQA (Tapaswi et al., 2016). In particular, one of the models achieves the state-of-the-art on all target datasets; for the TOEFL listening comprehension test, it outperforms the previous best model by 7%. Finally, we show that transfer learning is helpful even in unsupervised scenarios when correct answers for target QA dataset examples are not available.',\n",
       "  'year': 2017,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '34c8b5b816d3ae6d433ca5e5735f264754ff4c2c',\n",
       "  'title': 'N-LTP: An Open-source Neural Language Technology Platform for Chinese',\n",
       "  'authorId': '2256319',\n",
       "  'authorName': 'Wanxiang Che',\n",
       "  'abstract': 'We introduce N-LTP, an open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. In addition, a knowledge distillation method (Clark et al., 2019) where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users to use and view the processing results more easily and directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at https://github.com/HIT-SCIR/ltp.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '282083948b900a632d5bc9a6c7a4497c143e3932',\n",
       "  'title': 'Investigating the content and form of referring expressions in Mandarin: introducing the Mtuna corpus',\n",
       "  'authorId': '1789347',\n",
       "  'authorName': 'Kees van Deemter',\n",
       "  'abstract': 'East Asian languages are thought to handle reference differently from languages such as English, particularly in terms of the marking of definiteness and number. We present the first Data-Text corpus for Referring Expressions in Mandarin, and we use this corpus to test some initial hypotheses inspired by the theoretical linguistics literature. Our findings suggest that function words deserve more attention in Referring Expressions Generation than they have so far received, and they have a bearing on the debate about whether different languages make different trade-offs between clarity and brevity.',\n",
       "  'year': 2017,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': '64d02eacda1d74f0c929dea1666081fc7f2b0787',\n",
       "  'title': 'Detecting Non-compositional MWE Components using Wiktionary',\n",
       "  'authorId': '1754682',\n",
       "  'authorName': 'Bahar Salehi',\n",
       "  'abstract': 'We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary. The approach makes use of the definitions, synonyms and translations in Wiktionary, and is applicable to any type of MWE in any language, assuming the MWE is contained in Wiktionary. Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b0e471f63193dd58e80081c03ec712d12d3d1254',\n",
       "  'title': 'Knowledge Graph and Corpus Driven Segmentation and Answer Inference for Telegraphic Entity-seeking Queries',\n",
       "  'authorId': '144863691',\n",
       "  'authorName': 'Mandar Joshi',\n",
       "  'abstract': 'Much recent work focuses on formal interpretation of natural question utterances, with the goal of executing the resulting structured queries on knowledge graphs (KGs) such as Freebase. Here we address two limitations of this approach when applied to open-domain, entity-oriented Web queries. First, Web queries are rarely wellformed questions. They are “telegraphic”, with missing verbs, prepositions, clauses, case and phrase clues. Second, the KG is always incomplete, unable to directly answer many queries. We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment: a base entity e1, a relation type r, a target entity type t2, and contextual words s. The query seeks entity e2 2 t2 where r(e1,e2) holds, further evidenced by schema-agnostic words s. Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG. We do not trust the best or any specific query segmentation. Instead, evidence in favor of candidate e2s are aggregated across several segmentations. Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG, using over a thousand telegraphic queries adapted from TREC, INEX, and WebQuestions, show the efficacy of our approach. For one benchmark, MAP improves from 0.2‐0.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.29‐0.36 to 0.54.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6d38487e0e644f78d8827513c89230f239faa11f',\n",
       "  'title': 'Mapping probability word problems to executable representations',\n",
       "  'authorId': '34647353',\n",
       "  'authorName': 'Simon Suster',\n",
       "  'abstract': 'While solving math word problems automatically has received considerable attention in the NLP community, few works have addressed probability word problems specifically. In this paper, we employ and analyse various neural models for answering such word problems. In a two-step approach, the problem text is first mapped to a formal representation in a declarative language using a sequence-to-sequence model, and then the resulting representation is executed using a probabilistic programming system to provide the answer. Our best performing model incorporates general-domain contextualised word representations that were finetuned using transfer learning on another in-domain dataset. We also apply end-to-end models to this task, which bring out the importance of the two-step approach in obtaining correct solutions to probability problems.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '43bab902791fc844d80b005e2c54bbbbe9e64f26',\n",
       "  'title': 'Structural Adapters in Pretrained Language Models for AMR-to-Text Generation',\n",
       "  'authorId': '10430740',\n",
       "  'authorName': 'Leonardo F. R. Ribeiro',\n",
       "  'abstract': 'Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this paper, we propose StructAdapt, an adapter method to encode graph structure into PLMs. Contrary to prior work, StructAdapt effectively models interactions among the nodes based on the graph connectivity, only training graph structure-aware adapter parameters. In this way, we incorporate task-specific knowledge while maintaining the topological structure of the graph. We empirically show the benefits of explicitly encoding graph structure into PLMs using StructAdapt, outperforming the state of the art on two AMR-to-text datasets, training only 5.1% of the PLM parameters.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'd6d0f364886c6ba4318ae32c09dd2f48a03ba394',\n",
       "  'title': 'Assessing the performance of Olelo, a real-time biomedical question answering application',\n",
       "  'authorId': '1885693',\n",
       "  'authorName': 'Mariana Neves',\n",
       "  'abstract': 'Question answering (QA) can support physicians and biomedical researchers to find answers to their questions in the scientific literature. Such systems process large collections of documents in real time and include many natural language processing (NLP) procedures. We recently developed Olelo, a QA system for biomedicine which includes various NLP components, such as question processing, document and passage retrieval, answer processing and multi-document summarization. In this work, we present an evaluation of our system on the the fifth BioASQ challenge. We participated with the current state of the application and with an extension based on semantic role labeling that we are currently investigating. In addition to the BioASQ evaluation, we compared our system to other on-line biomedical QA systems in terms of the response time and the quality of the answers.',\n",
       "  'year': 2017,\n",
       "  'venue': 'BioNLP'},\n",
       " {'paperId': 'fe65628c0a1b328ffc4b6f648f7408a941e6f62b',\n",
       "  'title': 'Early and Late Combinations of Criteria for Reranking Distributional Thesauri',\n",
       "  'authorId': '1679133',\n",
       "  'authorName': 'Olivier Ferret',\n",
       "  'abstract': 'In this article, we first propose to exploit a new criterion for improving distributional thesauri. Following a bootstrapping perspective, we select relations between the terms of similar nominal compounds for building in an unsupervised way the training set of a classifier performing the reranking of a thesaurus. Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '698d675ba7134ac701de810c9ca4a6de72cb414b',\n",
       "  'title': 'Character-Level Question Answering with Attention',\n",
       "  'authorId': '145798491',\n",
       "  'authorName': 'David Golub',\n",
       "  'abstract': 'We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5f5e7284f9179f0a86ce4f1c9c9f93957e6b387b',\n",
       "  'title': 'KPQA: A Metric for Generative Question Answering Using Keyphrase Weights',\n",
       "  'authorId': '2109339794',\n",
       "  'authorName': 'Hwanhee Lee',\n",
       "  'abstract': 'In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQA metric, a new metric for evaluating the correctness of GenQA. Specifically, our new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our metric, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed metric has a significantly higher correlation with human judgments than existing metrics in various datasets. Code for KPQA-metric will be available at https://github.com/hwanheelee1993/KPQA.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '55e3553a825f9a307245016e735d9df37ea1ae41',\n",
       "  'title': 'The Replace Operator',\n",
       "  'authorId': '2425578',\n",
       "  'authorName': 'L. Karttunen',\n",
       "  'abstract': 'This paper introduces to the calculus of regular expressions a replace operator and defines a set of replacement expressions that concisely encode alternate variations of the operation. Replace expressions denote regular relations, defined in terms of other regular expression operators. The basic case is unconditional obligatory replacement. We develop several versions of conditional replacement that allow the operation to be constrained by context.',\n",
       "  'year': 1995,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '7294933e0456386bc464d2282680651908a4431b',\n",
       "  'title': 'Multi-level Translation Quality Prediction with QuEst++',\n",
       "  'authorId': '1702974',\n",
       "  'authorName': 'Lucia Specia',\n",
       "  'abstract': 'This paper presents QUEST++ , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level. It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g. sentences). QUEST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST++ achieves state-of-the-art performance.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '500d570ce02abf42bc1bc535620741d4c5665e6a',\n",
       "  'title': 'Linguistic Regularities in Sparse and Explicit Word Representations',\n",
       "  'authorId': '39455775',\n",
       "  'authorName': 'Omer Levy',\n",
       "  'abstract': 'Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.’s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.',\n",
       "  'year': 2014,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': '595394f3ab10a6ae5867303644a9807474e56111',\n",
       "  'title': 'Towards Assessing Changes in Degree of Depression through Facebook',\n",
       "  'authorId': '145035129',\n",
       "  'authorName': 'H. A. Schwartz',\n",
       "  'abstract': 'Depression is typically diagnosed as being present or absent. However, depression severity is believed to be continuously distributed rather than dichotomous. Severity may vary for a given patient daily and seasonally as a function of many variables ranging from life events to environmental factors. Repeated population-scale assessment of depression through questionnaires is expensive. In this paper we use survey responses and status updates from 28,749 Facebook users to develop a regression model that predicts users’ degree of depression based on their Facebook status updates. Our user-level predictive accuracy is modest, significantly outperforming a baseline of average user sentiment. We use our model to estimate user changes in depression across seasons, and find, consistent with literature, users’ degree of depression most often increases from summer to winter. We then show the potential to study factors driving individuals’ level of depression by looking at its most highly correlated language features.',\n",
       "  'year': 2014,\n",
       "  'venue': 'CLPsych@ACL'},\n",
       " {'paperId': '94f768408d9bfe3789c7105c192025a7fe2cfcf3',\n",
       "  'title': 'UPF-Cobalt Submission to WMT15 Metrics Task',\n",
       "  'authorId': '2006017',\n",
       "  'authorName': 'M. Fomicheva',\n",
       "  'abstract': 'An important limitation of automatic evaluation metrics is that, when comparing Machine Translation (MT) to a human reference, they are often unable to discriminate between acceptable variation and the differences that are indicative of MT errors. In this paper we present UPF-Cobalt evaluation system that addresses this issue by penalizing the differences in the syntactic contexts of aligned candidate and reference words. We evaluate our metric using the data from WMT workshops of the recent years and show that it performs competitively both at segment and at system levels.',\n",
       "  'year': 2015,\n",
       "  'venue': 'WMT@EMNLP'},\n",
       " {'paperId': 'bc0a618cc570551a13b337804c49a86e339add43',\n",
       "  'title': 'A Tree-to-Sequence Model for Neural NLG in Task-Oriented Dialog',\n",
       "  'authorId': '30586030',\n",
       "  'authorName': 'J. Rao',\n",
       "  'abstract': 'Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Sequence-to-sequence models on flat meaning representations (MR) have been dominant in this task, for example in the E2E NLG Challenge. Previous work has shown that a tree-structured MR can improve the model for better discourse-level structuring and sentence-level planning. In this work, we propose a tree-to-sequence model that uses a tree-LSTM encoder to leverage the tree structures in the input MR, and further enhance the decoding by a structure-enhanced attention mechanism. In addition, we explore combining these enhancements with constrained decoding to improve semantic correctness. Our experiments not only show significant improvements over standard seq2seq baselines, but also is more data-efficient and generalizes better to hard scenarios.',\n",
       "  'year': 2019,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': '2b932e2a56e5b1c34c2a926c3a30cbcec84232a8',\n",
       "  'title': 'Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable',\n",
       "  'authorId': '2113189',\n",
       "  'authorName': 'Viktor Hangya',\n",
       "  'abstract': 'Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language. Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here. We make two contributions. First, we test a delightfully simple method for domain adaptation of bilingual word embeddings. We evaluate these embeddings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks. We show that this method also helps in low-resource setups. Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '6e97cada4fc4850d09546c1a7e5a797e8e4c48d6',\n",
       "  'title': 'Tracking the Evolution of Written Language Competence in L2 Spanish Learners',\n",
       "  'authorId': '66363217',\n",
       "  'authorName': 'Alessio Miaschi',\n",
       "  'abstract': 'In this paper we present an NLP-based approach for tracking the evolution of written language competence in L2 Spanish learners using a wide range of linguistic features automatically extracted from students’ written productions. Beyond reporting classification results for different scenarios, we explore the connection between the most predictive features and the teaching curriculum, finding that our set of linguistic features often reflect the explicit instructions that students receive during each course.',\n",
       "  'year': 2020,\n",
       "  'venue': 'BEA'},\n",
       " {'paperId': '08fc2cdb4b2f8e81705d9c53c334213c34d1dbaf',\n",
       "  'title': 'Linguistically Motivated Descriptive Term Selection',\n",
       "  'authorId': '145848824',\n",
       "  'authorName': 'Karen Spärck Jones',\n",
       "  'abstract': 'A linguistically motivated approach to indexing, that is the provision of descriptive terms for texts of any kind, is presented and illustrated. The approach is designed to achieve good, i.e. accurate and flexible, indexing by identifying index term sources in the meaning representations built by a powerful general purpose analyser, and providing a range of text expressions constituting semantic and syntactic variants for each term concept. Indexing is seen as a legitimate form of shallow text processing, but one requiring serious semantically based language processing, particularly to obtain well-founded complex terms, which is the main objective of the project described. The type of indexing strategy described is further seen as having utility in a range of applications environments.',\n",
       "  'year': 1984,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '2db236ce426f65df381fbf48273fc24b3f0726d4',\n",
       "  'title': 'Investigating Continuous Space Language Models for Machine Translation Quality Estimation',\n",
       "  'authorId': '39888194',\n",
       "  'authorName': 'Kashif Shah',\n",
       "  'abstract': 'We present novel features designed with a deep neural network for Machine Translation (MT) Quality Estimation (QE). The features are learned with a Continuous Space Language Model to estimate the probabilities of the source and target segments. These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including postediting effort, human translation edit rate, post-editing time and METEOR. Results show significant improvements in prediction over the baseline, as well as over systems trained on state of the art feature sets for all datasets. More notably, the addition of the newly proposed features improves over the best QE systems in WMT12 and WMT14 by a significant margin.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'd461dbb4ecb4bf1c38cc25feaedbd4ce902d039d',\n",
       "  'title': 'Exploring the Role of Prior Beliefs for Argument Persuasion',\n",
       "  'authorId': '41152329',\n",
       "  'authorName': 'Esin Durmus',\n",
       "  'abstract': 'Public debate forums provide a common platform for exchanging opinions on a topic of interest. While recent studies in natural language processing (NLP) have provided empirical evidence that the language of the debaters and their patterns of interaction play a key role in changing the mind of a reader, research in psychology has shown that prior beliefs can affect our interpretation of an argument and could therefore constitute a competing alternative explanation for resistance to changing one’s stance. To study the actual effect of language use vs. prior beliefs on persuasion, we provide a new dataset and propose a controlled setting that takes into consideration two reader-level factors: political and religious ideology. We find that prior beliefs affected by these reader-level factors play a more important role than language use effects and argue that it is important to account for them in NLP studies of persuasion.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '7a5036d3841f730fb75832a43b4fc7d47a93ab01',\n",
       "  'title': 'Birds of a Feather Linked Together: A Discriminative Topic Model using Link-based Priors',\n",
       "  'authorId': '47718618',\n",
       "  'authorName': 'Weiwei Yang',\n",
       "  'abstract': 'A wide range of applications, from social media to scientific literature analysis, involve graphs in which documents are connected by links. We introduce a topic model for link prediction based on the intuition that linked documents will tend to have similar topic distributions, integrating a max-margin learning criterion and lexical term weights in the loss function. We validate our approach on the tweets from 2,000 Sina Weibo users and evaluate our model’s reconstruction of the social network.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b8d20c8b6a480c0f50fb9ef35280786321efdfad',\n",
       "  'title': 'A Tool Kit for Lexicon Building',\n",
       "  'authorId': '1923342',\n",
       "  'authorName': 'T. Ahlswede',\n",
       "  'abstract': 'This paper describes a set of interactive routines that can be used to create, maintain, and update a computer lexicon. The routines are available to the user as a set of commands resembling a simple operating system. The lexicon produced by this system is based on lexical-semantic relations, but is compatible with a variety of other models of lexicon structure. The lexicon builder is suitable for the generation of moderate-sized vocabularies and has been used to construct a lexicon for a small medical expert system. A future version of the lexicon builder will create a much larger lexicon by parsing definitions from machine-readable dictionaries.',\n",
       "  'year': 1985,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1fc61456c97fc79f1df6cb1bffb6a34f77d3049e',\n",
       "  'title': 'DOMLIN at SemEval-2019 Task 8: Automated Fact Checking exploiting Ratings in Community Question Answering Forums',\n",
       "  'authorId': '146552774',\n",
       "  'authorName': 'Dominik Stammbach',\n",
       "  'abstract': 'In the following, we describe our system developed for the Semeval2019 Task 8. We fine-tuned a BERT checkpoint on the qatar living forum dump and used this checkpoint to train a number of models. Our hand-in for subtask A consists of a fine-tuned classifier from this BERT checkpoint. For subtask B, we first have a classifier deciding whether a comment is factual or non-factual. If it is factual, we retrieve intra-forum evidence and using this evidence, have a classifier deciding the comment’s veracity. We trained this classifier on ratings which we crawled from qatarliving.com',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'c8725f13be7434b69738491c66b45c9225258253',\n",
       "  'title': 'The Web as a Knowledge-Base for Answering Complex Questions',\n",
       "  'authorId': '12371246',\n",
       "  'authorName': 'Alon Talmor',\n",
       "  'abstract': 'Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, ComplexWebQuestions, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'd7aa3c65eae71776ccc3cddafb1d4b288deae2cb',\n",
       "  'title': 'A Masked Segmental Language Model for Unsupervised Natural Language Segmentation',\n",
       "  'authorId': '2091009489',\n",
       "  'authorName': 'C.M. Downey',\n",
       "  'abstract': 'We introduce a Masked Segmental Language Model (MSLM) for joint language modeling and unsupervised segmentation. While near-perfect supervised methods have been developed for segmenting human-like linguistic units in resource-rich languages such as Chinese, many of the world’s languages are both morphologically complex, and have no large dataset of “gold” segmentations for supervised training. Segmental Language Models offer a unique approach by conducting unsupervised segmentation as the byproduct of a neural language modeling objective. However, current SLMs are limited in their scalability due to their recurrent architecture. We propose a new type of SLM for use in both unsupervised and lightly supervised segmentation tasks. The MSLM is built on a span-masking transformer architecture, harnessing a masked bidirectional modeling context and attention, as well as adding the potential for model scalability. In a series of experiments, our model outperforms the segmentation quality of recurrent SLMs on Chinese, and performs similarly to the recurrent model on English.',\n",
       "  'year': 2021,\n",
       "  'venue': 'SIGMORPHON'},\n",
       " {'paperId': '13fe71da009484f240c46f14d9330e932f8de210',\n",
       "  'title': 'Long Short-Term Memory-Networks for Machine Reading',\n",
       "  'authorId': '1941442',\n",
       "  'authorName': 'Jianpeng Cheng',\n",
       "  'abstract': 'In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '337cb8daf59b39830c6b5dc2783a0485da3afd10',\n",
       "  'title': 'Educational Content Generation for Business and Administration FL Courses with the NBU PLT Platform',\n",
       "  'authorId': '70387109',\n",
       "  'authorName': 'M. Stambolieva',\n",
       "  'abstract': 'The paper presents part of an ongoing project of the Laboratory for Language Technologies of New Bulgarian University – “An e-Platform for Language Teaching (PLT)” – the development of corpus-based teaching content for Business English courses. The presentation offers information on: 1/ corpus creation and corpus management with PLT; 2/ PLT corpus annotation; 3/ language task generation and the Language Task Bank (LTB); 4/ content transfer to the NBU Moodle platform, test generation and feedback on student performance.',\n",
       "  'year': 2017,\n",
       "  'venue': ''},\n",
       " {'paperId': '4c70bdfe7bbdf4899a50e25f6fdaff0d7ee97ac1',\n",
       "  'title': 'Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer',\n",
       "  'authorId': '49339267',\n",
       "  'authorName': 'Yanpeng Zhao',\n",
       "  'abstract': 'Machines that can represent and describe environmental soundscapes have practical potential, e.g., for audio tagging and captioning. Prevailing learning paradigms of audio-text connections have been relying on parallel audio-text data, which is, however, scarcely available on the web. We propose VIP-ANT that induces Audio-Text alignment without using any parallel audio-text data. Our key idea is to share the image modality between bi-modal image-text representations and bi-modal image-audio representations; the image modality functions as a pivot and connects audio and text in a tri-modal embedding space implicitly. In a difficult zero-shot setting with no paired audio-text data, our model demonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio classification tasks, and even surpasses the supervised state of the art for Clotho caption retrieval (with audio queries) by 2.2% R@1. We further investigate cases of minimal audio-text supervision, finding that, e.g., just a few hundred supervised audio-text pairs increase the zero-shot audio classification accuracy by 8% on US8K. However, to match human parity on some zero-shot tasks, our empirical scaling experiments suggest that we would need about 2^{21} \\\\approx 2\\\\text{M} supervised audio-caption pairs. Our work opens up new avenues for learning audio-text connections with little to no parallel audio-text data.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'd10407b86c6f125571322d81777b74a4ef26c4ba',\n",
       "  'title': 'Deep Neural Networks for Syntactic Parsing of Morphologically Rich Languages',\n",
       "  'authorId': '33035064',\n",
       "  'authorName': 'Joël Legrand',\n",
       "  'abstract': 'Morphologically rich languages (MRL) are languages in which much of the structural information is contained at the wordlevel, leading to high level word-form variation. Historically, syntactic parsing has been mainly tackled using generative models. These models assume input features to be conditionally independent, making difficult to incorporate arbitrary features. In this paper, we investigate the greedy discriminative parser described in (Legrand and Collobert, 2015), which relies on word embeddings, in the context of MRL. We propose to learn morphological embeddings and propagate morphological information through the tree using a recursive composition procedure. Experiments show that such embeddings can dramatically improve the average performance on different languages. Moreover, it yields state-of-the art performance for a majority of languages.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ff7c137acd4820ace838f37609ef716f3bebdd76',\n",
       "  'title': 'CAiRE_HKUST at SemEval-2019 Task 3: Hierarchical Attention for Dialogue Emotion Classification',\n",
       "  'authorId': '9162688',\n",
       "  'authorName': 'Genta Indra Winata',\n",
       "  'abstract': 'Detecting emotion from dialogue is a challenge that has not yet been extensively surveyed. One could consider the emotion of each dialogue turn to be independent, but in this paper, we introduce a hierarchical approach to classify emotion, hypothesizing that the current emotional state depends on previous latent emotions. We benchmark several feature-based classifiers using pre-trained word and emotion embeddings, state-of-the-art end-to-end neural network models, and Gaussian processes for automatic hyper-parameter search. In our experiments, hierarchical architectures consistently give significant improvements, and our best model achieves a 76.77% F1-score on the test set.',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '0035bf712b1ce5da56f8d6c04ff9ebbfcf01485d',\n",
       "  'title': 'Detecting Extraneous Content in Podcasts',\n",
       "  'authorId': '5022028',\n",
       "  'authorName': 'S. Reddy',\n",
       "  'abstract': 'Podcast episodes often contain material extraneous to the main content, such as advertisements, interleaved within the audio and the written descriptions. We present classifiers that leverage both textual and listening patterns in order to detect such content in podcast descriptions and audio transcripts. We demonstrate that our models are effective by evaluating them on the downstream task of podcast summarization and show that we can substantively improve ROUGE scores and reduce the extraneous content generated in the summaries.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '6d6ebc9ebe7ae1e5fdc426e56e35f266b802a48f',\n",
       "  'title': 'Abu-MaTran at WMT 2015 Translation Task: Morphological Segmentation and Web Crawling',\n",
       "  'authorId': '1731383',\n",
       "  'authorName': 'Raphaël Rubino',\n",
       "  'abstract': 'This paper presents the machine translation systems submitted by the Abu-MaTran project for the Finnish‐English language pair at the WMT 2015 translation task. We tackle the lack of resources and complex morphology of the Finnish language by (i) crawling parallel and monolingual data from the Web and (ii) applying rule-based and unsupervised methods for morphological segmentation. Several statistical machine translation approaches are evaluated and then combined to obtain our final submissions, which are the top performing English-to-Finnish unconstrained (all automatic metrics) and constrained (BLEU), and Finnish-to-English constrained (TER) systems.',\n",
       "  'year': 2015,\n",
       "  'venue': 'WMT@EMNLP'},\n",
       " {'paperId': 'b9485d1e2c66c3ae452ec4903c2a157caef4d2ed',\n",
       "  'title': 'Temporal Common Sense Acquisition with Minimal Supervision',\n",
       "  'authorId': '145360756',\n",
       "  'authorName': 'Ben Zhou',\n",
       "  'abstract': 'Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model. Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews). It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT. Thus, it will be an important component of temporal NLP.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '73c06804ff4585925456e616f2202276e15621c5',\n",
       "  'title': 'Langsmith: An Interactive Academic Text Revision System',\n",
       "  'authorId': '119804885',\n",
       "  'authorName': 'Takumi Ito',\n",
       "  'abstract': 'Despite the current diversity and inclusion initiatives in the academic community, researchers with a non-native command of English still face significant obstacles when writing papers in English. This paper presents the Langsmith editor, which assists inexperienced, non-native researchers to write English papers, especially in the natural language processing (NLP) field. Our system can suggest fluent, academic-style sentences to writers based on their rough, incomplete phrases or sentences. The system also encourages interaction between human writers and the computerized revision system. The experimental results demonstrated that Langsmith helps non-native English-speaker students write papers in English. The system is available at https://emnlp-demo.editor. langsmith.co.jp/.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'aed5b8fc320a6e22e152eccff8ae571b667863ad',\n",
       "  'title': 'Self-Learning Architecture for Natural Language Generation',\n",
       "  'authorId': '51940263',\n",
       "  'authorName': 'Hyungtak Choi',\n",
       "  'abstract': 'In this paper, we propose a self-learning architecture for generating natural language templates for conversational assistants. Generating templates to cover all the combinations of slots in an intent is time consuming and labor-intensive. We examine three different models based on our proposed architecture - Rule-based model, Sequence-to-Sequence (Seq2Seq) model and Semantically Conditioned LSTM (SC-LSTM) model for the IoT domain - to reduce the human labor required for template generation. We demonstrate the feasibility of template generation for the IoT domain using our self-learning architecture. In both automatic and human evaluation, the self-learning architecture outperforms previous works trained with a fully human-labeled dataset. This is promising for commercial conversational assistant solutions.',\n",
       "  'year': 2018,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': '06f7ff39f1bb1ebfbe4dbddc70af30b3bde3ac69',\n",
       "  'title': 'Utilization of Nganasan digital resources: a statistical approach to vowel harmony',\n",
       "  'authorId': '52220762',\n",
       "  'authorName': 'L. Fejes',\n",
       "  'abstract': 'According to the wide-spread belief, although Nganasan has vowel harmony, the harmonic class of a given stem is unpredictable, completely lexicalized. The research made on two different digital sources of Nganasan (a lexicon of a morphological analyzer with harmonic class of the stems tagged and a morphologically annotated corpus) shows that in most of the cases the harmonic class of stems is well predictable based on the vowels in it. Nganasan vowels belong to two harmonic classes except for one neutral vowel.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': 'f26f17ec49f2593bcc926051394871480a80c0c2',\n",
       "  'title': 'Density Matching for Bilingual Word Embedding',\n",
       "  'authorId': '2384711',\n",
       "  'authorName': 'Chunting Zhou',\n",
       "  'abstract': 'Recent approaches to cross-lingual word embedding have generally been based on linear transformations between the sets of embedding vectors in the two languages. In this paper, we propose an approach that instead expresses the two monolingual embedding spaces as probability densities defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. The method requires no explicit supervision, and can be learned with only a seed dictionary of words that have identical strings. We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving robustness and generalization to mappings between difficult language pairs or word pairs. On a benchmark data set of bilingual lexicon induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results, with particularly strong results being found on etymologically distant and/or morphologically rich languages.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '5de13c6a81b12676e31808e91527da4a7c24b914',\n",
       "  'title': 'Using Distributional Semantics to Trace Influence and Imitation in Romantic Orientalist Poetry',\n",
       "  'authorId': '35078346',\n",
       "  'authorName': 'Nitish Aggarwal',\n",
       "  'abstract': 'In this paper, we investigate whether textual analysis can yield evidence of shared vocabulary or formal textual characteristics in the works of 19th century poets Lord Byron and Thomas Moore in the genre of Romantic Orientalism. In particular, we identify and trace Byron’s influence on Moore’s writings to query whether Moore imitated Byron, as many reviewers of the time suggested. We use a Distributional Semantic Model (DSM) to analyze if there is a shared vocabulary of Romantic Orientalism, or if it is possible to characterize a literary genre in terms of vocabulary, rather than in terms of the particular plots, characters and themes. We discuss the results that DSM models are able to provide for an abstract overview of the influence of Lord Byron’s work on Thomas Moore.',\n",
       "  'year': 2014,\n",
       "  'venue': ''},\n",
       " {'paperId': 'b6b38f694ebb0d5719cddcdfaeee98cac32968ae',\n",
       "  'title': 'Interpreting Open-Domain Modifiers: Decomposition of Wikipedia Categories into Disambiguated Property-Value Pairs',\n",
       "  'authorId': '1724629',\n",
       "  'authorName': 'Marius Pasca',\n",
       "  'abstract': 'This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century’) within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories. In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semantically-anchored properties, rather than as mere strings in a previous method. It does so at a better trade-off between precision and recall.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6818f2b2fefee4ff9bc21ba67cb36e69a8a71b55',\n",
       "  'title': 'Identifying Word Translations in Non-Parallel Texts',\n",
       "  'authorId': '153584694',\n",
       "  'authorName': 'R. Rapp',\n",
       "  'abstract': 'Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts. This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts. The method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages.',\n",
       "  'year': 1995,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '5d1e7f2ff91c2b7c9828d5b8874cb14896e535e9',\n",
       "  'title': 'Aggregation Improves Learning: Experiments in Natural Language Generation for Intelligent Tutoring Systems',\n",
       "  'authorId': '1745798',\n",
       "  'authorName': 'Barbara Maria Di Eugenio',\n",
       "  'abstract': 'To improve the interaction between students and an intelligent tutoring system, we developed two Natural Language generators, that we systematically evaluated in a three way comparison that included the original system as well. We found that the generator which intuitively produces the best language does engender the most learning. Specifically, it appears that functional aggregation is responsible for the improvement.',\n",
       "  'year': 2005,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a53e6d65d8ef4d46dcabe347377f10f63507b5e8',\n",
       "  'title': 'Ranking election issues through the lens of social media',\n",
       "  'authorId': '3093086',\n",
       "  'authorName': 'Stephen Wan',\n",
       "  'abstract': 'Public events are often accompanied by a social media commentary that documents the public opinion and topics of importance related to these events. In this work, we describe work in collaboration with the State Library of New South Wales (NSW) to archive the social media commentary for the Australian state election in NSW, in March 2015, as a record for social scientists and historians to study in the years to come. Here, we provide an example of how one might utilise this data set, with an analysis of the data focusing on election issues. Specifically, we describe a method to produce rankings of election issues, which we find to correlate moderately to those of official commentators. Furthermore, using our time-series data, we show how the importance of key issues stabilises approximately a month before the actual election.',\n",
       "  'year': 2015,\n",
       "  'venue': 'LaTeCH@ACL'},\n",
       " {'paperId': '4fb0a181676a5200bc6e53dea1b770613c164aab',\n",
       "  'title': 'Leveraging Graph to Improve Abstractive Multi-Document Summarization',\n",
       "  'authorId': '48624966',\n",
       "  'authorName': 'Wei Li',\n",
       "  'abstract': 'Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '5a566513f417fa71d581d0b072fd25e68f99821f',\n",
       "  'title': 'Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language Models',\n",
       "  'authorId': '79327062',\n",
       "  'authorName': 'Yiben Yang',\n",
       "  'abstract': 'Recurrent neural network language models (RNNLM) form a valuable foundation for many NLP systems, but training the models can be computationally expensive, and may take days to train on a large corpus. We explore a technique that uses large corpus n-gram statistics as a regularizer for training a neural network LM on a smaller corpus. In experiments with the Billion-Word and Wikitext corpora, we show that the technique is effective, and more time-efficient than simply training on a larger sequential corpus. We also introduce new strategies for selecting the most informative n-grams, and show that these boost efficiency.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'f997c2f1f668b942c4cccd425bc192df651ed516',\n",
       "  'title': 'Coverage Embedding Models for Neural Machine Translation',\n",
       "  'authorId': '2013337',\n",
       "  'authorName': 'Haitao Mi',\n",
       "  'abstract': 'In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '1239206f4f5af078f7bfcc248cb654dc0ffbae00',\n",
       "  'title': 'JBNU at MRP 2019: Multi-level Biaffine Attention for Semantic Dependency Parsing',\n",
       "  'authorId': '1723468',\n",
       "  'authorName': 'Seung-Hoon Na',\n",
       "  'abstract': 'This paper describes Jeonbuk National University (JBNU)’s system for the 2019 shared task on Cross-Framework Meaning Representation Parsing (MRP 2019) at the Conference on Computational Natural Language Learning. Of the five frameworks, we address only the DELPH-IN MRS Bi-Lexical Dependencies (DP), Prague Semantic Dependencies (PSD), and Universal Conceptual Cognitive Annotation (UCCA) frameworks. We propose a unified parsing model using biaffine attention (Dozat and Manning, 2017), consisting of 1) a BERT-BiLSTM encoder and 2) a biaffine attention decoder. First, the BERT-BiLSTM for sentence encoder uses BERT to compose a sentence’s wordpieces into word-level embeddings and subsequently applies BiLSTM to word-level representations. Second, the biaffine attention decoder determines the scores for an edge’s existence and its labels based on biaffine attention functions between roledependent representations. We also present multi-level biaffine attention models by combining all the role-dependent representations that appear at multiple intermediate layers.',\n",
       "  'year': 2019,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': '622561b8e631dd926d5e6927812288bd22c5f231',\n",
       "  'title': 'Analogies in Complex Verb Meaning Shifts: the Effect of Affect in Semantic Similarity Models',\n",
       "  'authorId': '2797344',\n",
       "  'authorName': 'Maximilian Köper',\n",
       "  'abstract': 'We present a computational model to detect and distinguish analogies in meaning shifts between German base and complex verbs. In contrast to corpus-based studies, a novel dataset demonstrates that “regular” shifts represent the smallest class. Classification experiments relying on a standard similarity model successfully distinguish between four types of shifts, with verb classes boosting the performance, and affective features for abstractness, emotion and sentiment representing the most salient indicators.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'df80d1d53e8c8e17eb65174addeec668b8a59b71',\n",
       "  'title': 'Greedy Transition-Based Dependency Parsing with Stack LSTMs',\n",
       "  'authorId': '143668305',\n",
       "  'authorName': 'Miguel Ballesteros',\n",
       "  'abstract': 'We introduce a greedy transition-based parser that learns to represent parser states using recurrent neural networks. Our primary innovation that enables us to do this efficiently is a new control structure for sequential neural networks—the stack long short-term memory unit (LSTM). Like the conventional stack data structures used in transition-based parsers, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. Our model captures three facets of the parser’s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of transition actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. In addition, we compare two different word representations: (i) standard word vectors based on look-up tables and (ii) character-based models of words. Although standard word embedding models work well in all languages, the character-based models improve the handling of out-of-vocabulary words, particularly in morphologically rich languages. Finally, we discuss the use of dynamic oracles in training the parser. During training, dynamic oracles alternate between sampling parser states from the training data and from the model as it is being learned, making the model more robust to the kinds of errors that will be made at test time. Training our model with dynamic oracles yields a linear-time greedy parser with very competitive performance.',\n",
       "  'year': 2017,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': 'bd556cbd38292666450f1ed16474f8b82074379f',\n",
       "  'title': 'hyp: A Toolkit for Representing, Manipulating, and Optimizing Hypergraphs',\n",
       "  'authorId': '40262269',\n",
       "  'authorName': 'Markus Dreyer',\n",
       "  'abstract': 'We present hyp, an open-source toolkit for the representation, manipulation, and optimization of weighted directed hypergraphs. hyp provides compose, project, invert functionality, k-best path algorithms, the inside and outside algorithms, and more. Finite-state machines are modeled as a special case of directed hypergraphs. hyp consists of a C++ API, as well as a command line tool, and is available for download at github.com/sdl-research/hyp.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'ebe84feeed3cf6a297f5a2fa504647e3eeba05b5',\n",
       "  'title': 'Solving Hard Coreference Problems',\n",
       "  'authorId': '1981962',\n",
       "  'authorName': 'Haoruo Peng',\n",
       "  'abstract': 'Coreference resolution is a key problem in natural language understanding that still escapes reliable solutions. One fundamental difficulty has been that of resolving instances involving pronouns since they often require deep language understanding and use of background knowledge. In this paper we propose an algorithmic solution that involves a new representation for the knowledge required to address hard coreference problems, along with a constrained optimization framework that uses this knowledge in coreference decision making. Our representation, Predicate Schemas, is instantiated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '55ace73cb8f7c3c4bd6d8ead45c0ba6193d1afda',\n",
       "  'title': 'A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages',\n",
       "  'authorId': '3054462',\n",
       "  'authorName': 'Clara Vania',\n",
       "  'abstract': 'Parsers are available for only a handful of the world’s languages, since they require lots of training data. How far can we get with just a small amount of training data? We systematically compare a set of simple strategies for improving low-resource parsers: data augmentation, which has not been tested before; cross-lingual training; and transliteration. Experimenting on three typologically diverse low-resource languages—North Sami, Galician, and Kazah—We find that (1) when only the low-resource treebank is available, data augmentation is very helpful; (2) when a related high-resource treebank is available, cross-lingual training is helpful and complements data augmentation; and (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP/IJCNLP'},\n",
       " {'paperId': 'b054900959b2f464375160bbab9316804b1ee3f7',\n",
       "  'title': 'Findings of the Third Workshop on Automatic Simultaneous Translation',\n",
       "  'authorId': '2129412850',\n",
       "  'authorName': 'Ruiqing Zhang',\n",
       "  'abstract': 'This paper reports the results of the shared task we hosted on the Third Workshop of Automatic Simultaneous Translation (AutoSimTrans). The shared task aims to promote the development of text-to-text and speech-to-text simultaneous translation, and includes Chinese-English and English-Spanish tracks. The number of systems submitted this year has increased fourfold compared with last year. Additionally, the top 1 ranked system in the speech-to-text track is the first end-to-end submission we have received in the past three years, which has shown great potential. This paper reports the results and descriptions of the 14 participating teams, compares different evaluation metrics, and revisits the ranking method.',\n",
       "  'year': 2022,\n",
       "  'venue': 'AUTOSIMTRANS'},\n",
       " {'paperId': 'b3564be8b79f25585acb035f3deaf4ae93c26d8f',\n",
       "  'title': 'Theoretical Limitations of Self-Attention in Neural Sequence Models',\n",
       "  'authorId': '46686009',\n",
       "  'authorName': 'Michael Hahn',\n",
       "  'abstract': 'Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.',\n",
       "  'year': 2019,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '0b78dfaad91351125ba5ede0190e56a6920fc2b5',\n",
       "  'title': 'The Good, the Bad, and the Disagreement: Complex ground truth in rhetorical structure analysis',\n",
       "  'authorId': '2129805',\n",
       "  'authorName': 'Debopam Das',\n",
       "  'abstract': 'We present a proposal to analyze disagreement in Rhetorical Structure Theory annotation which takes into account what we consider “legitimate” disagreements. In rhetorical analysis, as in many other pragmatic annotation tasks, a certain amount of disagreement is to be expected, and it is important to distinguish true mistakes from legitimate disagreements due to different possible interpretations of the structure and intention of a text. Using different sets of annotations in German and English, we present an analysis of such possible disagreements, and propose an underspecified representation that captures the disagreements.',\n",
       "  'year': 2017,\n",
       "  'venue': ''},\n",
       " {'paperId': '31c872514c28a172f7f0221c8596aa5bfcdb9e98',\n",
       "  'title': '75 Languages, 1 Model: Parsing Universal Dependencies Universally',\n",
       "  'authorId': '51208108',\n",
       "  'authorName': 'D. Kondratyuk',\n",
       "  'abstract': 'We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'cec94332ff4618b28d0adb6ecddd2ef7a87d3188',\n",
       "  'title': 'Relation Extraction with Type-aware Map Memories of Word Dependencies',\n",
       "  'authorId': '2116379168',\n",
       "  'authorName': 'Guimin Chen',\n",
       "  'abstract': 'Relation extraction is an important task in information extraction and retrieval that aims to extract relations among the given entities from running texts. To achieve a good performance for this task, previous studies have shown that a good modeling of the contextual information is required, where the dependency tree of the input sentence can be a beneficial source among different types of contextual information. However, most of these studies focus on the dependency connections between words with limited attention paid to exploiting dependency types. In addition, they often treat different dependency connections equally in modeling so that suffer from the noise (inaccurate dependency parses) in the auto-generated dependency tree. In this paper, we propose a neural approach for relation extraction, with type-aware map memories (TaMM) for encoding dependency types obtained from an off-theshelf dependency parser for the input sentence. Specifically, for each word in an entity, TaMM maps all associated words along with the dependencies among them to memory slots and then assigns a weight to each slot according to its contribution to relation extraction. Our approach not only leverages dependency connections and types between words, but also distinguishes reliable dependency information from noisy ones and appropriately model them. The effectiveness of our approach is demonstrated by the experiments on two English benchmark datasets, where our approach achieves state-ofthe-art performance on both datasets.1',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '0de7c83b3faa48e21e43aab4a18b84c662f0dd30',\n",
       "  'title': 'Ranking Right-Wing Extremist Social Media Profiles by Similarity to Democratic and Extremist Groups',\n",
       "  'authorId': '3040010',\n",
       "  'authorName': 'Matthias Hartung',\n",
       "  'abstract': 'Social media are used by an increasing number of political actors. A small subset of these is interested in pursuing extremist motives such as mobilization, recruiting or radicalization activities. In order to counteract these trends, online providers and state institutions reinforce their monitoring efforts, mostly relying on manual workflows. We propose a machine learning approach to support manual attempts towards identifying right-wing extremist content in German Twitter profiles. Based on a fine-grained conceptualization of right-wing extremism, we frame the task as ranking each individual profile on a continuum spanning different degrees of right-wing extremism, based on a nearest neighbour approach. A quantitative evaluation reveals that our ranking model yields robust performance (up to 0.81 F_1 score) when being used for predicting discrete class labels. At the same time, the model provides plausible continuous ranking scores for a small sample of borderline cases at the division of right-wing extremism and New Right political movements.',\n",
       "  'year': 2017,\n",
       "  'venue': 'WASSA@EMNLP'},\n",
       " {'paperId': '761e606b19c48d03b077d5b9c37652260d18f073',\n",
       "  'title': 'Inducing Document Structure for Aspect-based Summarization',\n",
       "  'authorId': '2875615',\n",
       "  'authorName': 'Lea Frermann',\n",
       "  'abstract': 'Automatic summarization is typically treated as a 1-to-1 mapping from document to summary. Documents such as news articles, however, are structured and often cover multiple topics or aspects; and readers may be interested in only some of them. We tackle the task of aspect-based summarization, where, given a document and a target aspect, our models generate a summary centered around the aspect. We induce latent document structure jointly with an abstractive summarization objective, and train our models in a scalable synthetic setup. In addition to improvements in summarization over topic-agnostic baselines, we demonstrate the benefit of the learnt document structure: we show that our models (a) learn to accurately segment documents by aspect; (b) can leverage the structure to produce both abstractive and extractive aspect-based summaries; and (c) that structure is particularly advantageous for summarizing long documents. All results transfer from synthetic training documents to natural news articles from CNN/Daily Mail and RCV1.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'dbc169f84f10b436a120b88a071339672e5ac996',\n",
       "  'title': 'Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering',\n",
       "  'authorId': '1403069822',\n",
       "  'authorName': 'Daniel Fernández-González',\n",
       "  'abstract': 'Discontinuous constituent parsers have always lagged behind continuous approaches in terms of accuracy and speed, as the presence of constituents with discontinuous yield introduces extra complexity to the task. However, a discontinuous tree can be converted into a continuous variant by reordering tokens. Based on that, we propose to reduce discontinuous parsing to a continuous problem, which can then be directly solved by any off-the-shelf continuous parser. To that end, we develop a Pointer Network capable of accurately generating the continuous token arrangement for a given input sentence and define a bijective function to recover the original order. Experiments on the main benchmarks with two continuous parsers prove that our approach is on par in accuracy with purely discontinuous state-of-the-art algorithms, but considerably faster.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '13e484ebd19f5e28f801d0d6bc4fb371e6155bd0',\n",
       "  'title': 'Design and Evolution of a Language Technologies Curriculum',\n",
       "  'authorId': '2260563',\n",
       "  'authorName': 'Robert E. Frederking',\n",
       "  'abstract': 'The Language Technologies Institute (LTI) of the School of Computer Science at Carnegie Mellon University is one of the largest programs of its kind. We present here the initial design and subsequent evolution of our MS and PhD programs in Language Technologies. The motivations for the design and evolution are also presented.',\n",
       "  'year': 2002,\n",
       "  'venue': 'ACL 2002'},\n",
       " {'paperId': 'afd34cac67c749c26289cc3a4eec4edf096fbc54',\n",
       "  'title': 'NCSU_SAS_SAM: Deep Encoding and Reconstruction for Normalization of Noisy Text',\n",
       "  'authorId': '1403018058',\n",
       "  'authorName': 'Samuel P. Leeman-Munk',\n",
       "  'abstract': 'As a participant in the W-NUT Lexical Normalization for English Tweets challenge, we use deep learning to address the constrained task. Specifically, we use a combination of two augmented feed forward neural networks, a flagger that identifies words to be normalized and a normalizer, to take in a single token at a time and output a corrected version of that token. Despite avoiding off-the-shelf tools trained on external data and being an entirely context-free model, our system still achieved an F1-score of 81.49%, comfortably surpassing the next runner up by 1.5% and trailing the second place model by only 0.26%.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NUT@IJCNLP'},\n",
       " {'paperId': 'fe2b861be3e24955fb5cc9d3c7cab1af6f17d9b7',\n",
       "  'title': 'OPI@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text using RoBERTa Pre-trained Language Models',\n",
       "  'authorId': '2067171871',\n",
       "  'authorName': 'Rafal Poswiata',\n",
       "  'abstract': 'This paper presents our winning solution for the Shared Task on Detecting Signs of Depression from Social Media Text at LT-EDI-ACL2022. The task was to create a system that, given social media posts in English, should detect the level of depression as ‘not depressed’, ‘moderately depressed’ or ‘severely depressed’. We based our solution on transformer-based language models. We fine-tuned selected models: BERT, RoBERTa, XLNet, of which the best results were obtained for RoBERTa. Then, using the prepared corpus, we trained our own language model called DepRoBERTa (RoBERTa for Depression Detection). Fine-tuning of this model improved the results. The third solution was to use the ensemble averaging, which turned out to be the best solution. It achieved a macro-averaged F1-score of 0.583. The source code of prepared solution is available at https://github.com/rafalposwiata/depression-detection-lt-edi-2022.',\n",
       "  'year': 2022,\n",
       "  'venue': 'LTEDI'},\n",
       " {'paperId': '5b7a4b98018e6ae5b98fbce2f66bee8e42b1fa98',\n",
       "  'title': 'A survey of part-of-speech tagging approaches applied to K’iche’',\n",
       "  'authorId': '3262036',\n",
       "  'authorName': 'Francis M. Tyers',\n",
       "  'abstract': 'We study the performance of several popular neural part-of-speech taggers from the Universal Dependencies ecosystem on Mayan languages using a small corpus of 1435 annotated K’iche’ sentences consisting of approximately 10,000 tokens, with encouraging results: F\\\\_1 scores 93%+ on lemmatisation, part-of-speech and morphological feature assignment. The high performance motivates a cross-language part-of-speech tagging study, where K’iche’-trained models are evaluated on two other Mayan languages, Kaqchikel and Uspanteko: performance on Kaqchikel is good, 63-85%, and on Uspanteko modest, 60-71%. Supporting experiments lead us to conclude the relative diversity of morphological features as a plausible explanation for the limiting factors in cross-language tagging performance, providing some direction for future sentence annotation and collection work to support these and other Mayan languages.',\n",
       "  'year': 2021,\n",
       "  'venue': 'AMERICASNLP'},\n",
       " {'paperId': 'c8dfdb6bc17094fc1c35757a0020dea8d813b7b6',\n",
       "  'title': 'Improving Lexical Embeddings with Semantic Knowledge',\n",
       "  'authorId': '2482533',\n",
       "  'authorName': 'Mo Yu',\n",
       "  'abstract': 'Word embeddings learned on unlabeled data are a popular tool in semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1244287f66a38c6cf6dfbe18f22ccbc1792433e1',\n",
       "  'title': 'Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision',\n",
       "  'authorId': '2072592545',\n",
       "  'authorName': 'Wanyu Du',\n",
       "  'abstract': 'Revision is an essential part of the human writing process. It tends to be strategic, adaptive, and, more importantly, iterative in nature. Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions. Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants. In this work, we present a human-in-the-loop iterative text revision system, Read, Revise, Repeat (R3), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions. In R3, a text revision model provides text editing suggestions for human writers, who can accept or reject the suggested edits. The accepted edits are then incorporated into the model for the next iteration of document revision. Writers can therefore revise documents iteratively by interacting with the system and simply accepting/rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions. Empirical experiments show that R3 can generate revisions with comparable acceptance rate to human writers at early revision depths, and the human-machine interaction can get higher quality revisions with fewer iterations and edits. The collected human-model interaction dataset and system code are available at https://github.com/vipulraheja/IteraTeR. Our system demonstration is available at https://youtu.be/lK08tIpEoaE.',\n",
       "  'year': 2022,\n",
       "  'venue': 'IN2WRITING'},\n",
       " {'paperId': 'f01c433acfaef63d7dfc168a507fdb09cd48032e',\n",
       "  'title': 'Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study',\n",
       "  'authorId': '151257945',\n",
       "  'authorName': 'Xiangyang Mou',\n",
       "  'abstract': 'Abstract Recent advancements in open-domain question answering (ODQA), that is, finding answers from large open-domain corpus like Wikipedia, have led to human-level performance on many datasets. However, progress in QA over book stories (Book QA) lags despite its similar task formulation to ODQA. This work provides a comprehensive and quantitative analysis about the difficulty of Book QA: (1) We benchmark the research on the NarrativeQA dataset with extensive experiments with cutting-edge ODQA techniques. This quantifies the challenges Book QA poses, as well as advances the published state-of-the-art with a ∼7% absolute improvement on ROUGE-L. (2) We further analyze the detailed challenges in Book QA through human studies.1 Our findings indicate that the event-centric questions dominate this task, which exemplifies the inability of existing QA models to handle event-oriented scenarios.',\n",
       "  'year': 2021,\n",
       "  'venue': 'Transactions of the Association for Computational Linguistics'},\n",
       " {'paperId': '31ae8cbaa2bde120ad2314a2ec0963fbd401c51e',\n",
       "  'title': 'Relevant Emotion Ranking from Text Constrained with Emotion Relationships',\n",
       "  'authorId': '1725992',\n",
       "  'authorName': 'Deyu Zhou',\n",
       "  'abstract': 'Text might contain or invoke multiple emotions with varying intensities. As such, emotion detection, to predict multiple emotions associated with a given text, can be cast into a multi-label classification problem. We would like to go one step further so that a ranked list of relevant emotions are generated where top ranked emotions are more intensely associated with text compared to lower ranked emotions, whereas the rankings of irrelevant emotions are not important. A novel framework of relevant emotion ranking is proposed to tackle the problem. In the framework, the objective loss function is designed elaborately so that both emotion prediction and rankings of only relevant emotions can be achieved. Moreover, we observe that some emotions co-occur more often while other emotions rarely co-exist. Such information is incorporated into the framework as constraints to improve the accuracy of emotion detection. Experimental results on two real-world corpora show that the proposed framework can effectively deal with emotion detection and performs remarkably better than the state-of-the-art emotion detection approaches and multi-label learning methods.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'b4338116d0e0af2698bec809f2c6ffbeb53e8abe',\n",
       "  'title': 'Implicit Discourse Relation Identification for Open-domain Dialogues',\n",
       "  'authorId': '144592155',\n",
       "  'authorName': 'Mingyu Derek Ma',\n",
       "  'abstract': 'Discourse relation identification has been an active area of research for many years, and the challenge of identifying implicit relations remains largely an unsolved task, especially in the context of an open-domain dialogue system. Previous work primarily relies on a corpora of formal text which is inherently non-dialogic, i.e., news and journals. This data however is not suitable to handle the nuances of informal dialogue nor is it capable of navigating the plethora of valid topics present in open-domain dialogue. In this paper, we designed a novel discourse relation identification pipeline specifically tuned for open-domain dialogue systems. We firstly propose a method to automatically extract the implicit discourse relation argument pairs and labels from a dataset of dialogic turns, resulting in a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. Moreover, we have taken the first steps to leverage the dialogue features unique to our task to further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '55ac7573d3eaa57262bdd4955ea75030aa2e4eb7',\n",
       "  'title': 'BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology',\n",
       "  'authorId': '1389389165',\n",
       "  'authorName': 'Luke Gessler',\n",
       "  'abstract': 'An important question concerning contextualized word embedding (CWE) models like BERT is how well they can represent different word senses, especially those in the long tail of uncommon senses. Rather than build a WSD system as in previous work, we investigate contextualized embedding neighborhoods directly, formulating a query-by-example nearest neighbor retrieval task and examining ranking performance for words and senses in different frequency bands. In an evaluation on two English sense-annotated corpora, we find that several popular CWE models all outperform a random baseline even for proportionally rare senses, without explicit sense supervision. However, performance varies considerably even among models with similar architectures and pretraining regimes, with especially large differences for rare word senses, revealing that CWE models are not all created equal when it comes to approximating word senses in their native representations.',\n",
       "  'year': 2021,\n",
       "  'venue': 'BLACKBOXNLP'},\n",
       " {'paperId': '807f6a34971e3d217373da7319c999ed3881092d',\n",
       "  'title': 'The Kyoto University Cross-Lingual Pronoun Translation System',\n",
       "  'authorId': '3209719',\n",
       "  'authorName': 'Raj Dabre',\n",
       "  'abstract': 'In this paper we describe our system we designed and implemented for the crosslingual pronoun prediction task as a part of WMT 2016. The majority of the paper will be dedicated to the system whose outputs we submitted wherein we describe the simplified mathematical model, the details of the components and the working by means of an architecture diagram which also serves as a flowchart. We then discuss the results of the official scores and our observations on the same.',\n",
       "  'year': 2016,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': 'de44046f931e0eb47a0a705540d2e00175126eb8',\n",
       "  'title': 'On the Limitations of Unsupervised Bilingual Dictionary Induction',\n",
       "  'authorId': '1700187',\n",
       "  'authorName': 'Anders Søgaard',\n",
       "  'abstract': 'Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora - seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c8b2780824c37140eeada3c451caf2d0a8b77a84',\n",
       "  'title': 'BERT-Beta: A Proactive Probabilistic Approach to Text Moderation',\n",
       "  'authorId': '1491233580',\n",
       "  'authorName': 'Fei Tan',\n",
       "  'abstract': 'Text moderation for user generated content, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling, which is demonstrated to function well in comprehensive experiments. We also propose an explanation method to communicate the model decision clearly. Both propensity scoring and interpretation benefit text moderation in a novel manner. Finally, the proposed scaling mechanism for the linear model offers useful insights beyond this work.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6faf62ac9e69e66cb92c923b749fa071554d23ca',\n",
       "  'title': 'Learning Programmatic Idioms for Scalable Semantic Parsing',\n",
       "  'authorId': '1900163',\n",
       "  'authorName': 'Srini Iyer',\n",
       "  'abstract': 'Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2% BLEU score while reducing training time by more than 50%. This improved speed enables us to scale up the model by training on an extended training set that is 5\\\\times larger, to further move up the SOTA by an additional 2.3% BLEU and 0.9% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '96b414a7924d9ceb7254b22e29b129d392850592',\n",
       "  'title': 'An Experimental Comparison of Active Learning Strategies for Partially Labeled Sequences',\n",
       "  'authorId': '2022957',\n",
       "  'authorName': 'Diego Marcheggiani',\n",
       "  'abstract': 'Active learning (AL) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier. AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms, hence limiting the tedious effort of annotating a large collection of data. We experimentally investigate the behavior of several AL strategies for sequence labeling tasks (in a partially-labeled scenario) tailored on Partially-Labeled Conditional Random Fields, on four sequence labeling tasks: phrase chunking, part-of-speech tagging, named-entity recognition, and bioentity recognition.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'ec1ce50b7a65c6ac6cd04e57b7e5521e209b9d8b',\n",
       "  'title': 'Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models',\n",
       "  'authorId': '5041757',\n",
       "  'authorName': 'Taeuk Kim',\n",
       "  'abstract': 'As it has been unveiled that pre-trained language models (PLMs) are to some extent ca-pable of recognizing syntactic concepts in natural language, much effort has been made to develop a method for extracting complete (bi-nary) parses from PLMs without training separate parsers. We improve upon this paradigm by proposing a novel chart-based method and an effective top-K ensemble technique. More-over, we demonstrate that we can broaden the scope of application of the approach into multilingual settings. Speciﬁcally, we show that by applying our method on multilingual PLMs, it becomes possible to induce non-trivial parses for sentences from nine languages in an integrated and language-agnostic manner, attain-ing performance superior or comparable to that of unsupervised PCFGs. We also verify that our approach is robust to cross-lingual transfer. Finally, we provide analyses on the inner workings of our method. For instance, we discover universal attention heads which are consistently sensitive to syntactic information irrespective of the input language.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b5aa927c906101b3f8854a29f374551e3ea64474',\n",
       "  'title': 'Pre-trained language model representations for language generation',\n",
       "  'authorId': '2068070',\n",
       "  'authorName': 'Sergey Edunov',\n",
       "  'abstract': 'Pre-trained language model representations have been successful in a wide range of language understanding tasks. In this paper, we examine different strategies to integrate pre-trained representations into sequence to sequence models and apply it to neural machine translation and abstractive summarization. We find that pre-trained representations are most effective when added to the encoder network which slows inference by only 14%. Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup. While returns diminish with more labeled data, we still observe improvements when millions of sentence-pairs are available. Finally, on abstractive summarization we achieve a new state of the art on the full text version of CNN/DailyMail.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '6276bbe6cc56234d430725a31a27939eeec88149',\n",
       "  'title': 'Content-based Models of Quotation',\n",
       "  'authorId': '46295533',\n",
       "  'authorName': 'Ansel MacLaughlin',\n",
       "  'abstract': 'We explore the task of quotability identification, in which, given a document, we aim to identify which of its passages are the most quotable, i.e. the most likely to be directly quoted by later derived documents. We approach quotability identification as a passage ranking problem and evaluate how well both feature-based and BERT-based (Devlin et al., 2019) models rank the passages in a given document by their predicted quotability. We explore this problem through evaluations on five datasets that span multiple languages (English, Latin) and genres of literature (e.g. poetry, plays, novels) and whose corresponding derived documents are of multiple types (news, journal articles). Our experiments confirm the relatively strong performance of BERT-based models on this task, with the best model, a RoBERTA sequential sentence tagger, achieving an average rho of 0.35 and NDCG@1, 5, 50 of 0.26, 0.31 and 0.40, respectively, across all five datasets.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '415da1c6e0c872b434df0d334e5a6dfad4bd0786',\n",
       "  'title': 'Posterior regularization for Joint Modeling of Multiple Structured Prediction Tasks with Soft Constraints',\n",
       "  'authorId': '37195917',\n",
       "  'authorName': 'Kartik Goyal',\n",
       "  'abstract': 'We propose a multi-task learning objective for training joint structured prediction models when no jointly annotated data is available. We use conditional random fields as the joint predictive model and train their parameters by optimizing the marginal likelihood of all available annotations, with additional posterior constraints on the distributions of the latent variables imposed to enforce agreement. Experiments on named entity recognition and part-of-speech tagging show that the proposed model outperforms independent task estimation, and the posterior constraints provide a useful mechanism for incorporating domainspecific knowledge.',\n",
       "  'year': 2016,\n",
       "  'venue': 'SPNLP@EMNLP'},\n",
       " {'paperId': 'c1ce2e7ce538116df81752a432bfabe860e6e89a',\n",
       "  'title': 'Moving on from OntoNotes: Coreference Resolution Model Transfer',\n",
       "  'authorId': '2465658',\n",
       "  'authorName': 'Patrick Xia',\n",
       "  'abstract': 'Academic neural models for coreference resolution (coref) are typically trained on a single dataset, OntoNotes, and model improvements are benchmarked on that same dataset. However, real-world applications of coref depend on the annotation guidelines and the domain of the target dataset, which often differ from those of OntoNotes. We aim to quantify transferability of coref models based on the number of annotated documents available in the target dataset. We examine eleven target datasets and find that continued training is consistently effective and especially beneficial when there are few target documents. We establish new benchmarks across several datasets, including state-of-the-art results on PreCo.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'f84bc0df95c2fcab17567494de5e5868735062d8',\n",
       "  'title': 'Linked Open Treebanks. Interlinking Syntactically Annotated Corpora in the LiLa Knowledge Base of Linguistic Resources for Latin',\n",
       "  'authorId': '2043236',\n",
       "  'authorName': 'Francesco Mambrini',\n",
       "  'abstract': 'In spite of the current availability of large collections of treebanks that can be used and queried from one common place on the web, we are still far from achieving a real interconnection, both between treebanks themselves and with other (kinds of) linguistic resources. However, making resources interoperable is a crucial requirement to maximize the contribution of each single resource, as well as to account for the linguistic complexity of the texts provided by (annotated) corpora and particu-larly by treebanks. This paper describes how dependency treebanks are interlinked in a Knowledge Base of linguistic resources for Latin based on Linked Open Data practices and standards. The Knowledge base is built to make linguistic resources interact by integrating all types of annotation applied to a particular word/text into a common representation.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)'},\n",
       " {'paperId': '2b8b59a74d815a70bbb31892ce484510480be6fe',\n",
       "  'title': 'Analyzing Argumentative Discourse Units in Online Interactions',\n",
       "  'authorId': '39013305',\n",
       "  'authorName': 'Debanjan Ghosh',\n",
       "  'abstract': 'Argument mining of online interactions is in its infancy. One reason is the lack of annotated corpora in this genre. To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation. We propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ArgMining@ACL'},\n",
       " {'paperId': 'b3df38236d47d6334357c2ecbde4351fef490fe8',\n",
       "  'title': 'AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models',\n",
       "  'authorId': '3467205',\n",
       "  'authorName': 'Harish Tayyar Madabushi',\n",
       "  'abstract': 'Despite their success in a variety of NLP tasks, pre-trained language models, due to their heavy reliance on compositionality, fail in effectively capturing the meanings of multiword expressions (MWEs), especially idioms. Therefore, datasets and methods to improve the representation of MWEs are urgently needed. Existing datasets are limited to providing the degree of idiomaticity of expressions along with the literal and, where applicable, (a single) non-literal interpretation of MWEs. This work presents a novel dataset of naturally occurring sentences containing MWEs manu-ally classiﬁed into a ﬁne-grained set of meanings, spanning both English and Portuguese. We use this dataset in two tasks designed to test i) a language model’s ability to detect idiom usage, and ii) the effectiveness of a language model in generating representations of sentences containing idioms. Our experiments demonstrate that, on the task of detecting idiomatic usage, these models perform reason-ably well in the one-shot and few-shot scenarios, but that there is signiﬁcant scope for improvement in the zero-shot scenario. On the task of representing idiomaticity, we ﬁnd that pre-training is not always effective, while ﬁne-tuning could provide a sample efﬁcient method of learning representations of sentences containing MWEs.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'bc4b6bc216a9748aa5a37e0bf7b4ca0876f57e4b',\n",
       "  'title': 'SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check',\n",
       "  'authorId': '26382255',\n",
       "  'authorName': 'Xingyi Cheng',\n",
       "  'abstract': 'Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN). The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers. These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable. Experiments are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '73ac55374fbe5d4ffcddd9ca88cb2cbd1be2630a',\n",
       "  'title': 'Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks',\n",
       "  'authorId': '2608639',\n",
       "  'authorName': 'Ningyu Zhang',\n",
       "  'abstract': 'We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings. Here, the challenge is to learn accurate “few-shot” models for classes existing at the tail of the class distribution, for which little data is available. Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail. First, we propose to leverage implicit relational knowledge among class labels from knowledge graph embeddings and learn explicit relational knowledge using graph convolution networks. Second, we integrate that relational knowledge into relation extraction model by coarse-to-fine knowledge-aware attention mechanism. We demonstrate our results for a large-scale benchmark dataset which show that our approach significantly outperforms other baselines, especially for long-tail relations.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '2dad078c48278da520d5bd67ed2e4fca0ef85e83',\n",
       "  'title': 'On the Summarization of Consumer Health Questions',\n",
       "  'authorId': '2205800',\n",
       "  'authorName': 'Asma Ben Abacha',\n",
       "  'abstract': 'Question understanding is one of the main challenges in question answering. In real world applications, users often submit natural language questions that are longer than needed and include peripheral information that increases the complexity of the question, leading to substantially more false positives in answer retrieval. In this paper, we study neural abstractive models for medical question summarization. We introduce the MeQSum corpus of 1,000 summarized consumer health questions. We explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task. In particular, we show that semantic augmentation from question datasets improves the overall performance, and that pointer-generator networks outperform sequence-to-sequence attentional models on this task, with a ROUGE-1 score of 44.16%. We also present a detailed error analysis and discuss directions for improvement that are specific to question summarization.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd3406f220c16a20b877f10f5609fa589b629cc91',\n",
       "  'title': 'Leveraging Context Information for Natural Question Generation',\n",
       "  'authorId': '1748796',\n",
       "  'authorName': 'Linfeng Song',\n",
       "  'abstract': 'The task of natural question generation is to generate a corresponding question given the input passage (fact) and answer. It is useful for enlarging the training set of QA systems. Previous work has adopted sequence-to-sequence models that take a passage with an additional bit to indicate answer position as input. However, they do not explicitly model the information between answer and other context within the passage. We propose a model that matches the answer with the passage before generating the question. Experiments show that our model outperforms the existing state of the art using rich features.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'fd17caddc98d2db2c32a0d4951491a4c170fda3c',\n",
       "  'title': 'Stance Detection in Fake News A Combined Feature Representation',\n",
       "  'authorId': '46188821',\n",
       "  'authorName': 'Bilal Ghanem',\n",
       "  'abstract': 'With the uncontrolled increasing of fake news and rumors over the Web, different approaches have been proposed to address the problem. In this paper, we present an approach that combines lexical, word embeddings and n-gram features to detect the stance in fake news. Our approach has been tested on the Fake News Challenge (FNC-1) dataset. Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title. Our proposed approach has achieved an accurate result (59.6 % Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple feature representation. Furthermore, we have investigated the importance of different lexicons in the detection of the classification labels.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': 'e8e5bdb20333b4818426f515a61e2603cf18b24f',\n",
       "  'title': 'Team yeon-zi at SemEval-2019 Task 4: Hyperpartisan News Detection by De-noising Weakly-labeled Data',\n",
       "  'authorId': '40221187',\n",
       "  'authorName': 'Nayeon Lee',\n",
       "  'abstract': 'This paper describes our system that has been submitted to SemEval-2019 Task 4: Hyperpartisan News Detection. We focus on removing the noise inherent in the hyperpartisanship dataset from both data-level and model-level by leveraging semi-supervised pseudo-labels and the state-of-the-art BERT model. Our model achieves 75.8% accuracy in the final by-article dataset without ensemble learning.',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '47f3f165f05d9cd2eca29d1559cb65824641db6b',\n",
       "  'title': 'Extend, don’t rebuild: Phrasing conditional graph modification as autoregressive sequence labelling',\n",
       "  'authorId': '20308468',\n",
       "  'authorName': 'Leon Weber',\n",
       "  'abstract': 'Deriving and modifying graphs from natural language text has become a versatile basis technology for information extraction with applications in many subfields, such as semantic parsing or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020), by first encoding the original graph and then generating the modified one based on this encoding. In this work, we show that we can considerably increase performance on this problem by phrasing it as graph extension instead of graph generation. We propose the first model for the resulting graph extension problem based on autoregressive sequence labelling. On three scene graph modification data sets, this formulation leads to improvements in accuracy over the state-of-the-art between 13 and 24 percentage points. Furthermore, we introduce a novel data set from the biomedical domain which has much larger linguistic variability and more complex graphs than the scene graph modification data sets. For this data set, the state-of-the art fails to generalize, while our model can produce meaningful predictions.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '2081ac22151c1075fcc6533f0935c29d486bfa6f',\n",
       "  'title': 'A Sentence Cloze Dataset for Chinese Machine Reading Comprehension',\n",
       "  'authorId': '3043830',\n",
       "  'authorName': 'Yiming Cui',\n",
       "  'abstract': 'Owing to the continuous efforts by the Chinese NLP community, more and more Chinese machine reading comprehension datasets become available. To add diversity in this area, in this paper, we propose a new task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC). The proposed task aims to fill the right candidate sentence into the passage that has several blanks. We built a Chinese dataset called CMRC 2019 to evaluate the difficulty of the SC-MRC task. Moreover, to add more difficulties, we also made fake candidates that are similar to the correct ones, which requires the machine to judge their correctness in the context. The proposed dataset contains over 100K blanks (questions) within over 10K passages, which was originated from Chinese narrative stories. To evaluate the dataset, we implement several baseline systems based on the pre-trained models, and the results show that the state-of-the-art model still underperforms human performance by a large margin. We release the dataset and baseline system to further facilitate our community. Resources available through https://github.com/ymcui/cmrc2019',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '5ecf0d0d44489142ce0bb25849be6a72530692b4',\n",
       "  'title': 'Arc-swift: A Novel Transition System for Dependency Parsing',\n",
       "  'authorId': '50531624',\n",
       "  'authorName': 'Peng Qi',\n",
       "  'abstract': 'Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments. Correct individual decisions hence require global information about the sentence context and mistakes cause error propagation. This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a single transition. This allows the parser to leverage lexical information more directly in transition decisions. Hence, arc-swift can achieve significantly better performance with a very small beam size. Our parsers reduce error by 3.7–7.6% relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '8e988bf81b506edc2fd8be71db4831ccf80f3db9',\n",
       "  'title': 'Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models',\n",
       "  'authorId': '2124945496',\n",
       "  'authorName': 'Jiaoda Li',\n",
       "  'abstract': 'Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image is replaced with an unrelated image or noise, which suggests that the visual context might not be exploited by the model at all. We hypothesize that this might be caused by the nature of the commonly used evaluation benchmark, also known as Multi30K, where the translations of image captions were prepared without actually showing the images to human translators. In this paper, we present a qualitative study that examines the role of datasets in stimulating the leverage of visual modality and we propose methods to highlight the importance of visual signals in the datasets which demonstrate improvements in reliance of models on the source images. Our findings suggest the research on effective MMT architectures is currently impaired by the lack of suitable datasets and careful consideration must be taken in creation of future MMT datasets, for which we also provide useful insights.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f',\n",
       "  'title': 'Learning to Discover, Ground and Use Words with Segmental Neural Language Models',\n",
       "  'authorId': '2189948',\n",
       "  'authorName': 'Kazuya Kawakami',\n",
       "  'abstract': 'We propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences. In contrast to previous segmentation models that treat word segmentation as an isolated task, our model unifies word discovery, learning how words fit together to form sentences, and, by conditioning the model on visual context, how words’ meanings ground in representations of nonlinguistic modalities. Experiments show that the unconditional model learns predictive distributions better than character LSTM models, discovers words competitively with nonparametric Bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '94c7772c0ef25b4979103cb0d25d1304fe634e89',\n",
       "  'title': 'Summary-Source Proposition-level Alignment: Task, Datasets and Supervised Baseline',\n",
       "  'authorId': '40701584',\n",
       "  'authorName': 'Ori Ernst',\n",
       "  'abstract': 'Aligning sentences in a reference summary with their counterparts in source documents was shown as a useful auxiliary summarization task, notably for generating training data for salience detection. Despite its assessed utility, the alignment step was mostly approached with heuristic unsupervised methods, typically ROUGE-based, and was never independently optimized or evaluated. In this paper, we propose establishing summary-source alignment as an explicit task, while introducing two major novelties: (1) applying it at the more accurate proposition span level, and (2) approaching it as a supervised classification task. To that end, we created a novel training dataset for proposition-level alignment, derived automatically from available summarization evaluation data. In addition, we crowdsourced dev and test datasets, enabling model development and proper evaluation. Utilizing these data, we present a supervised proposition alignment baseline model, showing improved alignment-quality over the unsupervised approach.',\n",
       "  'year': 2020,\n",
       "  'venue': 'CONLL'},\n",
       " {'paperId': '9911b0e21b5229d3e87ab4c48371cdd5474d7120',\n",
       "  'title': 'UC Davis at SemEval-2019 Task 1: DAG Semantic Parsing with Attention-based Decoder',\n",
       "  'authorId': '150978762',\n",
       "  'authorName': 'Dian Yu',\n",
       "  'abstract': 'We present an encoder-decoder model for semantic parsing with UCCA SemEval 2019 Task 1. The encoder is a Bi-LSTM and the decoder uses recursive self-attention. The proposed model alleviates challenges and feature engineering in traditional transition-based and graph-based parsers. The resulting parser is simple and proved to effective on the semantic parsing task.',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'b6aa634d7066e7f371eace76ee02610ede58ee4c',\n",
       "  'title': 'A Partially Rule-Based Approach to AMR Generation',\n",
       "  'authorId': '69465819',\n",
       "  'authorName': 'Emma Manning',\n",
       "  'abstract': 'This paper presents a new approach to generating English text from Abstract Meaning Representation (AMR). In contrast to the neural and statistical MT approaches used in other AMR generation systems, this one is largely rule-based, supplemented only by a language model and simple statistical linearization models, allowing for more control over the output. We also address the difficulties of automatically evaluating AMR generation systems and the problems with BLEU for this task. We compare automatic metrics to human evaluations and show that while METEOR and TER arguably reflect human judgments better than BLEU, further research into suitable evaluation metrics is needed.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'c3e937bba9cdb4ac6a93f92154843c041c806ba9',\n",
       "  'title': 'Simple Fusion: Return of the Language Model',\n",
       "  'authorId': '48404632',\n",
       "  'authorName': 'Felix Stahlberg',\n",
       "  'abstract': 'Neural Machine Translation (NMT) typically leverages monolingual data in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training: We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the residual probability of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for fluency. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and cold fusion.',\n",
       "  'year': 2018,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': 'ad7bc86f9beea2f06b5c4a82f591daec22b9a5ec',\n",
       "  'title': 'CSECU-DSG at SemEval-2021 Task 5: Leveraging Ensemble of Sequence Tagging Models for Toxic Spans Detection',\n",
       "  'authorId': '2008151027',\n",
       "  'authorName': 'Tashin Hossain',\n",
       "  'abstract': 'The upsurge of prolific blogging and microblogging platforms enabled the abusers to spread negativity and threats greater than ever. Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms. This paper describes our participation in the SemEval 2021 toxic span detection task. The task requires detecting spans that convey toxic remarks from the given text. We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans. Finally, a majority voting ensemble method is used to determine the unified toxic spans. Experimental results depict the competitive performance of our model among the participants.',\n",
       "  'year': 2021,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': 'bb429a17280c2df86ac34789df880a4f728009ae',\n",
       "  'title': 'Probing for Referential Information in Language Models',\n",
       "  'authorId': '1947259',\n",
       "  'authorName': 'Ionut-Teodor Sorodoc',\n",
       "  'abstract': 'Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e6566ece21f6637c515fe9969f9d1ec6cca6d36c',\n",
       "  'title': 'Higher-Order Coreference Resolution with Coarse-to-Fine Inference',\n",
       "  'authorId': '2544107',\n",
       "  'authorName': 'Kenton Lee',\n",
       "  'abstract': 'We introduce a fully-differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '7d767f64e88fdec81a24190c629dcfe23c940793',\n",
       "  'title': 'Natural Language Generation for Effective Knowledge Distillation',\n",
       "  'authorId': '26917433',\n",
       "  'authorName': 'Raphael Tang',\n",
       "  'abstract': 'Knowledge distillation can effectively transfer knowledge from BERT, a deep language representation model, to traditional, shallow word embedding-based neural networks, helping them approach or exceed the quality of other heavyweight language representation models. As shown in previous work, critical to this distillation procedure is the construction of an unlabeled transfer dataset, which enables effective knowledge transfer. To create transfer set examples, we propose to sample from pretrained language models fine-tuned on task-specific text. Unlike previous techniques, this directly captures the purpose of the transfer set. We hypothesize that this principled, general approach outperforms rule-based techniques. On four datasets in sentiment classification, sentence similarity, and linguistic acceptability, we show that our approach improves upon previous methods. We outperform OpenAI GPT, a deep pretrained transformer, on three of the datasets, while using a single-layer bidirectional LSTM that runs at least ten times faster.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'ea7181c387bd2df867b1ebcbcfdc7284b4614afa',\n",
       "  'title': 'Unification-based Multimodal Parsing',\n",
       "  'authorId': '144039163',\n",
       "  'authorName': 'Michael Johnston',\n",
       "  'abstract': 'In order to realize their full potential, multimodal systems need to support not just input from multiple modes, but also synchronized integration of modes. Johnston et al (1997) model this integration using a unification operation over typed feature structures. This is an effective solution for a broad class of systems, but limits multimodal utterances to combinations of a single spoken phrase with a single gesture. We show how the unification-based approach can be scaled up to provide a full multimodal grammar formalism. In conjunction with a multidimensional chart parser, this approach supports integration of multiple elements distributed across the spatial, temporal, and acoustic dimensions of multimodal interaction. Integration strategies are stated in a high level unification-based rule formalism supporting rapid prototyping and iterative development of multimodal systems.',\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '353eb2e1a391ff1077fd875baf027c0f920c208e',\n",
       "  'title': 'Two Knives Cut Better Than One: Chinese Word Segmentation with Dual Decomposition',\n",
       "  'authorId': '2565228',\n",
       "  'authorName': 'Mengqiu Wang',\n",
       "  'abstract': 'There are two dominant approaches to Chinese word segmentation: word-based and character-based models, each with respective strengths. Prior work has shown that gains in segmentation performance can be achieved from combining these two types of models; however, past efforts have not provided a practical technique to allow mainstream adoption. We propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3e6a2a4397719cdf22e5e788640d83476bd67ff3',\n",
       "  'title': 'Improving Text-to-Pictograph Translation Through Word Sense Disambiguation',\n",
       "  'authorId': '3451971',\n",
       "  'authorName': 'L. Sevens',\n",
       "  'abstract': 'We describe the implementation of a Word Sense Disambiguation (WSD) tool in a Dutch Text-to-Pictograph translation system, which converts textual messages into sequences of pictographic images. The system is used in an online platform for Augmentative and Alternative Communication (AAC). In the original translation process, the appropriate sense of a word was not disambiguated before converting it into a pictograph. This often resulted in incorrect translations. The implementation of a WSD tool provides a better semantic understanding of the input messages.',\n",
       "  'year': 2016,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'e4b3dfebbd85f0637f23df11ea65f0dda5ac9981',\n",
       "  'title': 'A Speech-First Model for Repair Detection and Correction',\n",
       "  'authorId': '1844268',\n",
       "  'authorName': 'C. H. Nakatani',\n",
       "  'abstract': 'Interpreting fully natural speech is an important goal for spoken language understanding systems. However, while corpus studies have shown that about 10% of spontaneous utterances contain self-corrections, or REPAIRS, little is known about the extent to which cues in the speech signal may facilitate repair processing. We identify several cues based on acoustic and prosodic analysis of repairs in a corpus of spontaneous speech, and propose methods for exploiting these cues to detect and correct repairs. We test our acoustic-prosodic cues with other lexical cues to repair identification and find that precision rates of 89--93% and recall of 78--83% can be achieved, depending upon the cues employed, from a prosodically labeled corpus.',\n",
       "  'year': 1993,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '623fb92ad6a837e598b2c3c1a8412887df921fb6',\n",
       "  'title': 'UHH-LT at SemEval-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection',\n",
       "  'authorId': '3096279',\n",
       "  'authorName': 'Gregor Wiedemann',\n",
       "  'abstract': 'Fine-tuning of pre-trained transformer networks such as BERT yield state-of-the-art results for text classification tasks. Typically, fine-tuning is performed on task-specific training datasets in a supervised manner. One can also fine-tune in unsupervised manner beforehand by further pre-training the masked language modeling (MLM) task. Hereby, in-domain data for unsupervised MLM resembling the actual classification target dataset allows for domain adaptation of the model. In this paper, we compare current pre-trained transformer networks with and without MLM fine-tuning on their performance for offensive language detection. Our MLM fine-tuned RoBERTa-based classifier officially ranks 1st in the SemEval 2020 Shared Task 12 for the English language. Further experiments with the ALBERT model even surpass this result.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': '8dd7cac75655e22039f9b86799cb88d83e7bae49',\n",
       "  'title': 'Cross-lingual Aspect-based Sentiment Analysis with Aspect Term Code-Switching',\n",
       "  'authorId': '2108145336',\n",
       "  'authorName': 'Wenxuan Zhang',\n",
       "  'abstract': 'Many efforts have been made in solving the Aspect-based sentiment analysis (ABSA) task. While most existing studies focus on English texts, handling ABSA in resource-poor languages remains a challenging problem. In this paper, we consider the unsupervised cross-lingual transfer for the ABSA task, where only labeled data in the source language is available and we aim at transferring its knowledge to the target language having no labeled data. To this end, we propose an alignment-free label projection method to obtain high-quality pseudo-labeled data of the target language with the help of the translation system, which could preserve more accurate task-specific knowledge in the target language. For better utilizing the source and translated data, as well as enhancing the cross-lingual alignment, we design an aspect code-switching mechanism to augment the training data with code-switched bilingual sentences. To further investigate the importance of language-specific knowledge in solving the ABSA problem, we distill the above model on the unlabeled target language data which improves the performance to the same level of the supervised method.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '407961ba37a4cb04abf386ac0dbe8d3d2fbae040',\n",
       "  'title': 'Extracting Multiword Translations from Aligned Comparable Documents',\n",
       "  'authorId': '153584694',\n",
       "  'authorName': 'R. Rapp',\n",
       "  'abstract': 'Most previous attempts to identify translations of multiword expressions using comparable corpora relied on dictionaries of single words. The translation of a multiword was then constructed from the translations of its components. In contrast, in this work we try to determine the translation of a multiword unit by analyzing its contextual behaviour in aligned comparable documents, thereby not presupposing any given dictionary. Whereas with this method translation results for single words are rather good, the results for multiword units are considerably worse. This is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information. Thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually, and to combine the results.',\n",
       "  'year': 2014,\n",
       "  'venue': 'HyTra@EACL'},\n",
       " {'paperId': '18d079a6d72e3f0b0c9214f597b6b178265b05ee',\n",
       "  'title': 'Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies',\n",
       "  'authorId': '1947267',\n",
       "  'authorName': 'Michel Galley',\n",
       "  'abstract': 'We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.',\n",
       "  'year': 2004,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '2823a0ec4c4470c229ff54f34e5e43354e871501',\n",
       "  'title': 'Integrating Discourse Markers into a Pipelined Natural Language Generation Architecture',\n",
       "  'authorId': '1730221',\n",
       "  'authorName': 'Charles B. Callaway',\n",
       "  'abstract': 'Pipelined Natural Language Generation (NLG) systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions, lexical choice, and revision. This has given rise to discussions about the relative placement of these new modules in the overall architecture. Recent work on another aspect of multi-paragraph text, discourse markers, indicates it is time to consider where a discourse marker insertion algorithm fits in. We present examples which suggest that in a pipelined NLG architecture, the best approach is to strongly tie it to a revision component. Finally, we evaluate the approach in a working multi-page system.',\n",
       "  'year': 2003,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '5c4a09242e45ec31e9a608c8bb45387a4b8a6eda',\n",
       "  'title': 'Modelling Protagonist Goals and Desires in First-Person Narrative',\n",
       "  'authorId': '1928044',\n",
       "  'authorName': 'Elahe Rahimtoroghi',\n",
       "  'abstract': 'Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on computational models for this problem. We introduce a new dataset, DesireDB, which includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '7eb1d96fa4c7d3a36613139116e356364ea3edcd',\n",
       "  'title': 'Towards a Unified Approach to Memory- and Statistical-Based Machine Translation',\n",
       "  'authorId': '1695463',\n",
       "  'authorName': 'D. Marcu',\n",
       "  'abstract': 'We present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statistical-based translation model. Our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model. The translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection, while the commercial systems translated perfectly only 40-42% of them.',\n",
       "  'year': 2001,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3fbe7f9a0ca640c915d1b4e2cca6e49a15ad710a',\n",
       "  'title': 'Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction',\n",
       "  'authorId': '2143557418',\n",
       "  'authorName': 'Yubo Ma',\n",
       "  'abstract': 'In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a7291c7495a74396ca1a15fc71a8f8bc0988e159',\n",
       "  'title': 'Joint Decoding of Tree Transduction Models for Sentence Compression',\n",
       "  'authorId': '3303069',\n",
       "  'authorName': 'Jin-ge Yao',\n",
       "  'abstract': 'In this paper, we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores, by jointly decoding two components. In our proposed solution, rich local discriminative features can be easily integrated without increasing computational complexity. Utilizing an unobvious fact that the resulted two components can be independently decoded, we conduct efficient joint decoding based on dual decomposition. Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '76a134e245367d2a1d0fc35801a549d47ec98d0a',\n",
       "  'title': 'Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks',\n",
       "  'authorId': '10669236',\n",
       "  'authorName': 'Gaël Guibon',\n",
       "  'abstract': 'Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect emotions and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such context. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two datasets with different languages: daily conversations in English and customer service chat conversations in French. When applied to emotion classification in conversations, our method proved to be competitive even when compared to other ones.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '86467d64f173a27601957862617721fdd2f8285c',\n",
       "  'title': 'Transition-based Spinal Parsing',\n",
       "  'authorId': '143668305',\n",
       "  'authorName': 'Miguel Ballesteros',\n",
       "  'abstract': 'We present a transition-based arc-eager model to parse spinal trees, a dependencybased representation that includes phrasestructure information in the form of constituent spines assigned to tokens. As a main advantage, the arc-eager model can use a rich set of features combining dependency and constituent information, while parsing in linear time. We describe a set of conditions for the arc-eager system to produce valid spinal structures. In experiments using beam search we show that the model obtains a good trade-off between speed and accuracy, and yields state of the art performance for both dependency and constituent parsing measures.',\n",
       "  'year': 2015,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': '275eda0d215ca3059debf906cdc3b9df306f738e',\n",
       "  'title': 'Global Thread-level Inference for Comment Classification in Community Question Answering',\n",
       "  'authorId': '2708940',\n",
       "  'authorName': 'Shafiq R. Joty',\n",
       "  'abstract': 'Community question answering, a recent evolution of question answering in the Web context, allows a user to quickly consult the opinion of a number of people on a particular topic, thus taking advantage of the wisdom of the crowd. Here we try to help the user by deciding automatically which answers are good and which are bad for a given question. In particular, we focus on exploiting the output structure at the thread level in order to make more consistent global decisions. More specifically, we exploit the relations between pairs of comments at any distance in the thread, which we incorporate in a graph-cut and in an ILP frameworks. We evaluated our approach on the benchmark dataset of SemEval-2015 Task 3. Results improved over the state of the art, confirming the importance of using thread level information.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'c2c79f91c60a93513f152ca7a4a3263075c853d6',\n",
       "  'title': 'Generating Diverse Translations via Weighted Fine-tuning and Hypotheses Filtering for the Duolingo STAPLE Task',\n",
       "  'authorId': '5112699',\n",
       "  'authorName': 'Sweta Agrawal',\n",
       "  'abstract': 'This paper describes the University of Maryland’s submission to the Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Unlike the standard machine translation task, STAPLE requires generating a set of outputs for a given input sequence, aiming to cover the space of translations produced by language learners. We adapt neural machine translation models to this requirement by (a) generating n-best translation hypotheses from a model fine-tuned on learner translations, oversampled to reflect the distribution of learner responses, and (b) filtering hypotheses using a feature-rich binary classifier that directly optimizes a close approximation of the official evaluation metric. Combination of systems that use these two strategies achieves F1 scores of 53.9% and 52.5% on Vietnamese and Portuguese, respectively ranking 2nd and 4th on the leaderboard.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NGT'},\n",
       " {'paperId': '245b03b60cb4bf0235109af4e48f958fbab03b34',\n",
       "  'title': 'Learning Semantic Textual Similarity from Conversations',\n",
       "  'authorId': '2781059',\n",
       "  'authorName': 'Yinfei Yang',\n",
       "  'abstract': 'We present a novel approach to learn representations for sentence-level semantic similarity using conversational data. Our method trains an unsupervised model to predict conversational responses. The resulting sentence embeddings perform well on the Semantic Textual Similarity (STS) Benchmark and SemEval 2017’s Community Question Answering (CQA) question similarity subtask. Performance is further improved by introducing multitask training, combining conversational response prediction and natural language inference. Extensive experiments show the proposed model achieves the best performance among all neural models on the STS Benchmark and is competitive with the state-of-the-art feature engineered and mixed systems for both tasks.',\n",
       "  'year': 2018,\n",
       "  'venue': 'Rep4NLP@ACL'},\n",
       " {'paperId': 'db22ce8f53aa95f8bb00e0319b1d8b66e6895c96',\n",
       "  'title': 'Emoji-Based Transfer Learning for Sentiment Tasks',\n",
       "  'authorId': '2049401206',\n",
       "  'authorName': 'Susann Boy',\n",
       "  'abstract': 'Sentiment tasks such as hate speech detection and sentiment analysis, especially when performed on languages other than English, are often low-resource. In this study, we exploit the emotional information encoded in emojis to enhance the performance on a variety of sentiment tasks. This is done using a transfer learning approach, where the parameters learned by an emoji-based source task are transferred to a sentiment target task. We analyse the efficacy of the transfer under three conditions, i.e. i) the emoji content and ii) label distribution of the target task as well as iii) the difference between monolingually and multilingually learned source tasks. We find i.a. that the transfer is most beneficial if the target task is balanced with high emoji content. Monolingually learned source tasks have the benefit of taking into account the culturally specific use of emojis and gain up to F1 +0.280 over the baseline.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '641250e235d81e5b5c0023c32e71731aa0b0027c',\n",
       "  'title': 'Multi-resolution Annotations for Emoji Prediction',\n",
       "  'authorId': '2227771',\n",
       "  'authorName': 'Weicheng Ma',\n",
       "  'abstract': 'Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary task to many Natural Language Understanding (NLU) tasks. Labels in existing emoji prediction datasets are all passage-based and are usually under the multi-class classification setting. However, in many cases, one single emoji cannot fully cover the theme of a piece of text. It is thus useful to infer the part of text related to each emoji. The lack of multi-label and aspect-level emoji prediction datasets is one of the bottlenecks for this task. This paper annotates an emoji prediction dataset with passage-level multi-class/multi-label, and aspect-level multi-class annotations. We also present a novel annotation method with which we generate the aspect-level annotations. The annotations are generated heuristically, taking advantage of the self-attention mechanism in Transformer networks. We validate the annotations both automatically and manually to ensure their quality. We also benchmark the dataset with a pre-trained BERT model.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'bf386f23f2c826461e61c583d6cb63326e81c17b',\n",
       "  'title': 'Quantum-Inspired Complex Word Embedding',\n",
       "  'authorId': '41219390',\n",
       "  'authorName': 'Qiuchi Li',\n",
       "  'abstract': 'A challenging task for word embeddings is to capture the emergent meaning or polarity of a combination of individual words. For example, existing approaches in word embeddings will assign high probabilities to the words “Penguin” and “Fly” if they frequently co-occur, but it fails to capture the fact that they occur in an opposite sense - Penguins do not fly. We hypothesize that humans do not associate a single polarity or sentiment to each word. The word contributes to the overall polarity of a combination of words depending upon which other words it is combined with. This is analogous to the behavior of microscopic particles which exist in all possible states at the same time and interfere with each other to give rise to new states depending upon their relative phases. We make use of the Hilbert Space representation of such particles in Quantum Mechanics where we subscribe a relative phase to each word, which is a complex number, and investigate two such quantum inspired models to derive the meaning of a combination of words. The proposed models achieve better performances than state-of-the-art non-quantum models on binary sentence classification tasks.',\n",
       "  'year': 2018,\n",
       "  'venue': 'Rep4NLP@ACL'},\n",
       " {'paperId': '8ce47ba9335887c5c92750ffe15e6136329c85c0',\n",
       "  'title': 'Structured Prediction via Learning to Search under Bandit Feedback',\n",
       "  'authorId': '143816740',\n",
       "  'authorName': 'Amr Sharaf',\n",
       "  'abstract': 'We present an algorithm for structured prediction under online bandit feedback. The learner repeatedly predicts a sequence of actions, generating a structured output. It then observes feedback for that output and no others. We consider two cases: a pure bandit setting in which it only observes a loss, and more fine-grained feedback in which it observes a loss for every action. We find that the fine-grained feedback is necessary for strong empirical performance, because it allows for a robust variance-reduction strategy. We empirically compare a number of different algorithms and exploration methods and show the efficacy of BLS on sequence labeling and dependency parsing tasks.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SPNLP@EMNLP'},\n",
       " {'paperId': 'a2aa642db090b3aa28a44ccbc3c51fdb0be8335b',\n",
       "  'title': 'Cross-Domain Generalization of Neural Constituency Parsers',\n",
       "  'authorId': '47070750',\n",
       "  'authorName': 'Daniel Fried',\n",
       "  'abstract': 'Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing—but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '8a902a848c3710290f04f2d59030f5670d3433f8',\n",
       "  'title': 'Error Analysis and the Role of Morphology',\n",
       "  'authorId': '34887843',\n",
       "  'authorName': 'Marcel Bollmann',\n",
       "  'abstract': 'We evaluate two common conjectures in error analysis of NLP models: (i) Morphology is predictive of errors; and (ii) the importance of morphology increases with the morphological complexity of a language. We show across four different tasks and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true. Using morphological features does improve error prediction across tasks; however, this effect is less pronounced with morphologically complex languages. We speculate this is because morphology is more discriminative in morphologically simple languages. Across all four tasks, case and gender are the morphological features most predictive of error.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '3e95925d2bca43223453010ff8516a492287ce19',\n",
       "  'title': 'Global-Locally Self-Attentive Encoder for Dialogue State Tracking',\n",
       "  'authorId': '3428769',\n",
       "  'authorName': 'Victor Zhong',\n",
       "  'abstract': 'Dialogue state tracking, which estimates user goals and requests given the dialogue context, is an essential part of task-oriented dialogue systems. In this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. Our model uses global modules to shares parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. We show that this significantly improves tracking of rare states. GLAD obtains 88.3% joint goal accuracy and 96.4% request accuracy on the WoZ state tracking task, outperforming prior work by 3.9% and 4.8%. On the DSTC2 task, our model obtains 74.7% joint goal accuracy and 97.3% request accuracy, outperforming prior work by 1.3% and 0.8%',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '0f67b720a20dfa0608e4f6907925367f592cf1f9',\n",
       "  'title': 'KeLP at SemEval-2017 Task 3: Learning Pairwise Patterns in Community Question Answering',\n",
       "  'authorId': '2778426',\n",
       "  'authorName': 'Simone Filice',\n",
       "  'abstract': 'This paper describes the KeLP system participating in the SemEval-2017 community Question Answering (cQA) task. The system is a refinement of the kernel-based sentence pair modeling we proposed for the previous year challenge. It is implemented within the Kernel-based Learning Platform called KeLP, from which we inherit the team’s name. Our primary submission ranked first in subtask A, and third in subtasks B and C, being the only systems appearing in the top-3 ranking for all the English subtasks. This shows that the proposed framework, which has minor variations among the three subtasks, is extremely flexible and effective in tackling learning tasks defined on sentence pairs.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': '919145f7019f4669d6a86c4e2b719545212e55ff',\n",
       "  'title': 'Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons',\n",
       "  'authorId': '145485960',\n",
       "  'authorName': 'Jie Hao',\n",
       "  'abstract': 'Recent studies have shown that a hybrid of self-attention networks (SANs) and recurrent neural networks RNNs outperforms both individual architectures, while not much is known about why the hybrid models work. With the belief that modeling hierarchical structure is an essential complementary between SANs and RNNs, we propose to further enhance the strength of hybrid models with an advanced variant of RNNs – Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented inductive bias to perform tree-like composition. Experimental results on the benchmark machine translation task show that the proposed approach outperforms both individual architectures and a standard hybrid model. Further analyses on targeted linguistic evaluation and logical inference tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5b60e5216b66cfc7f9e0c41de154915efe6e5ef3',\n",
       "  'title': 'Semi-supervised Stochastic Multi-Domain Learning using Variational Inference',\n",
       "  'authorId': '40609859',\n",
       "  'authorName': 'Yitong Li',\n",
       "  'abstract': 'Supervised models of NLP rely on large collections of text which closely resemble the intended testing setting. Unfortunately matching text is often not available in sufficient quantity, and moreover, within any domain of text, data is often highly heterogenous. In this paper we propose a method to distill the important domain signal as part of a multi-domain learning system, using a latent variable model in which parts of a neural model are stochastically gated based on the inferred domain. We compare the use of discrete versus continuous latent variables, operating in a domain-supervised or a domain semi-supervised setting, where the domain is known only for a subset of training inputs. We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'cf7acb07d5d7bda08a67e10d09ec2ed1fb3c58cf',\n",
       "  'title': 'A Bayesian Model of Diachronic Meaning Change',\n",
       "  'authorId': '2875615',\n",
       "  'authorName': 'Lea Frermann',\n",
       "  'abstract': 'Word meanings change over time and an automated procedure for extracting this information from text would be useful for historical exploratory studies, information retrieval or question answering. We present a dynamic Bayesian model of diachronic meaning change, which infers temporal word representations as a set of senses and their prevalence. Unlike previous work, we explicitly model language change as a smooth, gradual process. We experimentally show that this modeling decision is beneficial: our model performs competitively on meaning change detection tasks whilst inducing discernible word senses and their development over time. Application of our model to the SemEval-2015 temporal classification benchmark datasets further reveals that it performs on par with highly optimized task-specific systems.',\n",
       "  'year': 2016,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '81fd62d954755d35fb55ca0ac1e8552948fa3503',\n",
       "  'title': 'SSN_MLRG1 at SemEval-2017 Task 4: Sentiment Analysis in Twitter Using Multi-Kernel Gaussian Process Classifier',\n",
       "  'authorId': '8007858',\n",
       "  'authorName': 'S. AngelDeborah',\n",
       "  'abstract': 'The SSN MLRG1 team for Semeval-2017 task 4 has applied Gaussian Process, with bag of words feature vectors and fixed rule multi-kernel learning, for sentiment analysis of tweets. Since tweets on the same topic, made at different times, may exhibit different emotions, their properties such as smoothness and periodicity also vary with time. Our experiments show that, compared to single kernel, multiple kernels are effective in learning the simultaneous presence of multiple properties.',\n",
       "  'year': 2017,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'a2b28a11e0697acd5b1cd4c279cd4d37853cade8',\n",
       "  'title': 'The AFRL-Ohio State WMT18 Multimodal System: Combining Visual with Traditional',\n",
       "  'authorId': '3456371',\n",
       "  'authorName': 'Jeremy Gwinnup',\n",
       "  'abstract': 'AFRL-Ohio State extends its usage of visual domain-driven machine translation for use as a peer with traditional machine translation systems. As a peer, it is enveloped into a system combination of neural and statistical MT systems to present a composite translation.',\n",
       "  'year': 2018,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '2d315208dd88eed245dc4ae1ae398af75aa291f8',\n",
       "  'title': 'Curriculum Learning for Natural Language Understanding',\n",
       "  'authorId': '1754285124',\n",
       "  'authorName': 'Benfeng Xu',\n",
       "  'abstract': 'With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'cc70ec5aa10ff158f7e9950ec38d65d6eb4a8ff3',\n",
       "  'title': 'Hell Hath No Fury? Correcting Bias in the NRC Emotion Lexicon',\n",
       "  'authorId': '1805988818',\n",
       "  'authorName': 'Samira Zad',\n",
       "  'abstract': 'There have been several attempts to create an accurate and thorough emotion lexicon in English, which identifies the emotional content of words. Of the several commonly used resources, the NRC emotion lexicon (Mohammad and Turney, 2013b) has received the most attention due to its availability, size, and its choice of Plutchik’s expressive 8-class emotion model. In this paper we identify a large number of troubling entries in the NRC lexicon, where words that should in most contexts be emotionally neutral, with no affect (e.g., ‘lesbian’, ‘stone’, ‘mountain’), are associated with emotional labels that are inaccurate, nonsensical, pejorative, or, at best, highly contingent and context-dependent (e.g., ‘lesbian’ labeled as Disgust and Sadness, ‘stone’ as Anger, or ‘mountain’ as Anticipation). We describe a procedure for semi-automatically correcting these problems in the NRC, which includes disambiguating POS categories and aligning NRC entries with other emotion lexicons to infer the accuracy of labels. We demonstrate via an experimental benchmark that the quality of the resources is thus improved. We release the revised resource and our code to enable other researchers to reproduce and build upon results.',\n",
       "  'year': 2021,\n",
       "  'venue': 'WOAH'},\n",
       " {'paperId': '2ea693e0aab29c658366a2f27770eecd1e5d367f',\n",
       "  'title': 'Classification and Analysis of Neologisms Produced by Learners of Spanish: Effects of Proficiency and Task',\n",
       "  'authorId': '1387852377',\n",
       "  'authorName': 'Shira Wein',\n",
       "  'abstract': 'The Spanish Learner Language Oral Corpora (SPLLOC) of transcribed conversations between investigators and language learners contains a set of neologism tags. In this work, the utterances tagged as neologisms are broken down into three categories: true neologisms, loanwords, and errors. This work examines the relationships between neologism, loanword, and error production and both language learner level and conversation task. The results of this study suggest that loanwords and errors are produced most frequently by language learners with moderate experience, while neologisms are produced most frequently by native speakers. This study also indicates that tasks that require descriptions of images draw more neologism, loanword and error production. We ultimately present a unique analysis of the implications of neologism, loanword, and error production useful for further work in second language acquisition research, as well as for language educators.',\n",
       "  'year': 2020,\n",
       "  'venue': 'WINLP'},\n",
       " {'paperId': 'efb4b0458e4a0c44d298632f2fa67e365e602e15',\n",
       "  'title': 'Synonymous Collocation Extraction Using Translation Information',\n",
       "  'authorId': '40354707',\n",
       "  'authorName': 'Hua Wu',\n",
       "  'abstract': 'Automatically acquiring synonymous collocation pairs such as and from corpora is a challenging task. For this task, we can, in general, have a large monolingual corpus and/or a very limited bilingual corpus. Methods that use monolingual corpora alone or use bilingual corpora alone are apparently inadequate because of low precision or low coverage. In this paper, we propose a method that uses both these resources to get an optimal compromise of precision and coverage. This method first gets candidates of synonymous collocation pairs based on a monolingual corpus and a word thesaurus, and then selects the appropriate pairs from the candidates using their translations in a second language. The translations of the candidates are obtained with a statistical translation model which is trained with a small bilingual corpus and a large monolingual corpus. The translation information is proved as effective to select synonymous collocation pairs. Experimental results indicate that the average precision and recall of our approach are 74% and 64% respectively, which outperform those methods that only use monolingual corpora and those that only use bilingual corpora.',\n",
       "  'year': 2003,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e5529a223176cbb19026a0ee7ae25521d9c39a88',\n",
       "  'title': 'Duality of Link Prediction and Entailment Graph Induction',\n",
       "  'authorId': '81007822',\n",
       "  'authorName': 'Mohammad Javad Hosseini',\n",
       "  'abstract': 'Link prediction and entailment graph induction are often treated as different problems. In this paper, we show that these two problems are actually complementary. We train a link prediction model on a knowledge graph of assertions extracted from raw text. We propose an entailment score that exploits the new facts discovered by the link prediction model, and then form entailment graphs between relations. We further use the learned entailments to predict improved link prediction scores. Our results show that the two tasks can benefit from each other. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '92a4016debc76f2a7a372a2606028fad650c31dc',\n",
       "  'title': 'Toward a deep dialectological representation of Indo-Aryan',\n",
       "  'authorId': '145961675',\n",
       "  'authorName': 'C. Cathcart',\n",
       "  'abstract': 'This paper presents a new approach to disentangling inter-dialectal and intra-dialectal relationships within one such group, the Indo-Aryan subgroup of Indo-European. We draw upon admixture models and deep generative models to tease apart historic language contact and language-specific behavior in the overall patterns of sound change displayed by Indo-Aryan languages. We show that a “deep” model of Indo-Aryan dialectology sheds some light on questions regarding inter-relationships among the Indo-Aryan languages, and performs better than a “shallow” model in terms of certain qualities of the posterior distribution (e.g., entropy of posterior distributions), and outline future pathways for model development.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Sixth Workshop on'},\n",
       " {'paperId': 'f283cd5dab36ac24f23298871824fe18b11019f4',\n",
       "  'title': 'Is it possible to recover personal health information from an automatically de-identified corpus of French EHRs?',\n",
       "  'authorId': '2105490',\n",
       "  'authorName': 'Cyril Grouin',\n",
       "  'abstract': 'De-identification aims at preserving patient confidentiality while enabling the use of clinical documents for furthering medical research. Herein, we aim to evaluate whether patient re-identification is possible on a corpus of de-identified clinical documents in French. Personal Health Identifiers are automatically marked by a de-identification system applied to the corpus, followed by reintroduction of plausible surrogates. The resulting documents are shown to individuals with varying knowledge of the documents and de-identification method. The individuals are asked to re-identify the patients. The amount of information recovered increases with familiarity with the documents and/or de-identification method. Surrogate re-introduction with localization from the same (vs. different) geographical area as the original documents is found more effective. The amount of information recovered was not sufficient to re-identify any of the patients, except when privileged access to the hospital health information system and several documents about the same patient were available.',\n",
       "  'year': 2015,\n",
       "  'venue': 'Louhi@EMNLP'},\n",
       " {'paperId': '5f4cd0153980292097629cc6e1f6766dc4ef3347',\n",
       "  'title': 'Analyzing and Integrating Dependency Parsers',\n",
       "  'authorId': '143957226',\n",
       "  'authorName': 'Ryan T. McDonald',\n",
       "  'abstract': 'There has been a rapid increase in the volume of research on data-driven dependency parsers in the past five years. This increase has been driven by the availability of treebanks in a wide variety of languages—due in large part to the CoNLL shared tasks—as well as the straightforward mechanisms by which dependency theories of syntax can encode complex phenomena in free word order languages. In this article, our aim is to take a step back and analyze the progress that has been made through an analysis of the two predominant paradigms for data-driven dependency parsing, which are often called graph-based and transition-based dependency parsing. Our analysis covers both theoretical and empirical aspects and sheds light on the kinds of errors each type of parser makes and how they relate to theoretical expectations. Using these observations, we present an integrated system based on a stacking learning framework and show that such a system can learn to overcome the shortcomings of each non-integrated system.',\n",
       "  'year': 2011,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': '365417a6ef1b352fef730f4383b795d3d5c73632',\n",
       "  'title': 'A Joint Learning Approach based on Self-Distillation for Keyphrase Extraction from Scientific Documents',\n",
       "  'authorId': '145242558',\n",
       "  'authorName': 'T. Lai',\n",
       "  'abstract': 'Keyphrase extraction is the task of extracting a small set of phrases that best describe a document. Most existing benchmark datasets for the task typically have limited numbers of annotated documents, making it challenging to train increasingly complex neural networks. In contrast, digital libraries store millions of scientific articles online, covering a wide range of topics. While a significant portion of these articles contain keyphrases provided by their authors, most other articles lack such kind of annotations. Therefore, to effectively utilize these large amounts of unlabeled articles, we propose a simple and efficient joint learning approach based on the idea of self-distillation. Experimental results show that our approach consistently improves the performance of baseline models for keyphrase extraction. Furthermore, our best models outperform previous methods for the task, achieving new state-of-the-art results on two public benchmarks: Inspec and SemEval-2017.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '66932482591c6b608ebc98359fdf7c54a595d539',\n",
       "  'title': 'Predicting the Difficulty of Language Proficiency Tests',\n",
       "  'authorId': '2752573',\n",
       "  'authorName': 'Lisa Beinborn',\n",
       "  'abstract': 'Language proficiency tests are used to evaluate and compare the progress of language learners. We present an approach for automatic difficulty prediction of C-tests that performs on par with human experts. On the basis of detailed analysis of newly collected data, we develop a model for C-test difficulty introducing four dimensions: solution difficulty, candidate ambiguity, inter-gap dependency, and paragraph difficulty. We show that cues from all four dimensions contribute to C-test difficulty.',\n",
       "  'year': 2014,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '7aced008122aa344ee2b889e69f66945b92fae73',\n",
       "  'title': 'FORGe at SemEval-2017 Task 9: Deep sentence generation based on a sequence of graph transducers',\n",
       "  'authorId': '2738095',\n",
       "  'authorName': 'Simon Mille',\n",
       "  'abstract': \"Comunicacio presentada a: The 11th International Workshop on Semantic Evaluations (SemEval-2017), celebrada del 3 al 4 d'agost de 2017 a Vancouver, Canada.\",\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': '2cc765e602458960c3fefadafd75456d6f85f958',\n",
       "  'title': 'Neural Machine Translation with Extended Context',\n",
       "  'authorId': '143675545',\n",
       "  'authorName': 'J. Tiedemann',\n",
       "  'abstract': 'We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.',\n",
       "  'year': 2017,\n",
       "  'venue': 'DiscoMT@EMNLP'},\n",
       " {'paperId': '9521ca90922348d5e5f110b8968c4ed66542872e',\n",
       "  'title': 'PathQG: Neural Question Generation from Facts',\n",
       "  'authorId': '2116420560',\n",
       "  'authorName': 'Siyuan Wang',\n",
       "  'abstract': 'Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way. We present a novel task of question generation given a query path in the knowledge graph constructed from the input text. We divide the task into two steps, namely, query representation learning and query-based question generation. We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation. We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework. We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex. Human evaluation also proves that our model is able to generate relevant and informative questions.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5c686171f1d6f841d4a06d2e602630bec5d3d19f',\n",
       "  'title': 'Learning Cross-lingual Representations with Matrix Factorization',\n",
       "  'authorId': '3466086',\n",
       "  'authorName': 'Hanan Aldarmaki',\n",
       "  'abstract': 'We present a matrix factorization model for learning cross-lingual representations. Using sentence-aligned corpora, the proposed model learns distributed representations by factoring the given data into language-dependent factors and one shared factor. Moreover, the model can quickly learn shared representations for more than two languages without undermining the quality of the monolingual components. The model achieves an accuracy of 88% on English to German cross-lingual document classification, and 0.8 Pearson correlation on Spanish-English cross-lingual semantic textual similarity. While the results do not beat state-of-the-art performance in these tasks, we show that the crosslingual models are at least as good as their monolingual counterparts.',\n",
       "  'year': 2016,\n",
       "  'venue': ''},\n",
       " {'paperId': 'e9ea28cca2154ce0fee075ed63b109be7a871f25',\n",
       "  'title': 'AttentiveChecker: A Bi-Directional Attention Flow Mechanism for Fact Verification',\n",
       "  'authorId': '134765210',\n",
       "  'authorName': 'Santosh T.Y.S.S',\n",
       "  'abstract': 'The recently released FEVER dataset provided benchmark results on a fact-checking task in which given a factual claim, the system must extract textual evidence (sets of sentences from Wikipedia pages) that support or refute the claim. In this paper, we present a completely task-agnostic pipelined system, AttentiveChecker, consisting of three homogeneous Bi-Directional Attention Flow (BIDAF) networks, which are multi-layer hierarchical networks that represent the context at different levels of granularity. We are the first to apply to this task a bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. AttentiveChecker can be used to perform document retrieval, sentence selection, and claim verification. Experiments on the FEVER dataset indicate that AttentiveChecker is able to achieve the state-of-the-art results on the FEVER test set.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '0f192e9c7a1e3fdc6e051fc502f74b04c53bb3a3',\n",
       "  'title': 'On the Language Coverage Bias for Neural Machine Translation',\n",
       "  'authorId': '12782489',\n",
       "  'authorName': 'Shuo Wang',\n",
       "  'abstract': 'Language coverage bias, which indicates the content-dependent differences between sentence pairs originating from the source and target languages, is important for neural machine translation (NMT) because the target-original training data is not well exploited in current practice. By carefully designing experiments, we provide comprehensive analyses of the language coverage bias in the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the sourceand target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both backand forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019).',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '79140081f9bdb267952a1af5b733bf6b72101f6f',\n",
       "  'title': 'Information Density and Quality Estimation Features as Translationese Indicators for Human Translation Classification',\n",
       "  'authorId': '1731383',\n",
       "  'authorName': 'Raphaël Rubino',\n",
       "  'abstract': 'This paper introduces information density and machine translation quality estimation inspired features to automatically detect and classify human translated texts. We investigate two settings: discriminating between translations and comparable originally authored texts, and distinguishing two levels of translation professionalism. Our framework is based on delexicalised sentence-level dense feature vector representations combined with a supervised machine learning approach. The results show state-of-the-art performance for mixed-domain translationese detection with information density and quality estimation based features, while results on translation expertise classification are mixed.',\n",
       "  'year': 2016,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '4bc9d6596069c9277b57a7ee1e1127d231f28663',\n",
       "  'title': 'Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders',\n",
       "  'authorId': '32573794',\n",
       "  'authorName': 'Andrew Drozdov',\n",
       "  'abstract': 'The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '0119a57cf88ef16e6dc291252fae340bb6b3953c',\n",
       "  'title': 'CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning',\n",
       "  'authorId': '51583409',\n",
       "  'authorName': 'Bill Yuchen Lin',\n",
       "  'abstract': 'Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., dog, frisbee, catch, throw); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., “a man throws a frisbee and his dog catches it”). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 77k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance (31.6% v.s. 63.5% in SPICE metric). Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA (76.9% to 78.4 in dev accuracy) by generating additional context.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '8054daf190ed8189aa42b0ebf4ae494a452b247a',\n",
       "  'title': 'Neural Machine Translation Techniques for Named Entity Transliteration',\n",
       "  'authorId': '3272639',\n",
       "  'authorName': 'Roman Grundkiewicz',\n",
       "  'abstract': 'Transliterating named entities from one language into another can be approached as neural machine translation (NMT) problem, for which we use deep attentional RNN encoder-decoder models. To build a strong transliteration system, we apply well-established techniques from NMT, such as dropout regularization, model ensembling, rescoring with right-to-left models, and back-translation. Our submission to the NEWS 2018 Shared Task on Named Entity Transliteration ranked first in several tracks.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NEWS@ACL'},\n",
       " {'paperId': '66c84abd01fdd84d9cd241dea8e487580f4f8922',\n",
       "  'title': 'Bridging Languages through Images with Deep Partial Canonical Correlation Analysis',\n",
       "  'authorId': '117095364',\n",
       "  'authorName': 'Guy Rotman',\n",
       "  'abstract': 'We present a deep neural network that leverages images to improve bilingual text embeddings. Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages, producing highly correlated bilingual embeddings. In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA). While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop a new stochastic iterative algorithm for its optimization. We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval. Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'b635009a98e89485b501ee2e873e11f52ecb40ea',\n",
       "  'title': 'Resolving Referring Expressions in Conversational Dialogs for Natural User Interfaces',\n",
       "  'authorId': '1709797',\n",
       "  'authorName': 'Asli Celikyilmaz',\n",
       "  'abstract': 'Unlike traditional over-the-phone spoken dialog systems (SDSs), modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system’s response to the user. Visual display of the system’s response not only changes human behavior when interacting with devices, but also creates new research areas in SDSs. Onscreen item identification and resolution in utterances is one critical problem to achieve a natural and accurate humanmachine communication. We pose the problem as a classification task to correctly identify intended on-screen item(s) from user utterances. Using syntactic, semantic as well as context features from the display screen, our model can resolve different types of referring expressions with up to 90% accuracy. In the experiments we also show that the proposed model is robust to domain and screen layout changes.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '9ccc9f903fde9e16b60a2e2d21acf76c696d266f',\n",
       "  'title': 'Exploiting Lexical Expansions and Boolean Compositions for Web Querying',\n",
       "  'authorId': '1712352',\n",
       "  'authorName': 'B. Magnini',\n",
       "  'abstract': 'This paper describes an experiment aiming at evaluating the role of NLP based optimizations (i.e. morphological derivation and synonymy expansion) in web search strategies. Keywords and their expansions are composed in two different Boolean expressions (i.e. expansion insertion and Cartesian combination) and then compared with a keyword conjunctive composition, considered as the baseline. Results confirm the hypothesis that linguistic optimizations significantly improve the search engine performances.',\n",
       "  'year': 2000,\n",
       "  'venue': ''},\n",
       " {'paperId': 'dd3d357b3edd50781b928044c835e69e1346c5dd',\n",
       "  'title': 'A Discriminative Topic Model using Document Network Structure',\n",
       "  'authorId': '47718618',\n",
       "  'authorName': 'Weiwei Yang',\n",
       "  'abstract': 'Document collections often have links between documents—citations, hyperlinks, or revisions—and which links are added is often based on topical similarity. To model these intuitions, we introduce a new topic model for documents situated within a network structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features. Experiments on a scientific paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '5f762f6893aa883bae91e3338826101e818fdf8d',\n",
       "  'title': 'Translation Rules with Right-Hand Side Lattices',\n",
       "  'authorId': '2120016',\n",
       "  'authorName': 'Fabien Cromierès',\n",
       "  'abstract': 'In Corpus-Based Machine Translation, the search space of the translation candidates for a given input sentence is often defined by a set of (cyclefree) context-free grammar rules. This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation (where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence). But it is also possible to describe Phrase-Based Machine Translation in this framework. We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules. We also demonstrate how the representation of the search space has an impact on decoding efficiency, and how it is possible to optimize this representation.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'c56aced0f0c5cfebefadb530cb08d736c3ac5c05',\n",
       "  'title': 'Retrieval Augmented Code Generation and Summarization',\n",
       "  'authorId': '3405393',\n",
       "  'authorName': 'Md. Rizwan Parvez',\n",
       "  'abstract': 'Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers’ code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5a1156664cbbc4382be28dc312673a6488dda468',\n",
       "  'title': 'Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning',\n",
       "  'authorId': '2113916510',\n",
       "  'authorName': 'Chen Shi',\n",
       "  'abstract': 'The lack of labeled data is one of the main challenges when building a task-oriented dialogue system. Existing dialogue datasets usually rely on human labeling, which is expensive, limited in size, and in low coverage. In this paper, we instead propose our framework auto-dialabel to automatically cluster the dialogue intents and slots. In this framework, we collect a set of context features, leverage an autoencoder for feature assembly, and adapt a dynamic hierarchical clustering method for intent and slot labeling. Experimental results show that our framework can promote human labeling cost to a great extent, achieve good intent clustering accuracy (84.1%), and provide reasonable and instructive slot labeling results.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b0693c9fddd05c211e2e3222a46ea949ffab0bf0',\n",
       "  'title': 'Lean Question Answering over Freebase from Scratch',\n",
       "  'authorId': '33219438',\n",
       "  'authorName': 'Xuchen Yao',\n",
       "  'abstract': 'For the task of question answering (QA) over Freebase on the WEBQUESTIONS dataset (Berant et al., 2013), we found that 85% of all questions (in the training set) can be directly answered via a single binary relation. Thus we turned this task into slot-filling for tuples: predicting relations to get answers given a question’s topic. We design efficient data structures to identify question topics organically from 46 million Freebase topic names, without employing any NLP processing tools. Then we present a lean QA system that runs in real time (in offline batch testing it answered two thousand questions in 51 seconds on a laptop). The system also achieved 7.8% better F1 score (harmonic mean of average precision and recall) than the previous state of the art.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '9af39292b9407888b98df2c697d92be229be9a07',\n",
       "  'title': 'Translating Phrases in Neural Machine Translation',\n",
       "  'authorId': '48631170',\n",
       "  'authorName': 'Xing Wang',\n",
       "  'abstract': 'Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '2b3f6fab90f9592662d2dc21cb650226b3ca3166',\n",
       "  'title': 'Doubly-Attentive Decoder for Multi-modal Neural Machine Translation',\n",
       "  'authorId': '3068677',\n",
       "  'authorName': 'Iacer Calixto',\n",
       "  'abstract': 'We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '35112c6bfa715b2bc86fc5b6c331f8a1a0b07add',\n",
       "  'title': 'Understanding and Predicting Empathic Behavior in Counseling Therapy',\n",
       "  'authorId': '1396239754',\n",
       "  'authorName': 'Verónica Pérez-Rosas',\n",
       "  'abstract': 'Counselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants’ engagement, participants’ verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a729503f528d5f0be0f897aa1841e1ff8ffcb313',\n",
       "  'title': 'Dense Hierarchical Retrieval for Open-Domain Question Answering',\n",
       "  'authorId': '2108335211',\n",
       "  'authorName': 'Ye Liu',\n",
       "  'abstract': 'Dense neural text retrieval has achieved promising results on open-domain Question Answering (QA), where latent representations of questions and passages are exploited for maximum inner product search in the retrieval process. However, current dense retrievers require splitting documents into short passages that usually contain local, partial and sometimes biased context, and highly depend on the splitting process. As a consequence, it may yield inaccurate and misleading hidden representations, thus deteriorating the final retrieval result. In this work, we propose Dense Hierarchical Retrieval (DHR), a hierarchical framework which can generate accurate dense representations of passages by utilizing both macroscopic semantics in the document and microscopic semantics specific to each passage. Specifically, a document-level retriever first identifies relevant documents, among which relevant passages are then retrieved by a passage-level retriever. The ranking of the retrieved passages will be further calibrated by examining the document-level relevance. In addition, hierarchical title structure and two negative sampling strategies (i.e., InDoc and In-Sec negatives) are investigated. We apply DHR to large-scale open-domain QA datasets. DHR significantly outperforms the original dense passage retriever, and helps an end-to-end QA system outperform the strong baselines on multiple open-domain QA benchmarks.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '06987b14a5c3645fb57cb70201f7940ecd674432',\n",
       "  'title': 'Leveraging Entity Linking and Related Language Projection to Improve Name Transliteration',\n",
       "  'authorId': '145668277',\n",
       "  'authorName': 'Ying Lin',\n",
       "  'abstract': 'Traditional name transliteration methods largely ignore source context information and inter-dependency among entities for entity disambiguation. We propose a novel approach to leverage state-of-the-art Entity Linking (EL) techniques to automatically correct name transliteration results, using collective inference from source contexts and additional evidence from knowledge base. Experiments on transliterating names from seven languages to English demonstrate that our approach achieves 2.6% to 15.7% absolute gain over the baseline model, and significantly advances state-of-the-art. When contextual information exists, our approach can achieve further gains (24.2%) by collectively transliterating and disambiguating multiple related entities. We also prove that combining Entity Linking and projecting resources from related languages obtained comparable performance as themethod using the same amount of training pairs in the original languageswithout Entity Linking.1',\n",
       "  'year': 2016,\n",
       "  'venue': 'NEWS@ACM'},\n",
       " {'paperId': '1b856b7dd486d0db7565031720db4e051420ec3b',\n",
       "  'title': 'Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation',\n",
       "  'authorId': '122309052',\n",
       "  'authorName': 'Haipeng Sun',\n",
       "  'abstract': 'Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a6ad47f7b4dc6c5f8791fa9f4db1118586eff891',\n",
       "  'title': 'Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach',\n",
       "  'authorId': '1720974',\n",
       "  'authorName': 'Pratik Jawanpuria',\n",
       "  'abstract': 'We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks. We next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting.',\n",
       "  'year': 2018,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '039ce73659332c12168de439e3f79e7039b636af',\n",
       "  'title': 'RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System',\n",
       "  'authorId': '4428136',\n",
       "  'authorName': 'Haoyang Wen',\n",
       "  'abstract': 'We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '891939c2ca684bf72b8edf8b741ca2d7956bd0f9',\n",
       "  'title': 'Second-order Co-occurrence Sensitivity of Skip-Gram with Negative Sampling',\n",
       "  'authorId': '3449121',\n",
       "  'authorName': 'Dominik Schlechtweg',\n",
       "  'abstract': 'We simulate first- and second-order context overlap and show that Skip-Gram with Negative Sampling is similar to Singular Value Decomposition in capturing second-order co-occurrence information, while Pointwise Mutual Information is agnostic to it. We support the results with an empirical study finding that the models react differently when provided with additional second-order information. Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks.',\n",
       "  'year': 2019,\n",
       "  'venue': 'BlackboxNLP@ACL'},\n",
       " {'paperId': '2f0aa51f0062061c7db1accdf9dd2be5bccfdd26',\n",
       "  'title': 'Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders',\n",
       "  'authorId': '145859353',\n",
       "  'authorName': 'Chen Xu',\n",
       "  'abstract': 'Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '681ad9677c76b40a5e16ca317a21ada12766d4ea',\n",
       "  'title': 'CICBUAPnlp at SemEval-2016 Task 4-A: Discovering Twitter Polarity using Enhanced Embeddings',\n",
       "  'authorId': '1403763319',\n",
       "  'authorName': 'Helena Gómez-Adorno',\n",
       "  'abstract': 'This paper presents our approach for SemEval 2016 task 4: Sentiment Analysis in Twitter. We participated in Subtask A: Message Polarity Classification. The aim is to classify Twitter messages into positive, neutral, and negative polarity. We used a lexical resource for pre-processing of social media data and train a neural network model for feature representation. Our resource includes dictionaries of slang words, contractions, abbreviations, and emoticons commonly used in social media. For the classification process, we pass the features obtained in an unsupervised manner into an SVM classifier.',\n",
       "  'year': 2016,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '7491052f53adac54f6dc87c74994679791b03704',\n",
       "  'title': 'Generating Referring Expressions: Making Referents Easy to Identify',\n",
       "  'authorId': '3249499',\n",
       "  'authorName': 'Ivandré Paraboni',\n",
       "  'abstract': 'It is often desirable that referring expressions be chosen in such a way that their referents are easy to identify. This article focuses on referring expressions in hierarchically structured domains, exploring the hypothesis that referring expressions can be improved by including logically redundant information in them if this leads to a significant reduction in the amount of search that is needed to identify the referent. Generation algorithms are presented that implement this idea by including logically redundant information into the generated expression, in certain well-circumscribed situations. To test our hypotheses, and to assess the performance of our algorithms, two controlled experiments with human subjects were conducted. The first experiment confirms that human judges have a preference for logically redundant expressions in the cases where our model predicts this to be the case. The second experiment suggests that readers benefit from the kind of logical redundancy that our algorithms produce, as measured in terms of the effort needed to identify the referent of the expression.',\n",
       "  'year': 2007,\n",
       "  'venue': 'Computational Linguistics'},\n",
       " {'paperId': 'fbdb1a7962572e428cf5b06582db85cbb6a3f492',\n",
       "  'title': 'STARC: Structured Annotations for Reading Comprehension',\n",
       "  'authorId': '1908728',\n",
       "  'authorName': 'Yevgeni Berzak',\n",
       "  'abstract': 'We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions. Our framework introduces a principled structure for the answer choices and ties them to textual span annotations. The framework is implemented in OneStopQA, a new high-quality dataset for evaluation and analysis of reading comprehension in English. We use this dataset to demonstrate that STARC can be leveraged for a key new application for the development of SAT-like reading comprehension materials: automatic annotation quality probing via span ablation experiments. We further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior, including error distributions and guessing ability. Our experiments also reveal that the standard multiple choice dataset in NLP, RACE, is limited in its ability to measure reading comprehension. 47% of its questions can be guessed by machines without accessing the passage, and 18% are unanimously judged by humans as not having a unique correct answer. OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '218cdcf97898617754abc249b186269b9eb2ea1e',\n",
       "  'title': 'Synthetic Data Made to Order: The Case of Parsing',\n",
       "  'authorId': '2099597',\n",
       "  'authorName': 'D. Wang',\n",
       "  'abstract': 'To approximately parse an unfamiliar language, it helps to have a treebank of a similar language. But what if the closest available treebank still has the wrong word order? We show how to (stochastically) permute the constituents of an existing dependency treebank so that its surface part-of-speech statistics approximately match those of the target language. The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum). This optimization procedure yields trees for a new artificial language that resembles the target language. We show that delexicalized parsers for the target language can be successfully trained using such “made to order” artificial languages.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5aa69bbe1a00e3edadb4bb10c9c14beeaebf2690',\n",
       "  'title': 'Incorporating Subjectivity into Gendered Ambiguous Pronoun (GAP) Resolution using Style Transfer',\n",
       "  'authorId': '1384359611',\n",
       "  'authorName': 'Kartikey Pant',\n",
       "  'abstract': 'The GAP dataset is a Wikipedia-based evaluation dataset for gender bias detection in coreference resolution, containing mostly objective sentences. Since subjectivity is ubiquitous in our daily texts, it becomes necessary to evaluate models for both subjective and objective instances. In this work, we present a new evaluation dataset for gender bias in coreference resolution, GAP-Subjective, which increases the coverage of the original GAP dataset by including subjective sentences. We outline the methodology used to create this dataset. Firstly, we detect objective sentences and transfer them into their subjective variants using a sequence-to-sequence model. Secondly, we outline the thresholding techniques based on fluency and content preservation to maintain the quality of the sentences. Thirdly, we perform automated and human-based analysis of the style transfer and infer that the transferred sentences are of high quality. Finally, we benchmark both GAP and GAP-Subjective datasets using a BERT-based model and analyze its predictive performance and gender bias.',\n",
       "  'year': 2022,\n",
       "  'venue': 'GEBNLP'},\n",
       " {'paperId': 'b2075a1ce0e9bea9a6b6c0448c67de067471e885',\n",
       "  'title': 'AMR Parsing via Graph-Sequence Iterative Inference',\n",
       "  'authorId': '46689291',\n",
       "  'authorName': 'Deng Cai',\n",
       "  'abstract': 'We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'b0572307afb7e769f360267d893500893f5d6b3d',\n",
       "  'title': 'SemEval-2017 Task 7: Detection and Interpretation of English Puns',\n",
       "  'authorId': '1818919',\n",
       "  'authorName': 'Tristan Miller',\n",
       "  'abstract': \"A pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect.\\xa0 Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-per-context assumption.\\xa0 This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns.\\xa0 We describe the motivation for these tasks, the evaluation methods, and the manually annotated data set.\\xa0 Finally, we present an overview and discussion of the participating systems' methodologies, resources, and results.\",\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': 'dc111b06ecf7ad7fdfc19a7a46d9208e6d001894',\n",
       "  'title': 'Rhetorical Structure in Dialog',\n",
       "  'authorId': '1690152',\n",
       "  'authorName': 'Amanda Stent',\n",
       "  'abstract': 'In this paper we report on several issues arising out of a first attempt to annotate task-oriented spoken dialog for rhetorical structure using Rhetorical Structure Theory. We discuss an annotation scheme we are developing to resolve the difficulties we have encountered.',\n",
       "  'year': 2000,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': '790e306355330ad10c1fc8216a8de774545f2ad7',\n",
       "  'title': 'Measuring the Similarity of Sentential Arguments in Dialogue',\n",
       "  'authorId': '2461655',\n",
       "  'authorName': 'Amita Misra',\n",
       "  'abstract': 'When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. Debate websites produce curated summaries of arguments on such topics; these summaries typically consist of lists of sentences that represent frequently paraphrased propositions, or labels capturing the essence of one particular aspect of an argument, e.g. Morality or Second Amendment. We call these frequently paraphrased propositions ARGUMENT FACETS. Like these curated sites, our goal is to induce and identify argument facets across multiple conversations, and produce summaries. However, we aim to do this automatically. We frame the problem as consisting of two steps: we first extract sentences that express an argument from raw social media dialogs, and then rank the extracted arguments in terms of their similarity to one another. Sets of similar arguments are used to represent argument facets. We show here that we can predict ARGUMENT FACET SIMILARITY with a correlation averaging 0.63 compared to a human topline averaging 0.68 over three debate topics, easily beating several reasonable baselines.',\n",
       "  'year': 2016,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '9e2fd661a172749a739fc146301d3636e4a9c4dc',\n",
       "  'title': 'Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction',\n",
       "  'authorId': '145119697',\n",
       "  'authorName': 'Shun Zheng',\n",
       "  'abstract': 'Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '9eda533cf0badf8dbed5c8240bb828b622328183',\n",
       "  'title': 'Fine-grained Fact Verification with Kernel Graph Attention Network',\n",
       "  'authorId': '49047064',\n",
       "  'authorName': 'Zhenghao Liu',\n",
       "  'abstract': 'Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions. Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification. KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification. Our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT’s effectiveness. All source codes of this work are available at https://github.com/thunlp/KernelGAT.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '99533a0d7e0f74dc77c7bf320afc13b075a316c7',\n",
       "  'title': 'A Simple Approach to Learning Unsupervised Multilingual Embeddings',\n",
       "  'authorId': '1720974',\n",
       "  'authorName': 'Pratik Jawanpuria',\n",
       "  'abstract': 'Recent progress on unsupervised learning of cross-lingual embeddings in bilingual setting has given impetus to learning a shared embedding space for several languages without any supervision. A popular framework to solve the latter problem is to jointly solve the following two sub-problems: 1) learning unsupervised word alignment between several pairs of languages, and 2) learning how to map the monolingual embeddings of every language to a shared multilingual space. In contrast, we propose a simple, two-stage framework in which we decouple the above two sub-problems and solve them separately using existing techniques. The proposed approach obtains surprisingly good performance in various tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing. When distant languages are involved, the proposed solution illustrates robustness and outperforms existing unsupervised multilingual word embedding approaches. Overall, our experimental results encourage development of multi-stage models for such challenging problems.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '25f32188f2aa7bc3a809372c24b17f863d887a10',\n",
       "  'title': 'Comparing Encoder-Only and Encoder-Decoder Transformers for Relation Extraction from Biomedical Texts: An Empirical Study on Ten Benchmark Datasets',\n",
       "  'authorId': '2692127',\n",
       "  'authorName': 'Mourad Sarrouti',\n",
       "  'abstract': 'Biomedical relation extraction, aiming to automatically discover high-quality and semantic relations between the entities from free text, is becoming a vital step for automated knowledge discovery. Pretrained language models have achieved impressive performance on various natural language processing tasks, including relation extraction. In this paper, we perform extensive empirical comparisons of encoder-only transformers with the encoder-decoder transformer, specifically T5, on ten public biomedical relation extraction datasets. We study the relation extraction task from four major biomedical tasks, namely chemical-protein relation extraction, disease-protein relation extraction, drug-drug interaction, and protein-protein interaction. We also explore the use of multi-task fine-tuning to investigate the correlation among major biomedical relation extraction tasks. We report performance (micro F-score) using T5, BioBERT and PubMedBERT, demonstrating that T5 and multi-task learning can improve the performance of the biomedical relation extraction task.',\n",
       "  'year': 2022,\n",
       "  'venue': 'BIONLP'},\n",
       " {'paperId': 'fa025e5d117929361bcf798437957762eb5bb6d4',\n",
       "  'title': 'Zero-Shot Relation Extraction via Reading Comprehension',\n",
       "  'authorId': '39455775',\n",
       "  'authorName': 'Omer Levy',\n",
       "  'abstract': 'We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.',\n",
       "  'year': 2017,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': '8a1a658e7ca2bf2de60462872465a3d4a948844b',\n",
       "  'title': 'If You Build Your Own NER Scorer, Non-replicable Results Will Come',\n",
       "  'authorId': '1737047',\n",
       "  'authorName': 'Constantine Lignos',\n",
       "  'abstract': 'We attempt to replicate a named entity recognition (NER) model implemented in a popular toolkit and discover that a critical barrier to doing so is the inconsistent evaluation of improper label sequences. We define these sequences and examine how two scorers differ in their handling of them, finding that one approach produces F1 scores approximately 0.5 points higher on the CoNLL 2003 English development and test sets. We propose best practices to increase the replicability of NER evaluations by increasing transparency regarding the handling of improper label sequences.',\n",
       "  'year': 2020,\n",
       "  'venue': 'INSIGHTS'},\n",
       " {'paperId': '101fc81be485f2c4e5e0f4a5fb45d1216e4533b7',\n",
       "  'title': 'Fortifying Toxic Speech Detectors against Disguised Toxicity',\n",
       "  'authorId': '40500540',\n",
       "  'authorName': 'Xiaochuang Han',\n",
       "  'abstract': \"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector's training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.\",\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '043af12b66c2d598739921ac541d6d49c744aa07',\n",
       "  'title': 'Situated Dialogue Learning through Procedural Environment Generation',\n",
       "  'authorId': '19179135',\n",
       "  'authorName': 'Prithviraj Ammanabrolu',\n",
       "  'abstract': 'We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution—an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ce59e68320848d9358b7574a0c9918756d88db17',\n",
       "  'title': 'The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges',\n",
       "  'authorId': '144224160',\n",
       "  'authorName': 'Beatrice Alex',\n",
       "  'abstract': 'In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students’ learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds.',\n",
       "  'year': 2021,\n",
       "  'venue': 'TEACHINGNLP'},\n",
       " {'paperId': '5860a361b9987572405b71a6b78d9bce347d1c40',\n",
       "  'title': 'Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism',\n",
       "  'authorId': '49776272',\n",
       "  'authorName': 'Pengfei Cao',\n",
       "  'abstract': 'Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories. For Chinese NER task, there is only a very small amount of annotated data available. Chinese NER task and Chinese word segmentation (CWS) task have many similar word boundaries. There are also specificities in each task. However, existing methods for Chinese NER either do not exploit word boundary information from CWS or cannot filter the specific information of CWS. In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the task-specific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit self-attention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used datasets show that our proposed model significantly and consistently outperforms other state-of-the-art methods.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '8ce4f05303f0850e58748af2c6757eb12a147f2c',\n",
       "  'title': 'Learning to Embed Words in Context for Syntactic Tasks',\n",
       "  'authorId': '3376969',\n",
       "  'authorName': 'Lifu Tu',\n",
       "  'abstract': 'We present models for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data. We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.',\n",
       "  'year': 2017,\n",
       "  'venue': 'Rep4NLP@ACL'},\n",
       " {'paperId': '3a54f19a29b2c8e2deb3fa1a0d8245c48fe14387',\n",
       "  'title': 'CAMsterdam at SemEval-2019 Task 6: Neural and graph-based feature extraction for the identification of offensive tweets',\n",
       "  'authorId': '146765524',\n",
       "  'authorName': 'Guy Aglionby',\n",
       "  'abstract': 'We describe the CAMsterdam team entry to the SemEval-2019 Shared Task 6 on offensive language identification in Twitter data. Our proposed model learns to extract textual features using a multi-layer recurrent network, and then performs text classification using gradient-boosted decision trees (GBDT). A self-attention architecture enables the model to focus on the most relevant areas in the text. In order to enrich input representations, we use node2vec to learn globally optimised embeddings for hashtags, which are then given as additional features to the GBDT classifier. Our best model obtains 78.79% macro F1-score on detecting offensive language (subtask A), 66.32% on categorising offence types (targeted/untargeted; subtask B), and 55.36% on identifying the target of offence (subtask C).',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '9985c55a9dccb63597b88440de78dc8342e1ebc1',\n",
       "  'title': 'Morphological Disambiguation by Voting Constraints',\n",
       "  'authorId': '1723120',\n",
       "  'authorName': 'Kemal Oflazer',\n",
       "  'abstract': 'We present a constraint-based morphological disambiguation system in which individual constraints vote on matching morphological parses, and disambiguation of all the tokens in a sentence is performed at the end by selecting parses that receive the highest votes. This constraint application paradigm makes the outcome of the disambiguation independent of the rule sequence, and hence relieves the rule developer from worrying about potentially conflicting rule sequencing. Our results for disambiguating Turkish indicate that using about 500 constraint rules and some additional simple statistics, we can attain a recall of 95--96% and a precision of 94--95% with about 1.01 parses per token. Our system is implemented in Prolog and we are currently investigating an efficient implementation based on finite state transducers.',\n",
       "  'year': 1997,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '033ec1f832297e11f2a7cc4aa7b462a125186d1b',\n",
       "  'title': 'MixingBoard: a Knowledgeable Stylized Integrated Text Generation Platform',\n",
       "  'authorId': '143856160',\n",
       "  'authorName': 'Xiang Gao',\n",
       "  'abstract': 'We present MixingBoard, a platform for quickly building demos with a focus on knowledge grounded stylized text generation. We unify existing text generation algorithms in a shared codebase and further adapt earlier algorithms for constrained generation. To borrow advantages from different models, we implement strategies for cross-model integration, from the token probability level to the latent space level. An interface to external knowledge is provided via a module that retrieves, on-the-fly, relevant knowledge from passages on the web or a document collection. A user interface for local development, remote webpage access, and a RESTful API are provided to make it simple for users to build their own demos.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a2b861604efd9fdbccbb83d2bfe90bea1e33fed1',\n",
       "  'title': 'QuoteR: A Benchmark of Quote Recommendation for Writing',\n",
       "  'authorId': '51466208',\n",
       "  'authorName': 'Fanchao Qi',\n",
       "  'abstract': 'It is very common to use quotations (quotes) to make our writings more elegant or convincing. To help people find appropriate quotes efficiently, the task of quote recommendation is presented, aiming to recommend quotes that fit the current context of writing. There have been various quote recommendation approaches, but they are evaluated on different unpublished datasets. To facilitate the research on this task, we build a large and fully open quote recommendation dataset called QuoteR, which comprises three parts including English, standard Chinese and classical Chinese. Any part of it is larger than previous unpublished counterparts. We conduct an extensive evaluation of existing quote recommendation methods on QuoteR. Furthermore, we propose a new quote recommendation model that significantly outperforms previous methods on all three parts of QuoteR. All the code and data of this paper can be obtained at https://github.com/thunlp/QuoteR.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'fb609c59b696e0634cb822d4fc40337e4bfcfa14',\n",
       "  'title': 'Si O No, Que Penses? Catalonian Independence and Linguistic Identity on Social Media',\n",
       "  'authorId': '144925592',\n",
       "  'authorName': 'Ian Stewart',\n",
       "  'abstract': 'Political identity is often manifested in language variation, but the relationship between the two is still relatively unexplored from a quantitative perspective. This study examines the use of Catalan, a language local to the semi-autonomous region of Catalonia in Spain, on Twitter in discourse related to the 2017 independence referendum. We corroborate prior findings that pro-independence tweets are more likely to include the local language than anti-independence tweets. We also find that Catalan is used more often in referendum-related discourse than in other contexts, contrary to prior findings on language variation. This suggests a strong role for the Catalan language in the expression of Catalonian political identity.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '472cd41fa2ba2e520706f232cae12db4a7b5e60a',\n",
       "  'title': 'Contextualized Perturbation for Textual Adversarial Attack',\n",
       "  'authorId': '7379232',\n",
       "  'authorName': 'Dianqi Li',\n",
       "  'abstract': 'Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '28f8535165e86c8e545e004c26e61ef7d4b466e3',\n",
       "  'title': 'Exploiting Word Internal Structures for Generic Chinese Sentence Representation',\n",
       "  'authorId': '2761251',\n",
       "  'authorName': 'Shaonan Wang',\n",
       "  'abstract': 'We introduce a novel mixed characterword architecture to improve Chinese sentence representations, by utilizing rich semantic information of word internal structures. Our architecture uses two key strategies. The first is a mask gate on characters, learning the relation among characters in a word. The second is a maxpooling operation on words, adaptively finding the optimal mixture of the atomic and compositional word representations. Finally, the proposed architecture is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b8a765c86cd03a56c48528c2c8ed8ecd5d2856c1',\n",
       "  'title': 'Entity-based Neural Local Coherence Modeling',\n",
       "  'authorId': '5765750',\n",
       "  'authorName': 'Sungho Jeon',\n",
       "  'abstract': 'In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models. Recent neural coherence models encode the input document using large-scale pretrained language models. Hence their basis for computing local coherence are words and even sub-words. The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role. Still, these models achieve state-of-the-art performance in several end applications. In contrast to these models, we compute coherence on the basis of entities by constraining the input to noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences leading to the notion of focus. This brings our model linguistically in line with pre-neural models of computing coherence. It also gives us better insight into the behaviour of the model thus leading to better explainability. Our approach is also in accord with a recent study (O’Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models. We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1ceba9b7499ef45eb3146c6cecd3b27e3bcacb13',\n",
       "  'title': 'Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification',\n",
       "  'authorId': '1771202',\n",
       "  'authorName': 'Linmei Hu',\n",
       "  'abstract': 'Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on long texts and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the graph. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '64a1dbdd7653eaca25c78e87335ee156b6f6959e',\n",
       "  'title': 'Constrained Language Models Yield Few-Shot Semantic Parsers',\n",
       "  'authorId': '39428234',\n",
       "  'authorName': 'Richard Shin',\n",
       "  'abstract': 'We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '8271250f79d232eadcbb8c1c26a50135fa649b05',\n",
       "  'title': 'CodeForTheChange at SemEval-2019 Task 8: Skip-Thoughts for Fact Checking in Community Question Answering',\n",
       "  'authorId': '52023192',\n",
       "  'authorName': 'Adithya Avvaru',\n",
       "  'abstract': 'The strengths of the scalable gradient tree boosting algorithm, XGBoost and distributed sentence encoder, Skip-Thought Vectors are not explored yet by the cQA research community. We tried to apply and combine these two effective methods for finding factual nature of the questions and answers. The work also include experimentation with other popular classifier models like AdaBoost Classifier, DecisionTree Classifier, RandomForest Classifier, ExtraTrees Classifier, XGBoost Classifier and Multi-layer Neural Network. In this paper, we present the features used, approaches followed for feature engineering, models experimented with and finally the results.',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '20de3fe9e7833d8468ece3dce2b32b5358c7ac25',\n",
       "  'title': 'Annotating Temporal Dependency Graphs via Crowdsourcing',\n",
       "  'authorId': '40040342',\n",
       "  'authorName': 'Jiarui Yao',\n",
       "  'abstract': 'We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora, provide a representation scheme that achieves a good trade-off between completeness and practicality in temporal annotation. We also provide a crowdsourcing strategy to annotate TDGs, and demonstrate the feasibility of this approach with an evaluation of the quality of the annotation, and the utility of the resulting data set by training a machine learning model on this data set. The data set is publicly available.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '69a974cbdc35c47cc2d23bf68b23c895f6a4ea95',\n",
       "  'title': 'Overview of the SustaiNLP 2020 Shared Task',\n",
       "  'authorId': '144906624',\n",
       "  'authorName': 'Alex Wang',\n",
       "  'abstract': 'We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We describe the task, its organization, and the submitted systems. Across the six submissions to the shared task, participants achieved efficiency gains of 20× over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SUSTAINLP'},\n",
       " {'paperId': '992018dd6d98f37352437c19f16c5bc87c3b6f22',\n",
       "  'title': 'Easy Questions First? A Case Study on Curriculum Learning for Question Answering',\n",
       "  'authorId': '2790926',\n",
       "  'authorName': 'Mrinmaya Sachan',\n",
       "  'abstract': 'Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems. Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning. Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively. We introduce a number of heuristics that improve upon selfpaced learning. Then, we argue that incorporating easy, yet, a diverse set of samples can further improve learning. We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e5bb97f5a2024d26b9da84cb6611b50a64509b00',\n",
       "  'title': 'Survey: Multiword Expression Processing: A Survey',\n",
       "  'authorId': '2614795',\n",
       "  'authorName': 'M. Constant',\n",
       "  'abstract': 'Multiword expressions (MWEs) are a class of linguistic forms spanning conventional word boundaries that are both idiosyncratic and pervasive across different languages. The structure of linguistic processing that depends on the clear distinction between words and phrases has to be re-thought to accommodate MWEs. The issue of MWE handling is crucial for NLP applications, where it raises a number of challenges. The emergence of solutions in the absence of guiding principles motivates this survey, whose aim is not only to provide a focused review of MWE processing, but also to clarify the nature of interactions between MWE processing and downstream applications. We propose a conceptual framework within which challenges and research contributions can be positioned. It offers a shared understanding of what is meant by “MWE processing,” distinguishing the subtasks of MWE discovery and identification. It also elucidates the interactions between MWE processing and two use cases: Parsing and machine translation. Many of the approaches in the literature can be differentiated according to how MWE processing is timed with respect to underlying use cases. We discuss how such orchestration choices affect the scope of MWE-aware systems. For each of the two MWE processing subtasks and for each of the two use cases, we conclude on open issues and research perspectives.',\n",
       "  'year': 2017,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': 'b21c78a62fbb945a19ae9a8935933711647e7d70',\n",
       "  'title': 'A Hierarchical Neural Autoencoder for Paragraphs and Documents',\n",
       "  'authorId': '49298465',\n",
       "  'authorName': 'Jiwei Li',\n",
       "  'abstract': 'Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Longshort term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '392b0c87a40ba12fe6182d042005ff0ab9582df9',\n",
       "  'title': 'HELP: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning',\n",
       "  'authorId': '3486313',\n",
       "  'authorName': 'Hitomi Yanaka',\n",
       "  'abstract': 'Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference (NLI). Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so-called monotonicity reasoning. Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model architectures themselves. To investigate this issue, we introduce a new dataset, called HELP, for handling entailments with lexical and logical phenomena. We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena. The results showed that our data augmentation improved the overall accuracy. We also find that the improvement is better on monotonicity inferences with lexical replacements than on downward inferences with disjunction and modification. This suggests that some types of inferences can be improved by our data augmentation while others are immune to it.',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'a22b5ba0f09ca5a5d8a88ff995828c72ffbbd673',\n",
       "  'title': 'Semantic Neural Machine Translation Using AMR',\n",
       "  'authorId': '1748796',\n",
       "  'authorName': 'Linfeng Song',\n",
       "  'abstract': 'It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model.',\n",
       "  'year': 2019,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': 'a81e68b1b63923637659742c60b55aa4436ecfee',\n",
       "  'title': 'Found in Translation: Reconstructing Phylogenetic Language Trees from Translations',\n",
       "  'authorId': '2653682',\n",
       "  'authorName': 'Ella Rabinovich',\n",
       "  'abstract': 'Translation has played an important role in trade, law, commerce, politics, and literature for thousands of years. Translators have always tried to be invisible; ideal translations should look as if they were written originally in the target language. We show that traces of the source language remain in the translation product to the extent that it is possible to uncover the history of the source language by looking only at the translation. Specifically, we automatically reconstruct phylogenetic language trees from monolingual texts (translated from several source languages). The signal of the source language is so powerful that it is retained even after two phases of translation. This strongly indicates that source language interference is the most dominant characteristic of translated texts, overshadowing the more subtle signals of universal properties of translation.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '332c57bdcd6782b4230b016fdff302fd897870fa',\n",
       "  'title': 'Ensembling Factored Neural Machine Translation Models for Automatic Post-Editing and Quality Estimation',\n",
       "  'authorId': '2140238',\n",
       "  'authorName': 'Chris Hokamp',\n",
       "  'abstract': 'This work presents a novel approach to Automatic Post-Editing (APE) and Word-Level Quality Estimation (QE) using ensembles of specialized Neural Machine Translation (NMT) systems. Word-level features that have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis. We train a suite of NMT models that use different input representations, but share the same output space. These models are then ensembled together, and tuned for both the APE and the QE task. We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework. Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.',\n",
       "  'year': 2017,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '278f3029a1ac81845e90fcd728c830d9897af552',\n",
       "  'title': 'A Universal System for Automatic Text-to-Phonetics Conversion',\n",
       "  'authorId': '6062349',\n",
       "  'authorName': 'C. Gafni',\n",
       "  'abstract': 'This paper describes an automatic text-to-phonetics conversion system. The system was constructed to primarily serve as a research tool. It is implemented in a general-purpose linguistic software, which allows it to be incorporated in a multifaceted linguistic research in essentially any language. The system currently relies on two mechanisms to generate phonetic transcriptions from texts: (i) importing ready-made phonetic word forms from external dictionaries, and (ii) automatic generation of phonetic word forms based on a set of deterministic linguistic rules. The current paper describes the proposed system and its potential application to linguistic research.',\n",
       "  'year': 2019,\n",
       "  'venue': 'RANLP'},\n",
       " {'paperId': 'cd0009c2819f9566930d520da46ca67e4ccf226d',\n",
       "  'title': 'Vocabulary Manipulation for Neural Machine Translation',\n",
       "  'authorId': '2013337',\n",
       "  'authorName': 'Haitao Mi',\n",
       "  'abstract': 'In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd16a3446188211608966da493194c3293c607673',\n",
       "  'title': 'Tailored Sequence to Sequence Models to Different Conversation Scenarios',\n",
       "  'authorId': '2108769177',\n",
       "  'authorName': 'Hainan Zhang',\n",
       "  'abstract': 'Sequence to sequence (Seq2Seq) models have been widely used for response generation in the area of conversation. However, the requirements for different conversation scenarios are distinct. For example, customer service requires the generated responses to be specific and accurate, while chatbot prefers diverse responses so as to attract different users. The current Seq2Seq model fails to meet these diverse requirements, by using a general average likelihood as the optimization criteria. As a result, it usually generates safe and commonplace responses, such as ‘I don’t know’. In this paper, we propose two tailored optimization criteria for Seq2Seq to different conversation scenarios, i.e., the maximum generated likelihood for specific-requirement scenario, and the conditional value-at-risk for diverse-requirement scenario. Experimental results on the Ubuntu dialogue corpus (Ubuntu service scenario) and Chinese Weibo dataset (social chatbot scenario) show that our proposed models not only satisfies diverse requirements for different scenarios, but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '18d04fa815cbc9e7a2eeab189a4d7388cb43b0cb',\n",
       "  'title': 'Using Predicate-Argument Structures for Information Extraction',\n",
       "  'authorId': '1760868',\n",
       "  'authorName': 'M. Surdeanu',\n",
       "  'abstract': 'In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.',\n",
       "  'year': 2003,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '5d72e7bd982c4b593c68b8af9cf157192185b3e6',\n",
       "  'title': 'Better Transition-Based AMR Parsing with a Refined Search Space',\n",
       "  'authorId': '2681038',\n",
       "  'authorName': 'Zhijiang Guo',\n",
       "  'abstract': 'This paper introduces a simple yet effective transition-based system for Abstract Meaning Representation (AMR) parsing. We argue that a well-defined search space involved in a transition system is crucial for building an effective parser. We propose to conduct the search in a refined search space based on a new compact AMR graph and an improved oracle. Our end-to-end parser achieves the state-of-the-art performance on various datasets with minimal additional information.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'c14f416bab5a4936deded9797883e14ac0b55fff',\n",
       "  'title': 'Differentiable Scheduled Sampling for Credit Assignment',\n",
       "  'authorId': '37195917',\n",
       "  'authorName': 'Kartik Goyal',\n",
       "  'abstract': 'We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding in sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure–a well-known technique for correcting exposure bias–we introduce a new training objective that is continuous and differentiable everywhere and can provide informative gradients near points where previous decoding decisions change their value. By using a related approximation, we also demonstrate a similar approach to sampled-based training. We show that our approach outperforms both standard cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '885d516b8a65fb64535b84a9a225a5c38dcc209a',\n",
       "  'title': 'Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline',\n",
       "  'authorId': '10324691',\n",
       "  'authorName': 'Kawin Ethayarajh',\n",
       "  'abstract': 'Using a random walk model of text generation, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings: take a weighted average of word embeddings and modify with SVD. This simple method even outperforms far more complex approaches such as LSTMs on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.’s model. We propose a random walk model that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our approach beats Arora et al.’s by up to 44.4% on textual similarity tasks and is competitive with state-of-the-art methods. Unlike Arora et al.’s method, ours requires no hyperparameter tuning, which means it can be used when there is no labelled data.',\n",
       "  'year': 2018,\n",
       "  'venue': 'Rep4NLP@ACL'},\n",
       " {'paperId': '266bd8542c87be1030d578638dc1b4e793b5a091',\n",
       "  'title': 'GO FIGURE: A Meta Evaluation of Factuality in Summarization',\n",
       "  'authorId': '119902504',\n",
       "  'authorName': 'Saadia Gabriel',\n",
       "  'abstract': \"Text generation models can generate factually inconsistent text containing distorted or fabricated facts about the source text. Recent work has focused on building evaluation models to verify the factual correctness of semantically constrained text generation tasks such as document summarization. While the field of factuality evaluation is growing fast, we don't have well-defined criteria for measuring the effectiveness, generalizability, reliability, or sensitivity of the factuality metrics. Focusing on these aspects, in this paper, we introduce a meta-evaluation framework for evaluating factual consistency metrics. We introduce five necessary, common-sense conditions for effective factuality metrics and experiment with nine recent factuality metrics using synthetic and human-labeled factuality data from short news, long news and dialogue summarization domains. Our framework enables assessing the efficiency of any new factual consistency metric on a variety of dimensions over multiple summarization domains and can be easily extended with new meta-evaluation criteria. We also present our conclusions towards standardizing the factuality evaluation metrics.\",\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'f678d4e43bb0b0c4cb886d0903e979cb7030963f',\n",
       "  'title': 'Hitachi at SemEval-2017 Task 12: System for temporal information extraction from clinical notes',\n",
       "  'authorId': '1409284362',\n",
       "  'authorName': 'R. SarathP.',\n",
       "  'abstract': 'This paper describes the system developed for the task of temporal information extraction from clinical narratives in the context of the 2017 Clinical TempEval challenge. Clinical TempEval 2017 addressed the problem of temporal reasoning in the clinical domain by providing annotated clinical notes, pathology and radiology reports in line with Clinical TempEval challenges 2015/16, across two different evaluation phases focusing on cross domain adaptation. Our team focused on subtasks involving extractions of temporal spans and relations for which the developed systems showed average F-score of 0.45 and 0.47 across the two phases of evaluations.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': 'ea988545ee21123dfa709233078c0afeabf87b24',\n",
       "  'title': 'Capturing CFLs with Tree Adjoining Grammars',\n",
       "  'authorId': '144156572',\n",
       "  'authorName': 'J. Rogers',\n",
       "  'abstract': 'We define a decidable class of TAGs that is strongly equivalent to CFGs and is cubic-time parsable. This class serves to lexicalize CFGs in the same manner as the LCFGs of Schabes and Waters but with considerably less restriction on the form of the grammars. The class provides a normal form for TAGs that generate local sets in much the same way that regular grammars provide a normal form for CFGs that generate regular sets.',\n",
       "  'year': 1994,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '9215ff5314fd9148680086e94878ffa0de9c8c06',\n",
       "  'title': 'Modeling Negotiation Subdialogues',\n",
       "  'authorId': '34800611',\n",
       "  'authorName': 'Lynn Lambert',\n",
       "  'abstract': 'This paper presents a plan-based model that handles negotiation subdialogues by inferring both the communicative actions that people pursue when speaking and the beliefs underlying these actions. We contend that recognizing the complex discourse actions pursued in negotiation subdialogues (e.g., expressing doubt) requires both a multistrength belief model and a process model that combines different knowledge sources in a unified framework. We show how our model identifies the structure of negotiation subdialogues, including recognizing expressions of doubt, implicit acceptance of communicated propositions, and negotiation subdialogues embedded within other negotiation subdialogues.',\n",
       "  'year': 1992,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '9d436d25981ea61db728bd490f0d54376d08953e',\n",
       "  'title': 'Temporal Reasoning on Implicit Events from Distant Supervision',\n",
       "  'authorId': '145360756',\n",
       "  'authorName': 'Ben Zhou',\n",
       "  'abstract': 'We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events—events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SymTime, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. SymTime outperforms strong baseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '928f9dccb806a3278d20d82cc53781c5f44e2bb1',\n",
       "  'title': 'Constituency Parsing with a Self-Attentive Encoder',\n",
       "  'authorId': '143808231',\n",
       "  'authorName': 'Nikita Kitaev',\n",
       "  'abstract': 'We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '73068d13d6e53876c374ebd4c862ec01351c9f39',\n",
       "  'title': 'Learning to Represent Image and Text with Denotation Graphs',\n",
       "  'authorId': '3047890',\n",
       "  'authorName': 'Bowen Zhang',\n",
       "  'abstract': 'Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images. In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on this https URL.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a032bbeb523215a9462afafc17a047fa37b7d735',\n",
       "  'title': 'Improved Parsing for Argument-Clusters Coordination',\n",
       "  'authorId': '1899924',\n",
       "  'authorName': 'Jessica Ficler',\n",
       "  'abstract': 'Syntactic parsers perform poorly in prediction of Argument-Cluster Coordination (ACC). We change the PTB representation of ACC to be more suitable for learning by a statistical PCFG parser, affecting 125 trees in the training set. Training on the modified trees yields a slight improvement in EVALB scores on sections 22 and 23. The main evaluation is on a corpus of 4th grade science exams, in which ACC structures are prevalent. On this corpus, we obtain an impressive x2.7 improvement in recovering ACC structures compared to a parser trained on the original PTB trees.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ee4385aaae742a14a5db828a5076208c9b33da88',\n",
       "  'title': 'Learning Event Graph Knowledge for Abductive Reasoning',\n",
       "  'authorId': '49134163',\n",
       "  'authorName': 'L. Du',\n",
       "  'abstract': 'Abductive reasoning aims at inferring the most plausible explanation for observed events, which would play critical roles in various NLP applications, such as reading comprehension and question answering. To facilitate this task, a narrative text based abductive reasoning task \\\\alphaNLI is proposed, together with explorations about building reasoning framework using pretrained language models. However, abundant event commonsense knowledge is not well exploited for this task. To fill this gap, we propose a variational autoencoder based model ege-RoBERTa, which employs a latent variable to capture the necessary commonsense knowledge from event graph for guiding the abductive reasoning task. Experimental results show that through learning the external event graph knowledge, our approach outperforms the baseline methods on the \\\\alphaNLI task.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '4040297f318bc05f5661cd7063e041456f2736c7',\n",
       "  'title': 'The Logical Analysis of Lexical Ambiguity',\n",
       "  'authorId': '145482266',\n",
       "  'authorName': 'D. Stallard',\n",
       "  'abstract': 'Theories of semantic interpretation which wish to capture as many generalizations as possible must face up to the manifoldly ambiguous and contextually dependent nature of word meaning. In this paper I present a two-level scheme of semantic interpretation in which the first level deals with the semantic consequences of syntactic structure and the second with the choice of word meaning. On the first level the meanings of ambiguous words, pronominal references, nominal compounds and metonomies are not treated as fixed, but are instead represented by free variables which range over predicates and functions. The context-dependence of lexical meaning is dealt with by the second level, a constraint propagation process which attempts to assign values to these variables on the basis of the logical coherence of the overall result. In so doing it makes use of a set of polysemy operators which map between lexical senses, thus making a potentially indefinite number of related senses available.',\n",
       "  'year': 1987,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '93f3e3ddc5ef5bd84a42be80ad87405635ec64af',\n",
       "  'title': 'Web-style ranking and SLU combination for dialog state tracking',\n",
       "  'authorId': '47271859',\n",
       "  'authorName': 'J. Williams',\n",
       "  'abstract': 'In spoken dialog systems, statistical state tracking aims to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states. This paper introduces two novel methods for this task. First, we explain how state tracking is structurally similar to web-style ranking, enabling mature, powerful ranking algorithms to be applied. Second, we show how to use multiple spoken language understanding engines (SLUs) in state tracking — multiple SLUs can expand the set of dialog states being tracked, and give more information about each, thereby increasing both recall and precision of state tracking. We evaluate on the second Dialog State Tracking Challenge; together these two techniques yield highest accuracy in 2 of 3 tasks, including the most difficult and general task.',\n",
       "  'year': 2014,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '25109699b098c786832c906e4b36fa76fb2b66a0',\n",
       "  'title': 'AMR dependency parsing with a typed semantic algebra',\n",
       "  'authorId': '2960997',\n",
       "  'authorName': 'Jonas Groschwitz',\n",
       "  'abstract': 'We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'da6815f291520640d00f6a978f6172933c133c39',\n",
       "  'title': 'Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition',\n",
       "  'authorId': '2119264432',\n",
       "  'authorName': 'Dingmin Wang',\n",
       "  'abstract': 'We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for clas- sification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both complexity and scalability when compared to the state-of-the-art methods, especially in the scenario of multi-domain dialogues entangled with many turns of utterances.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'c4feef736f44dd8c0e43dc15bdb90fcf2ba3d385',\n",
       "  'title': 'Training a Korean SRL System with Rich Morphological Features',\n",
       "  'authorId': '49170818',\n",
       "  'authorName': 'Young-Bum Kim',\n",
       "  'abstract': 'In this paper we introduce a semantic role labeler for Korean, an agglutinative language with rich morphology. First, we create a novel training source by semantically annotating a Korean corpus containing fine-grained morphological and syntactic information. We then develop a supervised SRL model by leveraging morphological features of Korean that tend to correspond with semantic roles. Our model also employs a variety of latent morpheme representations induced from a larger body of unannotated Korean text. These elements lead to state-of-the-art performance of 81.07% labeled F1, representing the best SRL performance reported to date for an agglutinative language.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c6ecbe644addc8d33159a873df0b39935d052da6',\n",
       "  'title': 'UWB: Machine Learning Approach to Aspect-Based Sentiment Analysis',\n",
       "  'authorId': '1917913',\n",
       "  'authorName': 'Tomas Brychcin',\n",
       "  'abstract': 'This paper describes our system participating in the aspect-based sentiment analysis task of Semeval 2014. The goal was to identify the aspects of given target entities and the sentiment expressed towards each aspect. We firstly introduce a system based on supervised machine learning, which is strictly constrained and uses the training data as the only source of information. This system is then extended by unsupervised methods for latent semantics discovery (LDA and semantic spaces) as well as the approach based on sentiment vocabularies. The evaluation was done on two domains, restaurants and laptops. We show that our approach leads to very promising results.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '851034d13010e22f7c5e95a5925aa7a452519eac',\n",
       "  'title': 'The WebNLG Challenge: Generating Text from DBPedia Data',\n",
       "  'authorId': '4145312',\n",
       "  'authorName': 'Emilie Colin',\n",
       "  'abstract': 'With the emergence of the linked data initiative and the rapid development of RDF (Resource Description Format) datasets, several approaches have recently been proposed for generating text from RDF data (Sun and Mellish, 2006; Duma and Klein, 2013; Bontcheva and Wilks, 2004; Cimiano et al., 2013; Lebret et al., 2016). To support the evaluation and comparison of such systems, we propose a shared task on generating text from DBPedia data. The training data will consist of Data/Text pairs where the data is a set of triples extracted from DBPedia and the text is a verbalisation of these triples. In essence, the task consists in mapping data to text. Specific subtasks include sentence segmentation (how to chunk the input data into sentences), lexicalisation (of the DBPedia properties), aggregation (how to avoid repetitions) and surface realisation (how to build a syntactically correct and natural sounding text).',\n",
       "  'year': 2016,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': '6deb944e1d9294e3c3f121db5576ed5827520a88',\n",
       "  'title': 'Boosting Entity Linking Performance by Leveraging Unlabeled Documents',\n",
       "  'authorId': '144106794',\n",
       "  'authorName': 'Phong Le',\n",
       "  'abstract': 'Modern entity linking systems rely on large collections of documents specifically annotated for the task (e.g., AIDA CoNLL). In contrast, we propose an approach which exploits only naturally occurring information: unlabeled documents and Wikipedia. Our approach consists of two stages. First, we construct a high recall list of candidate entities for each mention in an unlabeled document. Second, we use the candidate lists as weak supervision to constrain our document-level entity linking model. The model treats entities as latent variables and, when estimated on a collection of unlabelled texts, learns to choose entities relying both on local context of each mention and on coherence with other entities in the document. The resulting approach rivals fully-supervised state-of-the-art systems on standard test sets. It also approaches their performance in the very challenging setting: when tested on a test set sampled from the data used to estimate the supervised systems. By comparing to Wikipedia-only training of our model, we demonstrate that modeling unlabeled documents is beneficial.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '76984a8d4fbd069ad720152b473e330e64dc975b',\n",
       "  'title': 'Atypical Inputs in Educational Applications',\n",
       "  'authorId': '2669255',\n",
       "  'authorName': 'Su-Youn Yoon',\n",
       "  'abstract': 'In large-scale educational assessments, the use of automated scoring has recently become quite common. While the majority of student responses can be processed and scored without difficulty, there are a small number of responses that have atypical characteristics that make it difficult for an automated scoring system to assign a correct score. We describe a pipeline that detects and processes these kinds of responses at run-time. We present the most frequent kinds of what are called non-scorable responses along with effective filtering models based on various NLP and speech processing technologies. We give an overview of two operational automated scoring systems —one for essay scoring and one for speech scoring— and describe the filtering models they use. Finally, we present an evaluation and analysis of filtering models used for spoken responses in an assessment of language proficiency.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'b4b476ea4dbced01dbf9abd6114eebaf27288892',\n",
       "  'title': 'A Natural Diet: Towards Improving Naturalness of Machine Translation Output',\n",
       "  'authorId': '35307070',\n",
       "  'authorName': 'Markus Freitag',\n",
       "  'abstract': 'Machine translation (MT) evaluation often focuses on accuracy and fluency, without paying much attention to translation style. This means that, even when considered accurate and fluent, MT output can still sound less natural than high quality human translations or text originally written in the target language. Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence. In this work we propose a method for training MT systems to achieve a more natural style, i.e. mirroring the style of text originally written in the target language. Our method tags parallel training data according to the naturalness of the target side by contrasting language models trained on natural and translated data. Tagging data allows us to put greater emphasis on target sentences originally written in the target language. Automatic metrics show that the resulting models achieve lexical richness on par with human translations, mimicking a style much closer to sentences originally written in the target language. Furthermore, we find that their output is preferred by human experts when compared to the baseline translations.',\n",
       "  'year': 2022,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '67d40c3f7470287a3bccfccdb506bcb6d522ac8c',\n",
       "  'title': 'NCRF++: An Open-source Neural Sequence Labeling Toolkit',\n",
       "  'authorId': '145889118',\n",
       "  'authorName': 'Jie Yang',\n",
       "  'abstract': 'This paper describes NCRF++, a toolkit for neural sequence labeling. NCRF++ is designed for quick implementation of different neural sequence labeling models with a CRF inference layer. It provides users with an inference for building the custom model structure through configuration file with flexible neural feature design and utilization. Built on PyTorch http://pytorch.org/, the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '2bc2d3788d698ba398170f369670b5094f9d86fb',\n",
       "  'title': 'Lexical Paraphrasing for Document Retrieval and Node Identification',\n",
       "  'authorId': '48127057',\n",
       "  'authorName': 'Ingrid Zukerman',\n",
       "  'abstract': \"We investigate lexical paraphrasing in the context of two distinct applications: document retrieval and node identification. Document retrieval --- the first step in question answering --- retrieves documents that contain answers to user queries. Node identification --- performed in the context of a Bayesian argumentation system --- matches users' Natural Language sentences to nodes in a Bayesian network. Lexical paraphrases are generated using syntactic, semantic and corpus-based information. Our evaluation shows that lexical paraphrasing improves retrieval performance for both applications.\",\n",
       "  'year': 2003,\n",
       "  'venue': 'IWP@ACL'},\n",
       " {'paperId': '1574bbf90bdbf3ca27044c888d6f2c568a751fb6',\n",
       "  'title': 'deepQuest-py: Large and Distilled Models for Quality Estimation',\n",
       "  'authorId': '69930782',\n",
       "  'authorName': 'Fernando Alva-Manchego',\n",
       "  'abstract': 'We introduce deepQuest-py, a framework for training and evaluation of large and light-weight models for Quality Estimation (QE). deepQuest-py provides access to (1) state-of-the-art models based on pre-trained Transformers for sentence-level and word-level QE; (2) light-weight and efficient sentence-level models implemented via knowledge distillation; and (3) a web interface for testing models and visualising their predictions. deepQuest-py is available at https://github.com/sheffieldnlp/deepQuest-py under a CC BY-NC-SA licence.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '33560c52a5a90e1074a9c341b752bd9e8ac86f7d',\n",
       "  'title': 'AcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models',\n",
       "  'authorId': '1633124736',\n",
       "  'authorName': 'Yue Yu',\n",
       "  'abstract': 'Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training. Additionally, we design (1) a region-aware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model’s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2% on average. Our implementation is available at https://github.com/yueyu1030/actune.',\n",
       "  'year': 2022,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '08b63c60946fbba803518a6e55d27d58c37ee031',\n",
       "  'title': 'Last Words: Breaking News: Changing Attitudes and Practices',\n",
       "  'authorId': '1736049',\n",
       "  'authorName': 'B. Webber',\n",
       "  'abstract': 'Standard practice in our field has been to announce research results at our annual conference or one of its affiliated meetings such as EMNLP or at the biennial COLING conference. This year, by its submission deadline of 23 January 2007, the ACL Program Committee had received 588 main conference submissions, plus another 52 submissions to the Student Research Workshop, for a total of 640 papers. Similarly, by its subsequent deadline of 26 March 2007, the EMNLP Program Committee had received 398 submissions (excluding ones that were withdrawn or rejected without review). It was estimated that about a third of these (say 130) were the same or minor variations of papers submitted to ACL conference.1 With over 900 separate submissions, one might wonder if all breakthroughs in our field are really made in late fall or winter, just in time for these deadlines. If they’re not, why is it that these deadlines seem to define when new results are announced? Is there no credit to be gained from really being the first to publish some new method or theory or some clever take on an old one? Or are there no places to publish that will guarantee catching the field’s immediate attention (our equivalent of Science, Nature, or YouTube)? In short, why the veritable flood of words crashing up against conference deadlines and the veritable trickle reaching the editorial offices of the significant (and still growing) number of CL/NLP-related journals. A choice is clearly being made by researchers in the field, but is it one that should be encouraged? Could change bring about some better situation? Although our journals and conferences are well-respected (and the latter are also great fun and a major contributor to our sense of community), frustration with both has been heating up over the last year or so, and clear calls for change are in the air. The following is a summary of what I myself believe or have heard others claim to believe, along with some suggestions for possible solutions. I am indebted to discussions with Aravind Joshi, Mark Steedman, Lauri Karttunen, Julia Hockenmeier, Annie Zaenen (co-Editor-in-Chief of Linguistic Issues in Language Technology), John Tait (Executive Editor of Journal of Natural Language Engineering), Kam-Fai Wong and Jun’ichi Tsujii (co-Editors-in-Chief of ACM Transactions on Asian Language Information Processing), Shalom Lappin (co-Editor-in-Chief of Research on Language and Computation), and Robert Dale (Editor-in-Chief of this journal, Computational Linguistics), as well as the many comments I have read at the Natural Language Processing blog',\n",
       "  'year': 2007,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': '23d21bb495061564d2ce4540ea312959ab5e10f0',\n",
       "  'title': 'Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification',\n",
       "  'authorId': '1906099',\n",
       "  'authorName': 'Shuhuai Ren',\n",
       "  'abstract': 'Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous methods, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a framework named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for data augmentation. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best policy, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '8279fe474e25ee954d71f34f310a6dfdc09e5438',\n",
       "  'title': 'A View of Parsing',\n",
       "  'authorId': '1803660',\n",
       "  'authorName': 'R. Kaplan',\n",
       "  'abstract': 'The questions before this panel presuppose a distinction between parsing and interpretation. There are two other simple and obvious distinctions that I think are necessary for a reasonable discussion of the issues. First, we must clearly distinguish between the static specification of a process and its dynamic execution. Second, we must clearly distinguish two purposes that a natural language processing system might serve: one legitimate goal of a system is to perform some practical task efficiently and well, while a second goal is to assist in developing a scientific understanding of the cognitive operations that underlie human language processing. I will refer to parsers primarily oriented towards the former goal as Practical Parsers (PP) and refer to the others as Performance Model Parsers (PMP). With these distinctions in mind, let me now turn to the questions at hand.',\n",
       "  'year': 1981,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '9606ff5bdeb6b9bde63c5bf6ad22edeca51d35db',\n",
       "  'title': 'Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase',\n",
       "  'authorId': '1689647',\n",
       "  'authorName': 'Peter D. Turney',\n",
       "  'abstract': 'There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).',\n",
       "  'year': 2013,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '8ecaff11e660cc225f390b1a07fe5c1918c9d4af',\n",
       "  'title': 'Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce',\n",
       "  'authorId': '2642333',\n",
       "  'authorName': 'Minghui Qiu',\n",
       "  'abstract': 'Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist and observed a significant improvement over the existing online model.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '53f18d3bee0abdab40e3264862b3febbcd2cfe64',\n",
       "  'title': 'A Dataset and Classifier for Recognizing Social Media English',\n",
       "  'authorId': '3422038',\n",
       "  'authorName': 'Su Lin Blodgett',\n",
       "  'abstract': 'While language identification works well on standard texts, it performs much worse on social media language, in particular dialectal language—even for English. First, to support work on English language identification, we contribute a new dataset of tweets annotated for English versus non-English, with attention to ambiguity, code-switching, and automatic generation issues. It is randomly sampled from all public messages, avoiding biases towards pre-existing language classifiers. Second, we find that a demographic language model—which identifies messages with language similar to that used by several U.S. ethnic populations on Twitter—can be used to improve English language identification performance when combined with a traditional supervised language identifier. It increases recall with almost no loss of precision, including, surprisingly, for English messages written by non-U.S. authors. Our dataset and identifier ensemble are available online.',\n",
       "  'year': 2017,\n",
       "  'venue': 'NUT@EMNLP'},\n",
       " {'paperId': 'f233b38b552d40ac82db221442331bd1df8b884c',\n",
       "  'title': 'The First Cross-Lingual Challenge on Recognition, Normalization, and Matching of Named Entities in Slavic Languages',\n",
       "  'authorId': '1678833',\n",
       "  'authorName': 'J. Piskorski',\n",
       "  'abstract': 'This paper describes the outcomes of the first challenge on multilingual named entity recognition that aimed at recognizing mentions of named entities in web documents in Slavic languages, their normalization/lemmatization, and cross-language matching. It was organised in the context of the 6th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2017 conference. Although eleven teams signed up for the evaluation, due to the complexity of the task(s) and short time available for elaborating a solution, only two teams submitted results on time. The reported evaluation figures reflect the relatively higher level of complexity of named entity-related tasks in the context of processing texts in Slavic languages. Since the duration of the challenge goes beyond the date of the publication of this paper and updated picture of the participating systems and their corresponding performance can be found on the web page of the challenge.',\n",
       "  'year': 2017,\n",
       "  'venue': 'BSNLP@EACL'},\n",
       " {'paperId': 'd82dcb65640a32791d70603ec4e4312a994fd12e',\n",
       "  'title': 'Randomized Greedy Inference for Joint Segmentation, POS Tagging and Dependency Parsing',\n",
       "  'authorId': '1703465',\n",
       "  'authorName': 'Yuan Zhang',\n",
       "  'abstract': 'In this paper, we introduce a new approach for joint segmentation, POS tagging and dependency parsing. While joint modeling of these tasks addresses the issue of error propagation inherent in traditional pipeline architectures, it also complicates the inference task. Past research has addressed this challenge by placing constraints on the scoring function. In contrast, we propose an approach that can handle arbitrarily complex scoring functions. Specifically, we employ a randomized greedy algorithm that jointly predicts segmentations, POS tags and dependency trees. Moreover, this architecture readily handles different segmentation tasks, such as morphological segmentation for Arabic and word segmentation for Chinese. The joint model outperforms the state-of-the-art systems on three datasets, obtaining 2.1% TedEval absolute gain against the best published results in the 2013 SPMRL shared task. 1',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '3bb21a197b29e2b25fe8befbe6ac5cec66d25413',\n",
       "  'title': 'Exploratory Analysis of Social Media Prior to a Suicide Attempt',\n",
       "  'authorId': '4366352',\n",
       "  'authorName': 'Glen A. Coppersmith',\n",
       "  'abstract': 'Tragically, an estimated 42,000 Americans died by suicide in 2015, each one deeply affecting friends and family. Very little data and information is available about people who attempt to take their life, and thus scientific exploration has been hampered. We examine data from Twitter users who have attempted to take their life and provide an exploratory analysis of patterns in language and emotions around their attempt. We also show differences between those who have attempted to take their life and matched controls. We find quantifiable signals of suicide attempts in the language of social media data and estimate performance of a simple machine learning classifier with these signals as a non-invasive analysis in a screening process.',\n",
       "  'year': 2016,\n",
       "  'venue': 'CLPsych@HLT-NAACL'},\n",
       " {'paperId': 'c83ead0f98d7461f58c65c8d9e413f83c6d1131a',\n",
       "  'title': 'Breaking Bad: Extraction of Verb-Particle Constructions from a Parallel Subtitles Corpus',\n",
       "  'authorId': '2109295302',\n",
       "  'authorName': 'Aaron Smith',\n",
       "  'abstract': 'The automatic extraction of verb-particle constructions (VPCs) is of particular interest to the NLP community. Previous studies have shown that word alignment methods can be used with parallel corpora to successfully extract a range of multi-word expressions (MWEs). In this paper the technique is applied to a new type of corpus, made up of a collection of subtitles of movies and television series, which is parallel in English and Spanish. Building on previous research, it is shown that a precision level of 94±4.7% can be achieved in English VPC extraction. This high level of precision is achieved despite the difficulties of aligning and tagging subtitles data. Moreover, many of the extracted VPCs are not present in online lexical resources, highlighting the benefits of using this unique corpus type, which contains a large number of slang and other informal expressions. An added benefit of using the word alignment process is that translations are also automatically extracted for each VPC. A precision rate of 75±8.5% is found for the translations of English VPCs into Spanish. This study thus shows that VPCs are a particularly good subset of the MWE spectrum to attack using word alignment methods, and that subtitles data provide a range of interesting expressions that do not exist in other corpus types.',\n",
       "  'year': 2014,\n",
       "  'venue': 'MWE@EACL'},\n",
       " {'paperId': 'c485fa1e053fe65621bb76bf0ab1789472e21427',\n",
       "  'title': 'Incremental Skip-gram Model with Negative Sampling',\n",
       "  'authorId': '145766950',\n",
       "  'authorName': 'Nobuhiro Kaji',\n",
       "  'abstract': 'This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5e63fbf6e2051b6bc279eb246ac9263f2b47bce7',\n",
       "  'title': 'Evaluating Coherence in Dialogue Systems using Entailment',\n",
       "  'authorId': '46217681',\n",
       "  'authorName': 'Nouha Dziri',\n",
       "  'abstract': 'Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'e69207dfba9cfe055bbbe5efd1834b3d6aebe223',\n",
       "  'title': 'Computational Complexity and Lexical Funtional Grammar',\n",
       "  'authorId': '3086368',\n",
       "  'authorName': 'R. Berwick',\n",
       "  'abstract': \"An important goal of modern linguistic theory is to characterize as narrowly as possible the class of natural languages. One classical approach to this characterization has been to investigate the generative capacity of grammatical systems specifiable within particular linguistic theories. Formal results along these lines have already been obtained for certain kinds of Transformational Generat ive Grammars: for example, Peters and Ritchie 1973a showed that the theory of Transformational Grammar presented in Chomsky's Aspects of the Theory of Syntax 1965 is powerful enough to allow the specification of grammars for generating any recursively enumerable language, while Rounds 1973,1975 extended this work by demonstrating that moderate ly restricted Transformat ional Grammars (TGs) can generate languages whose recognition time is provably exponential. 1\",\n",
       "  'year': 1981,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '149980675757565b109ec1f8f0fa8ccc43c00045',\n",
       "  'title': 'Cambridge at SemEval-2021 Task 2: Neural WiC-Model with Data Augmentation and Exploration of Representation',\n",
       "  'authorId': '144725847',\n",
       "  'authorName': 'Zheng Yuan',\n",
       "  'abstract': 'This paper describes the system of the Cambridge team submitted to the SemEval-2021 shared task on Multilingual and Cross-lingual Word-in-Context Disambiguation. Building on top of a pre-trained masked language model, our system is first pre-trained on out-of-domain data, and then fine-tuned on in-domain data. We demonstrate the effectiveness of the proposed two-step training strategy and the benefits of data augmentation from both existing examples and new resources. We further investigate different representations and show that the addition of distance-based features is helpful in the word-in-context disambiguation task. Our system yields highly competitive results in the cross-lingual track without training on any cross-lingual data; and achieves state-of-the-art results in the multilingual track, ranking first in two languages (Arabic and Russian) and second in French out of 171 submitted systems.',\n",
       "  'year': 2021,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': '1ce96d8dbf69199ebd043de6cfa25d7e48b8ab03',\n",
       "  'title': 'Causal Effects of Linguistic Properties',\n",
       "  'authorId': '4099006',\n",
       "  'authorName': 'Reid Pryzant',\n",
       "  'abstract': 'We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer’s intent, and establish the assumptions necessary to identify this from observational data. Second, in practice, we only have access to noisy proxies for the linguistic properties of interest—e.g., predictions from classifiers and lexicons. We propose an estimator for this setting and prove that its bias is bounded when we perform an adjustment for the text. Based on these results, we introduce TextCause, an algorithm for estimating causal effects of linguistic properties. The method leverages (1) distant supervision to improve the quality of noisy proxies, and (2) a pre-trained language model (BERT) to adjust for the text. We show that the proposed method outperforms related approaches when estimating the effect of Amazon review sentiment on semi-simulated sales figures. Finally, we present an applied case study investigating the effects of complaint politeness on bureaucratic response times.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'c1088267b10e19d565865c2dcf0bc0f94696bf2e',\n",
       "  'title': 'Learning to Understand Phrases by Embedding the Dictionary',\n",
       "  'authorId': '145783676',\n",
       "  'authorName': 'Felix Hill',\n",
       "  'abstract': 'Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: reverse dictionaries that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences.',\n",
       "  'year': 2015,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '3ca2fa08229d3aa1c82eb1f277578c8b886d8871',\n",
       "  'title': 'Itihasa: A large-scale corpus for Sanskrit to English translation',\n",
       "  'authorId': '19509693',\n",
       "  'authorName': 'Rahul Aralikatte',\n",
       "  'abstract': 'This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The shlokas are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.',\n",
       "  'year': 2021,\n",
       "  'venue': 'WAT'},\n",
       " {'paperId': 'de353dfa64e35237a3daa09a3f1e3d1b530b8233',\n",
       "  'title': 'Towards an open-source universal-dependency treebank for Erzya',\n",
       "  'authorId': '71677874',\n",
       "  'authorName': 'Jack Rueter',\n",
       "  'abstract': 'This article describes the first steps towards a open-source dependency treebank for Erzya based on universal dependency (UD) annotation standards. The treebank contains 610 sentences with 6661 tokens and is based on texts from a range of open-source and public domain original Erzya sources. This ensures its free availability and extensibility. Texts in the treebank are first morphologically analyzed and disambiguated after which they are annotated manually for dependency structure. In the article we present some issues in dependency syntax for Erzya and how they are analyzed in the universal-dependency framework. Preliminary statistics are given for dependency parsing of Erzya, along with points of interest for future research. Tiivistelmä Tässä artikkelissa kerrotaan ersän kielen avoimen puupankin ensimmäisistä askeleista, joissa sovelletaan universaaliriippuvuus-annotaatiota (UD). Puupankki sisältää 610 virkettä joissa on yhteensä 6661 tokenia ja se perustuu avoimeen ersänkieliseen originaalikirjoituksiin. Tällä tavalla varmistetaan puupankin saatavuutta ja laajennettavuutta. Puupankin tekstit on ensin analysoitu morfologisella jäsentimellä ja disambiguoitu, minkä jälkeen suoritetaan loppuyksiselitteistäminen käsin ja lisätään riippuvuussuhteet. Artikkelissa esitetään joitakin kysymyksiä, jotka esiintyvät ersän lauseoppia sovellettaessa universaaliriippuvuuskehyksiin. Annetaan alkutilastoja ersän jäsennyksestä sekä ajatuksia tulevan tutkimuksen näkemyksistä. Abstract Те статиясонть сёрмадтано эрзянь келень од ресурсадо, конась весеменень панжадо, чувтокс валрисьмень пурнавксто, чувтонь банкто, ды юртонзо путомадо. Валрисьмень анализэнь теемстэ нолдави тевс масторлангонь вейсэнь аннотация, конаньсэ невтеви валрисьме пелькстнэнь вейкест-вейкест эйстэ чувтокс аштема лувост (Universal DependencyUD). Статиянть сёрмадомсто чувтонь банкось ашти 610 валрисьмеде, косо весемезэ 6661 токент (валтлотксема тешкст), материалось ашти весеменень панжадо эрзякс сёрмадозь литературанть эйстэ. Истя чувтонь банкось саеви-келейгавтови киненьмелезэ – ресурсась ванстсы оляксчинзэ. Васня пурнавксонь валрисьметненень тееви морфологиянь анализ, конасьмейле седе вадрялгавтови синтаксисэнь анализсэ.Те статиясонть сёрмадтано эрзянь келень од ресурсадо, конась весеменень панжадо, чувтокс валрисьмень пурнавксто, чувтонь банкто, ды юртонзо путомадо. Валрисьмень анализэнь теемстэ нолдави тевс масторлангонь вейсэнь аннотация, конаньсэ невтеви валрисьме пелькстнэнь вейкест-вейкест эйстэ чувтокс аштема лувост (Universal DependencyUD). Статиянть сёрмадомсто чувтонь банкось ашти 610 валрисьмеде, косо весемезэ 6661 токент (валтлотксема тешкст), материалось ашти весеменень панжадо эрзякс сёрмадозь литературанть эйстэ. Истя чувтонь банкось саеви-келейгавтови киненьмелезэ – ресурсась ванстсы оляксчинзэ. Васня пурнавксонь валрисьметненень тееви морфологиянь анализ, конасьмейле седе вадрялгавтови синтаксисэнь анализсэ. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ Мейле келень ванкшныцясь сонсь невти кона пелькстнэ конатнень эйстэ аштить. Статиясонть макстано зярыя кевкстемат, конат чачить эрзянь кель UDмарто вастневемстэ. Макстано эрзянь келень анализдэ васнянь статистика ды арсемат-мельть келень ванкшномань сыця ёнкстнэде-тевтнеде.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': '734089edd71619a3a2b3cebfe63f97dd92b2671c',\n",
       "  'title': 'COSY: COunterfactual SYntax for Cross-Lingual Understanding',\n",
       "  'authorId': '84004976',\n",
       "  'authorName': 'S. Yu',\n",
       "  'abstract': 'Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in cross-lingual tasks, yielding the state-of-the-art performance. However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task. We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and POS tags, into language models, based on the observation that universal syntax is transferable across different languages. Our approach, called COunterfactual SYntax (COSY), includes the design of SYntax-aware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax. To evaluate COSY, we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones. Our results show that COSY achieves the state-of-the-art performance for both tasks, without using auxiliary training data.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a43618a02e8d9b9db2e34b2ab766a766bbddc85d',\n",
       "  'title': 'Evaluating Centering-Based Metrics of Coherence',\n",
       "  'authorId': '2344910',\n",
       "  'authorName': 'Nikiforos Karamanis',\n",
       "  'abstract': 'We use a reliably annotated corpus to compare metrics of coherence based on Centering Theory with respect to their potential usefulness for text structuring in natural language generation. Previous corpus-based evaluations of the coherence of text according to Centering did not compare the coherence of the chosen text structure with that of the possible alternatives. A corpus-based methodology is presented which distinguishes between Centering-based metrics taking these alternatives into account, and represents therefore a more appropriate way to evaluate Centering from a text structuring perspective.',\n",
       "  'year': 2004,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c007fcb3958d371c0869dee329cc6c08332e92d7',\n",
       "  'title': 'UMDeep at SemEval-2017 Task 1: End-to-End Shared Weight LSTM Model for Semantic Textual Similarity',\n",
       "  'authorId': '40080808',\n",
       "  'authorName': 'Joe Barrow',\n",
       "  'abstract': 'We describe a modified shared-LSTM network for the Semantic Textual Similarity (STS) task at SemEval-2017. The network builds on previously explored Siamese network architectures. We treat max sentence length as an additional hyperparameter to be tuned (beyond learning rate, regularization, and dropout). Our results demonstrate that hand-tuning max sentence training length significantly improves final accuracy. After optimizing hyperparameters, we train the network on the multilingual semantic similarity task using pre-translated sentences. We achieved a correlation of 0.4792 for all the subtasks. We achieved the fourth highest team correlation for Task 4b, which was our best relative placement.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': '4360aae9018a90a9d070deedcbc0a5524fe84d52',\n",
       "  'title': 'Incremental Learning from Scratch for Task-Oriented Dialogue Systems',\n",
       "  'authorId': '1500387293',\n",
       "  'authorName': 'Weikang Wang',\n",
       "  'abstract': 'Clarifying user needs is essential for existing task-oriented dialogue systems. However, in real-world applications, developers can never guarantee that all possible user demands are taken into account in the design phase. Consequently, existing systems will break down when encountering unconsidered user needs. To address this problem, we propose a novel incremental learning framework to design task-oriented dialogue systems, or for short Incremental Dialogue System (IDS), without pre-defining the exhaustive list of user needs. Specifically, we introduce an uncertainty estimation module to evaluate the confidence of giving correct responses. If there is high confidence, IDS will provide responses to users. Otherwise, humans will be involved in the dialogue process, and IDS can learn from human intervention through an online learning module. To evaluate our method, we propose a new dataset which simulates unanticipated user needs in the deployment stage. Experiments show that IDS is robust to unconsidered user actions, and can update itself online by smartly selecting only the most effective training data, and hence attains better performance with less annotation cost.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '9fbca6164b8f94011cf1a1c3c10c23e2ba71a55b',\n",
       "  'title': 'TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning',\n",
       "  'authorId': '50087162',\n",
       "  'authorName': 'Yixuan Su',\n",
       "  'abstract': 'Masked language models (MLMs) such as BERT have revolutionized the ﬁeld of Natural Language Understanding in the past few years. However, existing pre-trained MLMs often output an anisotropic distribution of token representations that occupies a narrow subset of the entire representation space. Such token representations are not ideal, especially for tasks that demand discriminative semantic meanings of distinct tokens. In this work, we propose TaCL ( T oken- a ware C ontrastive L earning), a novel continual pre-training approach that encourages BERT to learn an isotropic and discriminative distribution of token representations. TaCL is fully unsupervised and requires no additional data. We extensively test our approach on a wide range of English and Chinese benchmarks. The results show that TaCL brings consistent and notable improvements over the original BERT model. Furthermore, we conduct detailed analysis to reveal the merits and inner-workings of our approach. 1',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL-HLT'},\n",
       " {'paperId': '6b1ae144bd17a1b60002abd52b19ac9234620ff2',\n",
       "  'title': 'Classification of telicity using cross-linguistic annotation projection',\n",
       "  'authorId': '33985877',\n",
       "  'authorName': 'Annemarie Friedrich',\n",
       "  'abstract': 'This paper addresses the automatic recognition of telicity, an aspectual notion. A telic event includes a natural endpoint (“she walked home”), while an atelic event does not (“she walked around”). Recognizing this difference is a prerequisite for temporal natural language understanding. In English, this classification task is difficult, as telicity is a covert linguistic category. In contrast, in Slavic languages, aspect is part of a verb’s meaning and even available in machine-readable dictionaries. Our contributions are as follows. We successfully leverage additional silver standard training data in the form of projected annotations from parallel English-Czech data as well as context information, improving automatic telicity classification for English significantly compared to previous work. We also create a new data set of English texts manually annotated with telicity.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '4690190d6c110f7525f7250e1acf4a4eab42519f',\n",
       "  'title': 'Certified Robustness to Adversarial Word Substitutions',\n",
       "  'authorId': '3422908',\n",
       "  'authorName': 'Robin Jia',\n",
       "  'abstract': 'State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models’ robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75% adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI; in comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 12% and 41%, respectively.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'd3f47edf4d894328ea7712e97fc21e7cc392fdc7',\n",
       "  'title': 'A Parser for Real-Time Speech Synthesis of Conversational Texts',\n",
       "  'authorId': '1720717',\n",
       "  'authorName': 'J. Bachenko',\n",
       "  'abstract': \"In this paper, we concern ourselves with an application of text-to-speech for speech-impaired, deaf, and hard of hearing people. The application is unusual because it requires real-time synthesis of unedited, spontaneously generated conversational texts transmitted via a Telecommunications Device for the Deaf (TDD). We describe a parser that we have implemented as a front end for a version of the Bell Laboratories text-to-speech synthesizer (Olive and Liberman 1985). The parser prepares TDD texts for synthesis by (a) performing lexical regularization of abbreviations and some non-standard forms, and (b) identifying prosodic phrase boundaries. Rules for identifying phrase boundaries are derived from the prosodic phrase grammar described in Bachenko and Fitzpatrick (1990). Following the parent analysis, these rules use a mix of syntactic and phonological factors to identify phrase boundaries but, unlike the parent system, they forgo building any hierarchical structure in order to bypass the need for a stacking mechanism; this permits the system to operate in near real time. As a component of the text-to-speech system, the parser has undergone rigorous testing during a successful three-month field trial at an AT&T telecommunications center in California. In addition, laboratory evaluations indicate that the parser's performance compares favorably with human judgments about phrasing.\",\n",
       "  'year': 1992,\n",
       "  'venue': 'ANLP'},\n",
       " {'paperId': '8559f2b6e61a07da52ce4dfc33a4b29e77a06e5f',\n",
       "  'title': 'Part-of-Speech Tagging Using a Variable Memory Markov Model',\n",
       "  'authorId': '144418438',\n",
       "  'authorName': 'Hinrich Schütze',\n",
       "  'abstract': 'We present a new approach to disambiguating syntactically ambiguous words in context, based on Variable Memory Markov (VMM) models. In contrast to fixed-length Markov models, which predict based on fixed-lenth histories, variable memory Markov models dynamically adapt their history length based on the training data, and hence may use fewer parameters. In a test of a VMM based tagger on the Brown corpus, 95.81% of tokens are correctly classified.',\n",
       "  'year': 1994,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'b19422464430c3c1177de4cba4613c01583e82c9',\n",
       "  'title': 'Information Navigation System Based on POMDP that Tracks User Focus',\n",
       "  'authorId': '2237192',\n",
       "  'authorName': 'Koichiro Yoshino',\n",
       "  'abstract': 'We present a spoken dialogue system for navigating information (such as news articles), and which can engage in small talk. At the core is a partially observable Markov decision process (POMDP), which tracks user’s state and focus of attention. The input to the POMDP is provided by a spoken language understanding (SLU) component implemented with logistic regression (LR) and conditional random fields (CRFs). The POMDP selects one of six action classes; each action class is implemented with its own module.',\n",
       "  'year': 2014,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '886467f05dbf3e18aecd7e994226b1ecd7fb27f1',\n",
       "  'title': 'Scalable Evaluation and Improvement of Document Set Expansion via Neural Positive-Unlabeled Learning',\n",
       "  'authorId': '41016275',\n",
       "  'authorName': 'Alon Jacovi',\n",
       "  'abstract': 'We consider the situation in which a user has collected a small set of documents on a cohesive topic, and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query, and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning—i.e., learning binary classifiers from only positive (the query documents) and unlabeled (the results of the IR engine) data. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting, showing that the standard implementations of state-of-the-art PU solutions fail. We propose solutions for each of the challenges and empirically validate them with ablation tests. We demonstrate the effectiveness of the new method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics, showing improvements over the common IR solution and other baselines.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '593e4e749bd2dbcaf8dc25298d830b41d435e435',\n",
       "  'title': 'A Minimal Span-Based Neural Constituency Parser',\n",
       "  'authorId': '144872294',\n",
       "  'authorName': 'Mitchell Stern',\n",
       "  'abstract': 'In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '8cef9900c04d7f661c08f4b5b1ed4337ace042a3',\n",
       "  'title': 'Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel',\n",
       "  'authorId': '145639633',\n",
       "  'authorName': 'Yao-Hung Hubert Tsai',\n",
       "  'abstract': 'Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer’s attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer’s attention. As an example, we propose a new variant of Transformer’s attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '373d03285a83ae5a4a4f090edc59d7bd21190974',\n",
       "  'title': 'Towards Generating Personalized Hospitalization Summaries',\n",
       "  'authorId': '1938196',\n",
       "  'authorName': 'S. Acharya',\n",
       "  'abstract': 'Most of the health documents, including patient education materials and discharge notes, are usually flooded with medical jargons and contain a lot of generic information about the health issue. In addition, patients are only provided with the doctor’s perspective of what happened to them in the hospital while the care procedure performed by nurses during their entire hospital stay is nowhere included. The main focus of this research is to generate personalized hospital-stay summaries for patients by combining information from physician discharge notes and nursing plan of care. It uses a metric to identify medical concepts that are Complex, extracts definitions for the concept from three external knowledge sources, and provides the simplest definition to the patient. It also takes various features of the patient into account, like their concerns and strengths, ability to understand basic health information, level of engagement in taking care of their health, and familiarity with the health issue and personalizes the content of the summaries accordingly. Our evaluation showed that the summaries contain 80% of the medical concepts that are considered as being important by both doctor and nurses. Three patient advisors (i.e. individuals who are trained in understanding patient experience extensively) verified the usability of our summaries and mentioned that they would like to get such summaries when they are discharged from hospital.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '79b095a8a414ef041eebab0e8cd8705d74463a3d',\n",
       "  'title': 'Instance-Based Neural Dependency Parsing',\n",
       "  'authorId': '33516663',\n",
       "  'authorName': 'Hiroki Ouchi',\n",
       "  'abstract': 'Abstract Interpretable rationales for model predictions are crucial in practical applications. We develop neural models that possess an interpretable inference process for dependency parsing. Our models adopt instance-based inference, where dependency edges are extracted and labeled by comparing them to edges in a training set. The training edges are explicitly used for the predictions; thus, it is easy to grasp the contribution of each edge to the predictions. Our experiments show that our instance-based models achieve competitive accuracy with standard neural models and have the reasonable plausibility of instance-based explanations.',\n",
       "  'year': 2021,\n",
       "  'venue': 'Transactions of the Association for Computational Linguistics'},\n",
       " {'paperId': '9bea721047df1812ae1b772f939a93ccf35567bc',\n",
       "  'title': 'Cross-narrative Temporal Ordering of Medical Events',\n",
       "  'authorId': '30088877',\n",
       "  'authorName': 'Preethi Raghavan',\n",
       "  'abstract': 'Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient’s history. We address the problem of aligning multiple medical event sequences, corresponding to different clinical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding, and (2) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms. The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives. We present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8% for the problem of multiple-sequence alignment.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '0c135580370f4c3f9fae9906ab165a4211749cc6',\n",
       "  'title': \"Devil's Advocate: Novel Boosting Ensemble Method from Psychological Findings for Text Classification\",\n",
       "  'authorId': '3437180',\n",
       "  'authorName': 'Hwiyeol Jo',\n",
       "  'abstract': 'We present a new form of ensemble method– Devil’s Advocate, which uses a deliberately dissenting model to force other submodels within the ensemble to better collaborate. Our method consists of two di ﬀ erent training settings: one follows the conventional training process (Norm), and the other is trained by artiﬁcially generated labels (DevAdv). After training the models, Norm models are ﬁne-tuned through an additional loss function, which uses the DevAdv model as a constraint. In making a ﬁnal decision, the proposed ensemble model sums the scores of Norm models and then subtracts the score of the DevAdv model. The DevAdv model improves the overall performance of the other models within the ensemble. In addition to our ensemble framework being based on psychological background, it also shows comparable or improved performance on 5 text classiﬁcation tasks when compared to conventional ensemble methods.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '1048223cfaee62ccd51c6b08ebe2ffe18ea3cc52',\n",
       "  'title': 'Evaluating hypotheses in geolocation on a very large sample of Twitter',\n",
       "  'authorId': '1754682',\n",
       "  'authorName': 'Bahar Salehi',\n",
       "  'abstract': 'Recent work in geolocation has made several hypotheses about what linguistic markers are relevant to detect where people write from. In this paper, we examine six hypotheses against a corpus consisting of all geo-tagged tweets from the US, or whose geo-tags could be inferred, in a 19% sample of Twitter history. Our experiments lend support to all six hypotheses, including that spelling variants and hashtags are strong predictors of location. We also study what kinds of common nouns are predictive of location after controlling for named entities such as dolphins or sharks',\n",
       "  'year': 2017,\n",
       "  'venue': 'NUT@EMNLP'},\n",
       " {'paperId': 'c166246d0552a5a83ca42e0d1950839ec39b06ac',\n",
       "  'title': 'Neural Machine Translation: Hindi-Nepali',\n",
       "  'authorId': '12352363',\n",
       "  'authorName': 'Sahinur Rahman Laskar',\n",
       "  'abstract': 'With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available parallel data to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi ⇔ Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved BLEU score 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.',\n",
       "  'year': 2019,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': 'abb2490625a05c88346e1d5ec7fae663a1100e18',\n",
       "  'title': 'Metaphor Comprehension - A Special Mode of Language Processing?',\n",
       "  'authorId': '87252295',\n",
       "  'authorName': 'J. Slack',\n",
       "  'abstract': 'The paper addresses the question of whether a complete language understanding system requires special procedures in order to comprehend metaphorical language. To answer this question it is necessary to delineate the processes involved in metaphor comprehension and to determine the uniqueness of such processes in the context of existing language understanding systems.',\n",
       "  'year': 1980,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'bdbbb740582a00dd6897073464bc7b17c6dc9cd0',\n",
       "  'title': 'Smart Selection',\n",
       "  'authorId': '1990190',\n",
       "  'authorName': 'P. Pantel',\n",
       "  'abstract': 'Natural touch interfaces, common now in devices such as tablets and smartphones, make it cumbersome for users to select text. There is a need for a new text selection paradigm that goes beyond the high acuity selection-by-mouse that we have relied on for decades. In this paper, we introduce such a paradigm, called Smart Selection, which aims to recover a user’s intended text selection from her touch input. We model the problem using an ensemble learning approach, which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph. We collect a dataset of true intended user selections and simulated user touches via a large-scale crowdsourcing task, which we release to the academic community. We show that our model effectively addresses the smart selection task and significantly outperforms various baselines and standalone linguistic analysis techniques.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e6f94081276a7a5e6aef34a080cb3d3a4b1b9c20',\n",
       "  'title': 'Rethinking Why Intermediate-Task Fine-Tuning Works',\n",
       "  'authorId': '3386996',\n",
       "  'authorName': 'Ting-Yun Chang',\n",
       "  'abstract': 'Supplementary Training on Intermediate Labeled-data Tasks (STILT) is a widely applied technique, which first fine-tunes the pretrained language models on an intermediate task before on the target task of interest. While STILT is able to further improve the performance of pretrained language models, it is still unclear why and when it works. Previous research shows that those intermediate tasks involving complex inference, such as commonsense reasoning, work especially well for RoBERTa-large. In this paper, we discover that the improvement from an intermediate task could be orthogonal to it containing reasoning or other complex skills — a simple real-fake discrimination task synthesized by GPT2 can benefit diverse target tasks. We conduct extensive experiments to study the impact of different factors on STILT. These findings suggest rethinking the role of intermediate fine-tuning in the STILT pipeline.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'e9cb09e08879047da8a39045c61324e1150ee49b',\n",
       "  'title': 'Findings of the Shared Task on Detecting Signs of Depression from Social Media',\n",
       "  'authorId': '2029660411',\n",
       "  'authorName': 'K. S.',\n",
       "  'abstract': 'Social media is considered as a platform whereusers express themselves. The rise of social me-dia as one of humanity’s most important publiccommunication platforms presents a potentialprospect for early identification and manage-ment of mental illness. Depression is one suchillness that can lead to a variety of emotionaland physical problems. It is necessary to mea-sure the level of depression from the socialmedia text to treat them and to avoid the nega-tive consequences. Detecting levels of depres-sion is a challenging task since it involves themindset of the people which can change period-ically. The aim of the DepSign-LT-EDI@ACL-2022 shared task is to classify the social me-dia text into three levels of depression namely“Not Depressed”, “Moderately Depressed”, and“Severely Depressed”. This overview presentsa description on the task, the data set, method-ologies used and an analysis on the results ofthe submissions. The models that were submit-ted as a part of the shared task had used a va-riety of technologies from traditional machinelearning algorithms to deep learning models.It could be observed from the result that thetransformer based models have outperformedthe other models. Among the 31 teams whohad submitted their results for the shared task,the best macro F1-score of 0.583 was obtainedusing transformer based model.',\n",
       "  'year': 2022,\n",
       "  'venue': 'LTEDI'},\n",
       " {'paperId': '9a9b67d46511b877c212d3ef8e09cf4b319ec68f',\n",
       "  'title': 'Combining the output of two coreference resolution systems for two source languages to improve annotation projection',\n",
       "  'authorId': '3457105',\n",
       "  'authorName': 'Yulia Grishina',\n",
       "  'abstract': 'Although parallel coreference corpora can to a high degree support the development of SMT systems, there are no large-scale parallel datasets available due to the complexity of the annotation task and the variability in annotation schemes. In this study, we exploit an annotation projection method to combine the output of two coreference resolution systems for two different source languages (English, German) in order to create an annotated corpus for a third language (Russian). We show that our technique is superior to projecting annotations from a single source language, and we provide an in-depth analysis of the projected annotations in order to assess the perspectives of our approach.',\n",
       "  'year': 2017,\n",
       "  'venue': 'DiscoMT@EMNLP'},\n",
       " {'paperId': 'd60b45abd6636147b24be9427afe87bebacb9899',\n",
       "  'title': 'MT Tuning on RED: A Dependency-Based Evaluation Metric',\n",
       "  'authorId': '1989344',\n",
       "  'authorName': 'Liangyou Li',\n",
       "  'abstract': 'In this paper, we describe our submission to WMT 2015 Tuning Task. We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA. Experiments are conducted using hierarchical phrase-based models on Czech‐English and English‐Czech tasks. Our results show that MIRA performs better than MERT in most cases. Using RED performs similarly to METEOR when tuning is performed using MIRA. We submit our system tuned by MIRA towards RED to WMT 2015. In human evaluations, we achieve the 1st rank in all 7 systems on the English‐Czech task and 6/9 on the Czech‐ English task.',\n",
       "  'year': 2015,\n",
       "  'venue': 'WMT@EMNLP'},\n",
       " {'paperId': 'a5515ce0511f8663237ceb2b765d6d76196665b9',\n",
       "  'title': 'Post-hoc Manipulations of Vector Space Models with Application to Semantic Role Labeling',\n",
       "  'authorId': '1776599',\n",
       "  'authorName': 'Jenna Kanerva',\n",
       "  'abstract': 'In this paper, we introduce several vector space manipulation methods that are applied to trained vector space models in a post-hoc fashion, and present an application of these techniques in semantic role labeling for Finnish and English. Specifically, we show that the vectors can be circularly shifted to encode syntactic information and subsequently averaged to produce representations of predicate senses and arguments. Further, we show that it is possible to effectively learn a linear transformation between the vector representations of predicates and their arguments, within the same vector space.',\n",
       "  'year': 2014,\n",
       "  'venue': 'CVSC@EACL'},\n",
       " {'paperId': '53e50a313ddfd222d958469edb6742f19458ec74',\n",
       "  'title': 'Modeling Semantic Plausibility by Injecting World Knowledge',\n",
       "  'authorId': '2116429728',\n",
       "  'authorName': 'Su Wang',\n",
       "  'abstract': 'Distributional data tells us that a man can swallow candy, but not that a man can swallow a paintball, since this is never attested. However both are physically plausible events. This paper introduces the task of semantic plausibility: recognizing plausible but possibly novel events. We present a new crowdsourced dataset of semantic plausibility judgments of single events such as man swallow paintball. Simple models based on distributional representations perform poorly on this task, despite doing well on selection preference, but injecting manually elicited knowledge about entity properties provides a substantial performance boost. Our error analysis shows that our new dataset is a great testbed for semantic plausibility models: more sophisticated knowledge representation and propagation could address many of the remaining errors.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '68784ccad56705628433d86bf1f5cd21e5e992fe',\n",
       "  'title': 'Can We Use Word Embeddings for Enhancing Guarani-Spanish Machine Translation?',\n",
       "  'authorId': '2003583640',\n",
       "  'authorName': 'Santiago Góngora',\n",
       "  'abstract': 'Machine translation for low-resource languages, such as Guarani, is a challenging task due to the lack of data. One way of tackling it is using pretrained word embeddings for model initialization. In this work we try to check if currently available data is enough to train rich embeddings for enhancing MT for Guarani and Spanish, by building a set of word embedding collections and training MT systems using them. We found that the trained vectors are strong enough to slightly improve the performance of some of the translation models and also to speed up the training convergence.',\n",
       "  'year': 2022,\n",
       "  'venue': 'COMPUTEL'},\n",
       " {'paperId': 'd5870389f0e0ca08787acdcdcb0301fe9d6413c6',\n",
       "  'title': 'Multi-Multi-View Learning: Multilingual and Multi-Representation Entity Typing',\n",
       "  'authorId': '3261470',\n",
       "  'authorName': 'Yadollah Yaghoobzadeh',\n",
       "  'abstract': 'Accurate and complete knowledge bases (KBs) are paramount in NLP. We employ mul-itiview learning for increasing the accuracy and coverage of entity type information in KBs. We rely on two metaviews: language and representation. For language, we consider high-resource and low-resource languages from Wikipedia. For representation, we consider representations based on the context distribution of the entity (i.e., on its embedding), on the entity’s name (i.e., on its surface form) and on its description in Wikipedia. The two metaviews language and representation can be freely combined: each pair of language and representation (e.g., German embedding, English description, Spanish name) is a distinct view. Our experiments on entity typing with fine-grained classes demonstrate the effectiveness of multiview learning. We release MVET, a large multiview — and, in particular, multilingual — entity typing dataset we created. Mono- and multilingual fine-grained entity typing systems can be evaluated on this dataset.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '030865ddd9d8615d5e9b96e293f02a1a62fd95f6',\n",
       "  'title': 'CDRNN: Discovering Complex Dynamics in Human Language Processing',\n",
       "  'authorId': '40372491',\n",
       "  'authorName': 'Cory Shain',\n",
       "  'abstract': 'The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain & Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time). Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study. Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '5db91e3454f1f48c8d139c8b0e1bccbda0a218ee',\n",
       "  'title': 'A Sequence-to-Sequence Approach to Dialogue State Tracking',\n",
       "  'authorId': '2047904135',\n",
       "  'authorName': 'Yue Feng',\n",
       "  'abstract': 'This paper is concerned with dialogue state tracking (DST) in a task-oriented dialogue system. Building a DST module that is highly effective is still a challenging issue, although significant progresses have been made recently. This paper proposes a new approach to dialogue state tracking, referred to as Seq2Seq-DU, which formalizes DST as a sequence-to-sequence problem. Seq2Seq-DU employs two BERT-based encoders to respectively encode the utterances in the dialogue and the descriptions of schemas, an attender to calculate attentions between the utterance embeddings and the schema embeddings, and a decoder to generate pointers to represent the current state of dialogue. Seq2Seq-DU has the following advantages. It can jointly model intents, slots, and slot values; it can leverage the rich representations of utterances and schemas based on BERT; it can effectively deal with categorical and non-categorical slots, and unseen schemas. In addition, Seq2Seq-DU can also be used in the NLU (natural language understanding) module of a dialogue system. Experimental results on benchmark datasets in different settings (SGD, MultiWOZ2.2, MultiWOZ2.1, WOZ2.0, DSTC2, M2M, SNIPS, and ATIS) show that Seq2Seq-DU outperforms the existing methods.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ed75be4ade13718cf144d347df799f08d1e58f5e',\n",
       "  'title': 'SWOW-8500: Word Association task for Intrinsic Evaluation of Word Embeddings',\n",
       "  'authorId': '37574242',\n",
       "  'authorName': 'Avijit Thawani',\n",
       "  'abstract': 'Downstream evaluation of pretrained word embeddings is expensive, more so for tasks where current state of the art models are very large architectures. Intrinsic evaluation using word similarity or analogy datasets, on the other hand, suffers from several disadvantages. We propose a novel intrinsic evaluation task employing large word association datasets (particularly the Small World of Words dataset). We observe correlations not just between performances on SWOW-8500 and previously proposed intrinsic tasks of word similarity prediction, but also with downstream tasks (eg. Text Classification and Natural Language Inference). Most importantly, we report better confidence intervals for scores on our word association task, with no fall in correlation with downstream performance.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for'},\n",
       " {'paperId': 'a87c816b9b4c5f322211994a54d6e6c9e4bb3595',\n",
       "  'title': 'A Preliminary Evaluation of the Impact of Syntactic Structure in Semantic Textual Similarity and Semantic Relatedness Tasks',\n",
       "  'authorId': '2265142',\n",
       "  'authorName': 'Ngoc Phuoc An Vo',\n",
       "  'abstract': 'The well related tasks of evaluating the Semantic Textual Similarity and Semantic Relatedness have been under a special attention in NLP community. Many different approaches have been proposed, implemented and evaluated at different levels, such as lexical similarity, word/string/POS tags overlapping, semantic modeling (LSA, LDA), etc. However, at the level of syntactic structure, it is not clear how significant it contributes to the overall accuracy. In this paper, we make a preliminary evaluation of the impact of the syntactic structure in the tasks by running and analyzing the results from several experiments regarding to how syntactic structure contributes to solving these tasks.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'e006247c9584f39593bed908827cca40b74cdf66',\n",
       "  'title': 'An Algebra for Semantic Construction in Constraint-based Grammars',\n",
       "  'authorId': '15379653',\n",
       "  'authorName': 'Ann A. Copestake',\n",
       "  'abstract': 'We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unification-based approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.',\n",
       "  'year': 2001,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '7811a164bba17816a9a07b6457eac7be13bd0dce',\n",
       "  'title': 'Type-based MCMC for Sampling Tree Fragments from Forests',\n",
       "  'authorId': '2602530',\n",
       "  'authorName': 'Xiaochang Peng',\n",
       "  'abstract': 'This paper applies type-based Markov Chain Monte Carlo (MCMC) algorithms to the problem of learning Synchronous Context-Free Grammar (SCFG) rules from a forest that represents all possible rules consistent with a fixed word alignment. While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference. We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges. These methods lead to improvements in both log likelihood and BLEU score in our experiments.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'e7e69a3116483568f932f401da8c3b998558f720',\n",
       "  'title': 'An Efficient Chart-based Algorithm for Partial-Parsing of Unrestricted Texts',\n",
       "  'authorId': '27119069',\n",
       "  'authorName': 'David D. McDonald',\n",
       "  'abstract': 'We present an efficient algorithm for chart-based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task. The parser gains algorithmic efficiency through a reduction of its search space. As each new edge is added to the chart, the algorithm checks only the topmost of the edges adjacent to it, rather than all such edges as in conventional treatments. The resulting spanning edges are insured to be the correct ones by carefully controlling the order in which edges are introduced so that every final constituent covers the longest possible span. This is facilitated through the use of phrase boundary heuristics based on the placement of function words, and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words. A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and nonterminal edges, thereby reducing the amount of ambiguity and thus the number of edges, since only edges with a valid semantic interpretation are ever introduced.',\n",
       "  'year': 1992,\n",
       "  'venue': 'ANLP'},\n",
       " {'paperId': 'e4ff665aaa611906dff14b67d1b4d6e27371c48e',\n",
       "  'title': 'A rule-based system for cross-lingual parsing of Romance languages with Universal Dependencies',\n",
       "  'authorId': '144918133',\n",
       "  'authorName': 'Marcos Garcia',\n",
       "  'abstract': 'This article describes MetaRomance, a rule-based cross-lingual parser for Romance languages submitted to CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. The system is an almost delexicalized parser which does not need training data to analyze Romance languages. It contains linguistically motivated rules based on PoS-tag patterns. The rules included in MetaRomance were developed in about 12 hours by one expert with no prior knowledge in Universal Dependencies, and can be easily extended using a transparent formalism. In this paper we compare the performance of MetaRomance with other supervised systems participating in the competition, paying special attention to the parsing of different treebanks of the same language. We also compare our system with a delexicalized parser for Romance languages, and take advantage of the harmonized annotation of Universal Dependencies to propose a language ranking based on the syntactic distance each variety has from Romance languages.',\n",
       "  'year': 2017,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': '970f7986ba14e167d76414b70bbc8c66c0244388',\n",
       "  'title': 'Slice-Aware Neural Ranking',\n",
       "  'authorId': '145579682',\n",
       "  'authorName': 'Gustavo Penha',\n",
       "  'abstract': 'Understanding when and why neural ranking models fail for an IR task via error analysis is an important part of the research cycle. Here we focus on the challenges of (i) identifying categories of difficult instances (a pair of question and response candidates) for which a neural ranker is ineffective and (ii) improving neural ranking for such instances. To address both challenges we resort to slice-based learning for which the goal is to improve effectiveness of neural models for slices (subsets) of data. We address challenge (i) by proposing different slicing functions (SFs) that select slices of the dataset—based on prior work we heuristically capture different failures of neural rankers. Then, for challenge (ii) we adapt a neural ranking model to learn slice-aware representations, i.e. the adapted model learns to represent the question and responses differently based on the model’s prediction of which slices they belong to. Our experimental results (the source code and data are available at https://github.com/Guzpenha/slice_based_learning) across three different ranking tasks and four corpora show that slice-based learning improves the effectiveness by an average of 2% over a neural ranker that is not slice-aware.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SCAI'},\n",
       " {'paperId': '46147f08468e873ff90d1d51e65493f262c7bb57',\n",
       "  'title': 'A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data',\n",
       "  'authorId': '3382568',\n",
       "  'authorName': 'Adam Trischler',\n",
       "  'abstract': 'Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\\\\it MCTest} benchmark. Partly because of its limited size, prior work on {\\\\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\\\\it MCTest}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15\\\\% absolute).',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c1bf19dcbdda44149369f0c9e4d2b3c4bccaa2f3',\n",
       "  'title': 'Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning',\n",
       "  'authorId': '48025720',\n",
       "  'authorName': 'Ernie Chang',\n",
       "  'abstract': 'Recent advancements in data-to-text generation largely take on the form of neural end-to-end systems. Efforts have been dedicated to improving text generation systems by changing the order of training samples in a process known as curriculum learning. Past research on sequence-to-sequence learning showed that curriculum learning helps to improve both the performance and convergence speed. In this work, we delve into the same idea surrounding the training samples consisting of structured data and text pairs, where at each update, the curriculum framework selects training samples based on the model’s competence. Specifically, we experiment with various difficulty metrics and put forward a soft edit distance metric for ranking training samples. On our benchmarks, it shows faster convergence speed where training time is reduced by 38.7% and performance is boosted by 4.84 BLEU.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '21dabd3a989d578a32fb024732a2107e8da28581',\n",
       "  'title': 'Improving Unsupervised Extractive Summarization with Facet-Aware Modeling',\n",
       "  'authorId': '120436437',\n",
       "  'authorName': 'Xinnian Liang',\n",
       "  'abstract': 'Unsupervised extractive summarization aims to extract salient sentences from documents without labeled corpus. Existing methods are mostly graph-based by computing sentence centrality. These methods usually tend to select sentences within the same facet, however, which often leads to the facet bias problem especially when the document has multiple facets (i.e. long-document and multidocuments). To address this problem, we proposed a novel facet-aware centrality-based ranking model. We let the model pay more attention to different facets by introducing a sentence-document weight. The weight is added to the sentence centrality score. We evaluate our method on a wide range of summarization tasks that include 8 representative benchmark datasets. Experimental results show that our method consistently outperforms strong baselines especially in longand multi-document scenarios and even performs comparably to some supervised models. Extensive analyses confirm that the performance gains come from alleviating the facet bias problem.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '3316bc75e1d46e58008e89f109da1fb2f6b6efb4',\n",
       "  'title': 'Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders',\n",
       "  'authorId': '46331602',\n",
       "  'authorName': 'Irene Li',\n",
       "  'abstract': 'Learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains. For example, one may be an expert in the natural language processing (NLP) domain, but want to determine the best order in which to learn new concepts in an unfamiliar Computer Vision domain (CV). Both domains share some common concepts, such as machine learning basics and deep learning models. In this paper, we solve the task of unsupervised cross-domain concept prerequisite chain learning, using an optimized variational graph autoencoder. Our model learns to transfer concept prerequisite relations from an information-rich domain (source domain) to an information-poor domain (target domain), substantially surpassing other baseline models. In addition, we expand an existing dataset by introducing two new domains—-CV and Bioinformatics (BIO). The annotated data and resources as well as the code will be made publicly available.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '176f31e2bb261d1c2c2b72d6d16008e22379b5ba',\n",
       "  'title': 'Monte Carlo Syntax Marginals for Exploring and Using Dependency Parses',\n",
       "  'authorId': '145137850',\n",
       "  'authorName': 'Katherine A. Keith',\n",
       "  'abstract': 'Dependency parsing research, which has made significant gains in recent years, typically focuses on improving the accuracy of single-tree predictions. However, ambiguity is inherent to natural language syntax, and communicating such ambiguity is important for error analysis and better-informed downstream applications. In this work, we propose a transition sampling algorithm to sample from the full joint distribution of parse trees defined by a transition-based parsing model, and demonstrate the use of the samples in probabilistic dependency analysis. First, we define the new task of dependency path prediction, inferring syntactic substructures over part of a sentence, and provide the first analysis of performance on this task. Second, we demonstrate the usefulness of our Monte Carlo syntax marginal method for parser error analysis and calibration. Finally, we use this method to propagate parse uncertainty to two downstream information extraction applications: identifying persons killed by police and semantic role assignment.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '7af286a7696080dc913b035c6ff533529a872173',\n",
       "  'title': 'A Dictionary Data Processing Environment and Its Application in Algorithmic Processing of Pali Dictionary Data for Future NLP Tasks',\n",
       "  'authorId': '2668681',\n",
       "  'authorName': 'J. Knauth',\n",
       "  'abstract': 'This paper presents a highly flexible infrastructur e for processing digitized dictionaries and that can be used to build NLP tools in the future. This infrastructure is especially suitable for low resource languages where some digitized information is available but not (yet) suitable for algorithmic use. It allows researchers to do at least some processing in an algorithmic way using the full power of the C# programming language, reducing the effort of manual editing of the data. To test this in practice, the paper de scribes the processing steps taken by making use of this infrastructure in order to identify wor d classes and cross references in the dictionary of Pali in the context of the SeNeReKo p roject. We also conduct an experiment to make use of this data and show the importance of th e dictionary. This paper presents the experiences and results of the selected approach.',\n",
       "  'year': 2014,\n",
       "  'venue': 'WSSANLP@COLING'},\n",
       " {'paperId': 'a388a8af4cf6ae2c93a69cef31781dd5f2eb4577',\n",
       "  'title': 'OMAM at SemEval-2017 Task 4: Evaluation of English State-of-the-Art Sentiment Analysis Models for Arabic and a New Topic-based Model',\n",
       "  'authorId': '3490018',\n",
       "  'authorName': 'R. Baly',\n",
       "  'abstract': 'While sentiment analysis in English has achieved significant progress, it remains a challenging task in Arabic given the rich morphology of the language. It becomes more challenging when applied to Twitter data that comes with additional sources of noise including dialects, misspellings, grammatical mistakes, code switching and the use of non-textual objects to express sentiments. This paper describes the “OMAM” systems that we developed as part of SemEval-2017 task 4. We evaluate English state-of-the-art methods on Arabic tweets for subtask A. As for the remaining subtasks, we introduce a topic-based approach that accounts for topic specificities by predicting topics or domains of upcoming tweets, and then using this information to predict their sentiment. Results indicate that applying the English state-of-the-art method to Arabic has achieved solid results without significant enhancements. Furthermore, the topic-based method ranked 1st in subtasks C and E, and 2nd in subtask D.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': '40448ec376e3bc6f706f51bfc30a4a4cc0e7b43b',\n",
       "  'title': 'BERT-based Lexical Substitution',\n",
       "  'authorId': '150341221',\n",
       "  'authorName': 'Wangchunshu Zhou',\n",
       "  'abstract': 'Previous studies on lexical substitution tend to obtain substitute candidates by finding the target word’s synonyms from lexical resources (e.g., WordNet) and then rank the candidates based on its contexts. These approaches have two limitations: (1) They are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources; (2) They fail to take into account the substitution’s influence on the global context of the sentence. To address these issues, we propose an end-to-end BERT-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources. Our approach first applies dropout to the target word’s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word’s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution’s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ef0e11df5452cbaeaaafa0e839647ca5d757e676',\n",
       "  'title': 'Integrating Order Information and Event Relation for Script Event Prediction',\n",
       "  'authorId': '116326671',\n",
       "  'authorName': 'Zhongqing Wang',\n",
       "  'abstract': 'There has been a recent line of work automatically learning scripts from unstructured texts, by modeling narrative event chains. While the dominant approach group events using event pair relations, LSTMs have been used to encode full chains of narrative events. The latter has the advantage of learning long-range temporal orders, yet the former is more adaptive to partial orders. We propose a neural model that leverages the advantages of both methods, by using LSTM hidden states as features for event pair modelling. A dynamic memory network is utilized to automatically induce weights on existing events for inferring a subsequent event. Standard evaluation shows that our method significantly outperforms both methods above, giving the best results reported so far.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '7175caf7568d46c857380d0e5b64653819d5cc45',\n",
       "  'title': 'MultiLexNorm: A Shared Task on Multilingual Lexical Normalization',\n",
       "  'authorId': '3449407',\n",
       "  'authorName': 'R. Goot',\n",
       "  'abstract': 'Lexical normalization is the task of transforming an utterance into its standardized form. This task is beneficial for downstream analysis, as it provides a way to harmonize (often spontaneous) linguistic variation. Such variation is typical for social media on which information is shared in a multitude of ways, including diverse languages and code-switching. Since the seminal work of Han and Baldwin (2011) a decade ago, lexical normalization has attracted attention in English and multiple other languages. However, there exists a lack of a common benchmark for comparison of systems across languages with a homogeneous data and evaluation setup. The MULTILEXNORM shared task sets out to fill this gap. We provide the largest publicly available multilingual lexical normalization benchmark including 12 language variants. We propose a homogenized evaluation setup with both intrinsic and extrinsic evaluation. As extrinsic evaluation, we use dependency parsing and part-ofspeech tagging with adapted evaluation metrics (a-LAS, a-UAS, and a-POS) to account for alignment discrepancies. The shared task hosted at W-NUT 2021 attracted 9 participants and 18 submissions. The results show that neural normalization systems outperform the previous state-of-the-art system by a large margin. Downstream parsing and part-of-speech tagging performance is positively affected but to varying degrees, with improvements of up to 1.72 a-LAS, 0.85 a-UAS, and 1.54 a-POS for the winning system.1',\n",
       "  'year': 2021,\n",
       "  'venue': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)'},\n",
       " {'paperId': '9e3b7e27b11b4ca0662ffc4c9cd8da6ccf23457f',\n",
       "  'title': 'Process Based Evaluation of Computer Generated Poetry',\n",
       "  'authorId': '145085806',\n",
       "  'authorName': 'Stephen McGregor',\n",
       "  'abstract': 'This paper presents and evaluates a novel system for computer generated poetry. Framed within contemporary theoretical trends in the evaluation of computational creativity, we investigate how evidence of generative process influences readers’ opinions of computer generated textual output. In addition to a technical description of our system, we present results from a study asking respondents to evaluate short computer generated poems prefaced with different types of descriptions, in some cases objectively presenting the poem as the product of a statistical analysis of corpora and in some cases subjectively presenting the computer as a self-aware agent.',\n",
       "  'year': 2016,\n",
       "  'venue': 'CC-NLG'},\n",
       " {'paperId': '93b4cc549a1bc4bc112189da36c318193d05d806',\n",
       "  'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform',\n",
       "  'authorId': '40642935',\n",
       "  'authorName': 'Matt Gardner',\n",
       "  'abstract': 'Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ArXiv'},\n",
       " {'paperId': '944cba683d10d8c1a902e05cd68e32a9f47b372e',\n",
       "  'title': 'Unsupervised Word Sense Disambiguation Rivaling Supervised Methods',\n",
       "  'authorId': '1693517',\n",
       "  'authorName': 'David Yarowsky',\n",
       "  'abstract': 'This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.',\n",
       "  'year': 1995,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1c2499f11b4d061f6a5f0a0a50504a31a7d83090',\n",
       "  'title': 'Semantic Representation for Dialogue Modeling',\n",
       "  'authorId': '6713131',\n",
       "  'authorName': 'Xuefeng Bai',\n",
       "  'abstract': 'Although neural models have achieved competitive results in dialogue systems, they have shown limited ability in representing core semantics, such as ignoring important entities. To this end, we exploit Abstract Meaning Representation (AMR) to help dialogue modeling. Compared with the textual input, AMR explicitly provides core semantic knowledge and reduces data sparsity. We develop an algorithm to construct dialogue-level AMR graphs from sentence-level AMRs and explore two ways to incorporate AMRs into dialogue systems. Experimental results on both dialogue understanding and response generation tasks show the superiority of our model. To our knowledge, we are the first to leverage a formal semantic representation into neural dialogue modeling.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '249f5538b38456db743f5b593197a15bd47e041d',\n",
       "  'title': 'Local Word Discovery for Interactive Transcription',\n",
       "  'authorId': '145588117',\n",
       "  'authorName': 'William Arbuthnot Sir Lane',\n",
       "  'abstract': 'Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for Kunwinjku, a morphologically-complex Australian language. We combine a finite state implementation of a published grammar with a partial lexicon, and apply this to a noisy phone representation of the signal. We locate known lexemes in the signal and use the morphological transducer to build these out into hypothetical, morphologically-complex words for human validation. We show that applying a single iteration of this method results in a relative transcription density gain of 17%. Further, we find that 75% of breath groups in the test set receive at least one correct partial or full-word suggestion.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '4a850e47df73f75cc049baedaf988ce1a0cf3a35',\n",
       "  'title': 'DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation',\n",
       "  'authorId': '2154942791',\n",
       "  'authorName': 'Wei Chen',\n",
       "  'abstract': 'Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses. In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses. With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental results show that our model achieves the new state-of-the-art results on all these datasets.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '2f2e7c04c979547453ffd96055dba1c484da95be',\n",
       "  'title': 'Extracting Personal Medical Events for User Timeline Construction using Minimal Supervision',\n",
       "  'authorId': '23175870',\n",
       "  'authorName': 'Aakanksha Naik',\n",
       "  'abstract': 'In this paper, we describe a system for automatic construction of user disease progression timelines from their posts in online support groups using minimal supervision. In recent years, several online support groups have been established which has led to a huge increase in the amount of patient-authored text available. Creating systems which can automatically extract important medical events and create disease progression timelines for users from such text can help in patient health monitoring as well as studying links between medical events and users’ participation in support groups. Prior work in this domain has used manually constructed keyword sets to detect medical events. In this work, our aim is to perform medical event detection using minimal supervision in order to develop a more general timeline construction system. Our system achieves an accuracy of 55.17%, which is 92% of the performance achieved by a supervised baseline system.',\n",
       "  'year': 2017,\n",
       "  'venue': 'BioNLP'},\n",
       " {'paperId': 'adc17f3e150555dcebb59bd29f4fce73176e2e12',\n",
       "  'title': 'Learning Translation Models from Monolingual Continuous Representations',\n",
       "  'authorId': '145341916',\n",
       "  'authorName': 'Kai Zhao',\n",
       "  'abstract': 'Translation models often fail to generate good translations for infrequent words or phrases. Previous work attacked this problem by inducing new translation rules from monolingual data with a semi-supervised algorithm. However, this approach does not scale very well since it is very computationally expensive to generate new translation rules for only a few thousand sentences. We propose a much faster and simpler method that directly hallucinates translation rules for infrequent phrases based on phrases with similar continuous representations for which a translation is known. To speed up the retrieval of similar phrases, we investigate approximated nearest neighbor search with redundant bit vectors which we find to be three times faster and significantly more accurate than locality sensitive hashing. Our approach of learning new translation rules improves a phrase-based baseline by up to 1.6 BLEU on Arabic-English translation, it is three-orders of magnitudes faster than existing semi-supervised methods and 0.5 BLEU more accurate.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'b509a51a3421098e5f9b78cb1b3255670e1da5fa',\n",
       "  'title': 'Visualization, Search, and Error Analysis for Coreference Annotations',\n",
       "  'authorId': '1773121',\n",
       "  'authorName': 'Markus Gärtner',\n",
       "  'abstract': 'We present the ICARUS Coreference Explorer, an interactive tool to browse and search coreference-annotated data. It can display coreference annotations as a tree, as an entity grid, or in a standard textbased display mode, and lets the user switch freely between the different modes. The tool can compare two different annotations on the same document, allowing system developers to evaluate errors in automatic system predictions. It features a flexible search engine, which enables the user to graphically construct search queries over sets of documents annotated with coreference.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '114745b95c7029c5163b745a37829d1e85fc3083',\n",
       "  'title': 'Recurrent Positional Embedding for Neural Machine Translation',\n",
       "  'authorId': '2849740',\n",
       "  'authorName': 'Kehai Chen',\n",
       "  'abstract': 'In the Transformer network architecture, positional embeddings are used to encode order dependencies into the input representation. However, this input representation only involves static order dependencies based on discrete numerical information, that is, are independent of word content. To address this issue, this work proposes a recurrent positional embedding approach based on word vector. In this approach, these recurrent positional embeddings are learned by a recurrent neural network, encoding word content-based order dependencies into the input representation. They are then integrated into the existing multi-head self-attention model as independent heads or part of each head. The experimental results revealed that the proposed approach improved translation performance over that of the state-of-the-art Transformer baseline in WMT’14 English-to-German and NIST Chinese-to-English translation tasks.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '461188735d46dc1062f5d1d382d940a24c355fad',\n",
       "  'title': 'Automatic Extraction of Commonsense LocatedNear Knowledge',\n",
       "  'authorId': '40027632',\n",
       "  'authorName': 'Frank F. Xu',\n",
       "  'abstract': 'LocatedNear relation is a kind of commonsense knowledge describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus. Also, we release two benchmark datasets for evaluation and future research.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3764baa7465201f054083d02b58fa75f883c4461',\n",
       "  'title': 'A New Statistical Parser Based on Bigram Lexical Dependencies',\n",
       "  'authorId': '143707112',\n",
       "  'authorName': 'M. Collins',\n",
       "  'abstract': 'This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.',\n",
       "  'year': 1996,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '522cf1ff2549e2865c83cb69118d9931fd6948ce',\n",
       "  'title': 'Meaning Banking beyond Events and Roles',\n",
       "  'authorId': '3461596',\n",
       "  'authorName': 'Johan Bos',\n",
       "  'abstract': 'In this talk I will discuss the analysis of several semantic phenomena that need meaning representations that can describe attributes of propositional contexts. I will do this in a version of Discourse Representation Theory, using a universal semantic tagset developed as part of a project that aims to produce a large meaning bank (a semantically-annotated corpus) for four languages (English, Dutch, German and Italian).',\n",
       "  'year': 2017,\n",
       "  'venue': ''},\n",
       " {'paperId': '981995fd64611f475179b280f4e9c241051ac185',\n",
       "  'title': 'Knowledge Inheritance for Pre-trained Language Models',\n",
       "  'authorId': '50625437',\n",
       "  'authorName': 'Yujia Qin',\n",
       "  'abstract': 'Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named “knowledge inheritance” (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs’ pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '99314a532a3358cb86064fc8917ed2c283227539',\n",
       "  'title': 'NAT: Noise-Aware Training for Robust Neural Sequence Labeling',\n",
       "  'authorId': '134442417',\n",
       "  'authorName': 'Marcin Namysl',\n",
       "  'abstract': 'Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs—as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1b60e235fb995d97a25f940f396a484b0ac7b8da',\n",
       "  'title': 'Joint Summarization-Entailment Optimization for Consumer Health Question Understanding',\n",
       "  'authorId': '46180513',\n",
       "  'authorName': 'Khalil Mrini',\n",
       "  'abstract': 'Understanding the intent of medical questions asked by patients, or Consumer Health Questions, is an essential skill for medical Conversational AI systems. We propose a novel data-augmented and simple joint learning approach combining question summarization and Recognizing Question Entailment (RQE) in the medical domain. Our data augmentation approach enables to use just one dataset for joint learning. We show improvements on both tasks across four biomedical datasets in accuracy (+8%), ROUGE-1 (+2.5%) and human evaluation scores. Human evaluation shows joint learning generates faithful and informative summaries. Finally, we release our code, the two question summarization datasets extracted from a large-scale medical dialogue dataset, as well as our augmented datasets.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NLPMC'},\n",
       " {'paperId': '197410510312a9db323c291dbdde973fe14b9742',\n",
       "  'title': 'Speech Act Modeling of Written Asynchronous Conversations with Task-Specific Embeddings and Conditional Structured Models',\n",
       "  'authorId': '2708940',\n",
       "  'authorName': 'Shafiq R. Joty',\n",
       "  'abstract': 'This paper addresses the problem of speech act recognition in written asynchronous conversations (e.g., fora, emails). We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences. Our models use sentence representations encoded by a long short term memory (LSTM) recurrent neural model. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTMs provide better task-specific representations, and (ii) the global joint model improves over local models.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '4fa1a3d26910458131536913e3f6db2cbeb79e4b',\n",
       "  'title': 'Training Strategies for Neural Multilingual Morphological Inflection',\n",
       "  'authorId': '20493598',\n",
       "  'authorName': 'Adam Ek',\n",
       "  'abstract': 'This paper presents the submission of team GUCLASP to SIGMORPHON 2021 Shared Task on Generalization in Morphological Inflection Generation. We develop a multilingual model for Morphological Inflection and primarily focus on improving the model by using various training strategies to improve accuracy and generalization across languages.',\n",
       "  'year': 2021,\n",
       "  'venue': 'Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology'},\n",
       " {'paperId': 'ded9df86cd38a939e120f88b3fa3ec9e71bd3fc5',\n",
       "  'title': 'DUCS at SemEval-2022 Task 6: Exploring Emojis and Sentiments for Sarcasm Detection',\n",
       "  'authorId': '123552507',\n",
       "  'authorName': 'Vandita Grover',\n",
       "  'abstract': \"This paper describes the participation of team DUCS at SemEval 2022 Task 6: iSarcasmEval - Intended Sarcasm Detection in English and Arabic. Team DUCS participated in SubTask A of iSarcasmEval which was to determine if the given English text was sarcastic or not. In this work, emojis were utilized to capture how they contributed to the sarcastic nature of a text. It is observed that emojis can augment or reverse the polarity of a given statement. Thus sentiment polarities and intensities of emojis, as well as those of text, were computed to determine sarcasm. Use of capitalization, word repetition, and use of punctuation marks like '!' were factored in as sentiment intensifiers. An NLP augmenter was used to tackle the imbalanced nature of the sarcasm dataset. Several architectures comprising of various ML and DL classifiers, and transformer models like BERT and Multimodal BERT were experimented with. It was observed that Multimodal BERT outperformed other architectures tested and achieved an F1-score of 30.71%. The key takeaway of this study was that sarcastic texts are usually positive sentences. In general emojis with positive polarity are used more than those with negative polarities in sarcastic texts.\",\n",
       "  'year': 2022,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': 'dbb05b7ea8f48e561e0a79102f8168ecfb842892',\n",
       "  'title': 'Portable Natural Language Generation using SPOKESMAN',\n",
       "  'authorId': '3227843',\n",
       "  'authorName': 'M. Meteer',\n",
       "  'abstract': 'This paper reports on the SPOKESMAN natural language generation system, which is a domain independent text generator designed to incrementally produce text for an underlying application program. This work is a direct outgrowth of the work we reported on at the last ACL Applied Conference in 1988, where we connected an application program directly to the linguistic component, Mumble-86. The major addition has been a new component to the system, a text planner that provides the capability to compose the utterance incrementally. The central feature of the text planning component is a new level of representation that both captures more linguistic generalizations and makes the system more portable, so that we can easily interface to different domains and different kinds of application programs. This larger system is called \"Spokesman\", as it acts as the mouthpiece for a number of application programs.',\n",
       "  'year': 1992,\n",
       "  'venue': 'ANLP'},\n",
       " {'paperId': '0a1a1ce23795f91b8129e4baf2bf93cbd65c4e24',\n",
       "  'title': 'Temporal Generalization for Spoken Language Understanding',\n",
       "  'authorId': '3025472',\n",
       "  'authorName': 'Judith Gaspers',\n",
       "  'abstract': 'Spoken Language Understanding (SLU) models in industry applications are usually trained offline on historic data, but have to perform well on incoming user requests after deployment. Since the application data is not available at training time, this is formally similar to the domain generalization problem, where domains correspond to different temporal segments of the data, and the goal is to build a model that performs well on unseen domains, e.g., upcoming data. In this paper, we explore different strategies for achieving good temporal generalization, including instance weighting, temporal fine-tuning, learning temporal features and building a temporally-invariant model. Our results on data of large-scale SLU systems show that temporal information can be leveraged to improve temporal generalization for SLU models.',\n",
       "  'year': 2022,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '5e9c85235210b59a16bdd84b444a904ae271f7e7',\n",
       "  'title': 'On Measuring Social Biases in Sentence Encoders',\n",
       "  'authorId': '98868399',\n",
       "  'authorName': 'Chandler May',\n",
       "  'abstract': 'The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test’s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '919e94379f2191e8c24903278299565c97105779',\n",
       "  'title': 'Combining Local and Document-Level Context: The LMU Munich Neural Machine Translation System at WMT19',\n",
       "  'authorId': '2982191',\n",
       "  'authorName': 'Dario Stojanovski',\n",
       "  'abstract': 'We describe LMU Munich’s machine translation system for English→German translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this model by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.',\n",
       "  'year': 2019,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '43420e2df152a58db57de8368c7a7ac2911019be',\n",
       "  'title': 'Less Is More: Attention Supervision with Counterfactuals for Text Classification',\n",
       "  'authorId': '5841595',\n",
       "  'authorName': 'Seungtaek Choi',\n",
       "  'abstract': 'We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including sentiment analysis and news categorization.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '052aff8a24b6c0e515d9299ebf0f5594cbb91e01',\n",
       "  'title': \"What's Yours and What's Mine: Determining Intellectual Attribution in Scientific Text\",\n",
       "  'authorId': '2480901',\n",
       "  'authorName': 'Simone Teufel',\n",
       "  'abstract': 'We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves.We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text.',\n",
       "  'year': 2000,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '72fe521bd4eff2d33ddf76302ccca7745ad19e57',\n",
       "  'title': 'A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network',\n",
       "  'authorId': '2115802253',\n",
       "  'authorName': 'Chenxi Zhu',\n",
       "  'abstract': 'In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '640d0e792e2934b0d6bcd9b01c45e8ac7b8c262e',\n",
       "  'title': 'ASOBEK at SemEval-2016 Task 1: Sentence Representation with Character N-gram Embeddings for Semantic Textual Similarity',\n",
       "  'authorId': '3467384',\n",
       "  'authorName': 'Asli Eyecioglu',\n",
       "  'abstract': 'A growing body of research has recently been conducted on semantic textual similarity using a variety of neural network models. While re- cent research focuses on word-based represen- tation for phrases, sentences and even paragraphs, this study considers an alternative approach based on character n-grams. We generate embeddings for character n-grams using a continuous-bag-of-n-grams neural network model. Three different sentence rep- resentations based on n-gram embeddings are considered. Results are reported for experi- ments with bigram, trigram and 4-gram em- beddings on the STS Core dataset for SemEval-2016 Task 1.',\n",
       "  'year': 2016,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'f71792d07fe172780cae07c1c0edc308014c612b',\n",
       "  'title': 'TeamX: A Sentiment Analyzer with Enhanced Lexicon Mapping and Weighting Scheme for Unbalanced Data',\n",
       "  'authorId': '2965600',\n",
       "  'authorName': 'Yasuhide Miura',\n",
       "  'abstract': 'This paper describes the system that has been used by TeamX in SemEval-2014 Task 9 Subtask B. The system is a sentiment analyzer based on a supervised text categorization approach designed with following two concepts. Firstly, since lexicon features were shown to be effective in SemEval-2013 Task 2, various lexicons and pre-processors for them are introduced to enhance lexical information. Secondly, since a distribution of sentiment on tweets is known to be unbalanced, an weighting scheme is introduced to bias an output of a machine learner. For the test run, the system was tuned towards Twitter texts and successfully achieved high scoring results on Twitter data, average F1 70.96 on Twitter2014 and average F1 56.50 on Twitter2014Sarcasm.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '7362fce19653c8649fe66b4826309b6302a6b841',\n",
       "  'title': 'Global Transition-based Non-projective Dependency Parsing',\n",
       "  'authorId': '2450508',\n",
       "  'authorName': 'Carlos Gómez-Rodríguez',\n",
       "  'abstract': 'Shi, Huang, and Lee (2017a) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features. However, their results were limited to projective parsing. In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH₄ algorithm, an O(n^4) mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks. To make MH₄ compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e938c9fadb33447327998d3323b12f0ac4450743',\n",
       "  'title': 'Piecewise Latent Variables for Neural Variational Text Processing',\n",
       "  'authorId': '35224828',\n",
       "  'authorName': 'Iulian Serban',\n",
       "  'abstract': 'Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '298288e03674701adb4b3514a4dad6495ef046fe',\n",
       "  'title': 'Inflection Generation as Discriminative String Transduction',\n",
       "  'authorId': '40156252',\n",
       "  'authorName': 'Garrett Nicolai',\n",
       "  'abstract': 'We approach the task of morphological inflection generation as discriminative string transduction. Our supervised system learns to generate word-forms from lemmas accompanied by morphological tags, and refines them by referring to the other forms within a paradigm. Results of experiments on six diverse languages with varying amounts of training data demonstrate that our approach improves the state of the art in terms of predicting inflected word-forms.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'bab96172f2dea7410b50f49b5583da5ebd9d15f2',\n",
       "  'title': 'DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs',\n",
       "  'authorId': '2453967',\n",
       "  'authorName': 'Lasha Abzianidze',\n",
       "  'abstract': 'Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks.',\n",
       "  'year': 2020,\n",
       "  'venue': 'CONLL'},\n",
       " {'paperId': '9a5938af01b4d83793b6743c3f7560b36c6a212f',\n",
       "  'title': 'On Losses for Modern Language Models',\n",
       "  'authorId': '1413111141',\n",
       "  'authorName': 'Stephane T Aroca-Ouellette',\n",
       "  'abstract': \"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks -- sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant -- that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERT Base on the GLUE benchmark using fewer than a quarter of the training tokens.\",\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '182f00ec5334a45f97b7b48279c5652b14ebfc96',\n",
       "  'title': 'Tw-StAR at SemEval-2018 Task 1: Preprocessing Impact on Multi-label Emotion Classification',\n",
       "  'authorId': '22192148',\n",
       "  'authorName': 'Hala Mulki',\n",
       "  'abstract': 'In this paper, we describe our contribution in SemEval-2018 contest. We tackled task 1 “Affect in Tweets”, subtask E-c “Detecting Emotions (multi-label classification)”. A multilabel classification system Tw-StAR was developed to recognize the emotions embedded in Arabic, English and Spanish tweets. To handle the multi-label classification problem via traditional classifiers, we employed the binary relevance transformation strategy while a TF-IDF scheme was used to generate the tweets’ features. We investigated using single and combinations of several preprocessing tasks to further improve the performance. The results showed that specific combinations of preprocessing tasks could significantly improve the evaluation measures. This has been later emphasized by the official results as our system ranked 3rd for both Arabic and Spanish datasets and 14th for the English dataset.',\n",
       "  'year': 2018,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'd34e6c84119cef8eb9149a27d6b4903131407ea6',\n",
       "  'title': 'How Large Are Lions? Inducing Distributions over Quantitative Attributes',\n",
       "  'authorId': '51131518',\n",
       "  'authorName': 'Yanai Elazar',\n",
       "  'abstract': 'Most current NLP systems have little knowledge about quantitative attributes of objects and events. We propose an unsupervised method for collecting quantitative information from large amounts of web data, and use it to create a new, very large resource consisting of distributions over physical quantities associated with objects, adjectives, and verbs which we call Distributions over Quantitative (DoQ). This contrasts with recent work in this area which has focused on making only relative comparisons such as “Is a lion bigger than a wolf?”. Our evaluation shows that DoQ compares favorably with state of the art results on existing datasets for relative comparisons of nouns and adjectives, and on a new dataset we introduce.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '6eebb6be0087720032626b45896633be48392636',\n",
       "  'title': 'Unsupervised Word Alignment Using Frequency Constraint in Posterior Regularized EM',\n",
       "  'authorId': '2300756',\n",
       "  'authorName': 'Hidetaka Kamigaito',\n",
       "  'abstract': 'Generative word alignment models, such as IBM Models, are restricted to oneto-many alignment, and cannot explicitly represent many-to-many relationships in a bilingual text. The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other. In this paper, we focus on the posterior regularization framework (Ganchev et al., 2010) that can force two directional word alignment models to agree with each other during training, and propose new constraints that can take into account the difference between function words and content words. Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline. We also observed gains in Japanese-toEnglish translation tasks, which prove the effectiveness of our methods under grammatically different language pairs.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '471b2a411f498501ff74eea0959b6f3b76b79cf0',\n",
       "  'title': 'Generating Training Data for Semantic Role Labeling based on Label Transfer from Linked Lexical Resources',\n",
       "  'authorId': '39006085',\n",
       "  'authorName': 'Silvana Hartmann',\n",
       "  'abstract': 'We present a new approach for generating role-labeled training data using Linked Lexical Resources, i.e., integrated lexical resources that combine several resources (e.g., Word-Net, FrameNet, Wiktionary) by linking them on the sense or on the role level. Unlike resource-based supervision in relation extraction, we focus on complex linguistic annotations, more specifically FrameNet senses and roles. The automatically labeled training data (www.ukp.tu-darmstadt.de/knowledge-based-srl/) are evaluated on four corpora from different domains for the tasks of word sense disambiguation and semantic role classification. Results show that classifiers trained on our generated data equal those resulting from a standard supervised setting.',\n",
       "  'year': 2016,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '2ec308198dfa8dd4f349ce54e23345f800b28e2c',\n",
       "  'title': 'Learning to Identify Follow-Up Questions in Conversational Question Answering',\n",
       "  'authorId': '2103994667',\n",
       "  'authorName': 'Souvik Kundu',\n",
       "  'abstract': 'Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions. Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently. In this paper, we introduce a new follow-up question identification task. We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question. It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question. Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '705bc908fe5f4962e0b2251ffb2f42a91d9eec4c',\n",
       "  'title': 'Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index',\n",
       "  'authorId': '48322149',\n",
       "  'authorName': 'Roma Patel',\n",
       "  'abstract': 'The input vocabulary and the representations learned are crucial to the performance of neural NLP models. Using the full vocabulary results in less explainable and more memory intensive models, with the embedding layer often constituting the majority of model parameters. It is thus common to use a smaller vocabulary to lower memory requirements and construct more interpertable models. We propose a vocabulary selection method that views words as members of a team trying to maximize the model’s performance. We apply power indices from cooperative game theory, including the Shapley value and Banzhaf index, that measure the relative importance of individual team members in accomplishing a joint task. We approximately compute these indices to identify the most influential words. Our empirical evaluation examines multiple NLP tasks, including sentence and document classification, question answering and textual entailment. We compare to baselines that select words based on frequency, TF-IDF and regression coefficients under L1 regularization, and show that this game-theoretic vocabulary selection outperforms all baseline on a range of different tasks and datasets.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '2ca85d854da1bfc5b5887419851487e7b468e441',\n",
       "  'title': 'WiC = TSV = WSD: On the Equivalence of Three Semantic Tasks',\n",
       "  'authorId': '39918547',\n",
       "  'authorName': 'B. Hauer',\n",
       "  'abstract': 'The Word-in-Context (WiC) task has attracted considerable attention in the NLP community, as demonstrated by the popularity of the recent MCL-WiC SemEval shared task. Systems and lexical resources from word sense disambiguation (WSD) are often used for the WiC task and WiC dataset construction. In this paper, we establish the exact relationship between WiC and WSD, as well as the related task of target sense verification (TSV). Building upon a novel hypothesis on the equivalence of sense and meaning distinctions, we demonstrate through the application of tools from theoretical computer science that these three semantic classification problems can be pairwise reduced to each other, and therefore are equivalent. The results of experiments that involve systems and datasets for both WiC and WSD provide strong empirical evidence that our problem reductions work in practice.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '803e32471c7d09ea57ff4e280e96856724044eff',\n",
       "  'title': 'Bilingual Correspondence Recursive Autoencoder for Statistical Machine Translation',\n",
       "  'authorId': '34739384',\n",
       "  'authorName': 'Jinsong Su',\n",
       "  'abstract': 'Learning semantic representations and tree structures of bilingual phrases is beneficial for statistical machine translation. In this paper, we propose a new neural network model called Bilingual Correspondence Recursive Autoencoder (BCorrRAE) to model bilingual phrases in translation. We incorporate word alignments into BCorrRAE to allow it freely access bilingual constraints at different levels. BCorrRAE minimizes a joint objective on the combination of a recursive autoencoder reconstruction error, a structural alignment consistency error and a crosslingual reconstruction error so as to not only generate alignment-consistent phrase structures, but also capture different levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '3b2664a15b46e95eecb9573f21c36892037b0264',\n",
       "  'title': 'Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition',\n",
       "  'authorId': '144560698',\n",
       "  'authorName': 'Yun He',\n",
       "  'abstract': 'Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '23f40946f0be0c0428f85d7b48fac4c260e1a9cf',\n",
       "  'title': 'Few-Shot Representation Learning for Out-Of-Vocabulary Words',\n",
       "  'authorId': '3407296',\n",
       "  'authorName': 'Ziniu Hu',\n",
       "  'abstract': 'Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. In this paper, we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as embedding trained with abundant observations) based on limited contexts. Specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from K observations. Furthermore, we propose to use Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for OOV words and improves downstream tasks when the embedding is utilized.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a56ebc39b8c527774be705cccdcb5f66c7302e0c',\n",
       "  'title': 'Rational Recurrences',\n",
       "  'authorId': '49349645',\n",
       "  'authorName': 'Hao Peng',\n",
       "  'abstract': 'Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'adb83557ed5095c7c48b1082e8f49ecf0fb90adb',\n",
       "  'title': 'Lingua Custodia at WMT’19: Attempts to Control Terminology',\n",
       "  'authorId': '3456893',\n",
       "  'authorName': 'Franck Burlot',\n",
       "  'abstract': 'This paper describes Lingua Custodia’s submission to the WMT’19 news shared task for German-to-French on the topic of the EU elections. We report experiments on the adaptation of the terminology of a machine translation system to a specific topic, aimed at providing more accurate translations of specific entities like political parties and person names, given that the shared task provided no in-domain training parallel data dealing with the restricted topic. Our primary submission to the shared task uses backtranslation generated with a type of decoding allowing the insertion of constraints in the output in order to guarantee the correct translation of specific terms that are not necessarily observed in the data.',\n",
       "  'year': 2019,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': 'e9611f1b59ce2302fe047d833bcb2781ec3ca9a3',\n",
       "  'title': 'A Deep Generative Model of Vowel Formant Typology',\n",
       "  'authorId': '1750769',\n",
       "  'authorName': 'Ryan Cotterell',\n",
       "  'abstract': 'What makes some types of languages more probable than others? For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be? The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language. In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains. In contrast to previous work, we work directly with the acoustic information—the first two formant values—rather than modeling discrete sets of symbols from the international phonetic alphabet. We develop a novel generative probability model and report results on over 200 languages.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '1f04dc343808165f421ebebd1f53a73454d4a2aa',\n",
       "  'title': 'A Crowd-Annotated Spanish Corpus for Humor Analysis',\n",
       "  'authorId': '145691225',\n",
       "  'authorName': 'Santiago Castro',\n",
       "  'abstract': 'Computational Humor involves several tasks, such as humor recognition, humor generation, and humor scoring, for which it is useful to have human-curated data. In this work we present a corpus of 27,000 tweets written in Spanish and crowd-annotated by their humor value and funniness score, with about four annotations per tweet, tagged by 1,300 people over the Internet. It is equally divided between tweets coming from humorous and non-humorous accounts. The inter-annotator agreement Krippendorff’s alpha value is 0.5710. The dataset is available for general usage and can serve as a basis for humor detection and as a first step to tackle subjectivity.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SocialNLP@ACL'},\n",
       " {'paperId': '6d872a93566ccf29dbd8223a6fe3e86518b5704e',\n",
       "  'title': 'Two Birds with One Stone: Unified Model Learning for Both Recall and Ranking in News Recommendation',\n",
       "  'authorId': '2118839668',\n",
       "  'authorName': 'Chuhan Wu',\n",
       "  'abstract': 'Recall and ranking are two critical steps in personalized news recommendation. Most existing news recommender systems conduct personalized news recall and ranking separately with different models. However, maintaining multiple models leads to high computational cost and poses great challenges to meeting the online latency requirement of news recommender systems. In order to handle this problem, in this paper we propose UniRec, a unified method for recall and ranking in news recommendation. In our method, we first infer user embedding for ranking from the historical news click behaviors of a user using a user encoder model. Then we derive the user embedding for recall from the obtained user embedding for ranking by using it as the attention query to select a set of basis user embeddings which encode different general user interests and synthesize them into a user embedding for recall. The extensive experiments on benchmark dataset demonstrate that our method can improve both efficiency and effectiveness for recall and ranking in news recommendation.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '4da13261af1cc53130f813fe3aaf558f14cd2b3f',\n",
       "  'title': 'Recognizing Biographical Sections in Wikipedia',\n",
       "  'authorId': '2179409',\n",
       "  'authorName': 'Alessio Palmero Aprosio',\n",
       "  'abstract': 'Wikipedia is the largest collection of encyclopedic data ever written in the history of humanity. Thanks to its coverage and its availability in machine-readable format, it has become a primary resource for largescale research in historical and cultural studies. In this work, we focus on the subset of pages describing persons, and we investigate the task of recognizing biographical sections from them: given a person’s page, we identify the list of sections where information about her/his life is present. We model this as a sequence classification problem, and propose a supervised setting, in which the training data are acquired automatically. Besides, we show that six simple features extracted only from the section titles are very informative and yield good results well above a strong baseline.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '1f3ac86b4fefa3f1d6713d061318bdf92a456d2e',\n",
       "  'title': 'Document-Level Text Simplification: Dataset, Criteria and Baseline',\n",
       "  'authorId': '2068172988',\n",
       "  'authorName': 'Renliang Sun',\n",
       "  'abstract': 'Text simplification is a valuable technique. However, current research is limited to sentence simplification. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on Wikipedia dumps, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the dataset is reliable. Then, we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally, we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the baseline models.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '2bde9b4149d416614b2866343bb9102a63bd6ae9',\n",
       "  'title': 'Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter',\n",
       "  'authorId': '49346854',\n",
       "  'authorName': 'Qi Zhang',\n",
       "  'abstract': 'Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas. The task of automatically extracting them have received considerable attention in recent decades. Different from previous studies, which are usually focused on automatically extracting keyphrases from documents or articles, in this study, we considered the problem of automatically extracting keyphrases from tweets. Because of the length limitations of Twitter-like sites, the performances of existing methods usually drop sharply. We proposed a novel deep recurrent neural network (RNN) model to combine keywords and context information to perform this problem. To evaluate the proposed method, we also constructed a large-scale dataset collected from Twitter. The experimental results showed that the proposed method performs significantly better than previous methods.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '499556c6ed1daec62e5a57456213cf4f921460f1',\n",
       "  'title': 'An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction',\n",
       "  'authorId': '92963185',\n",
       "  'authorName': 'Stefan Larson',\n",
       "  'abstract': 'Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scope—i.e., queries that do not fall into any of the system’s supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'bff8ae9e28323d217b9ad5a7321e58f79607f557',\n",
       "  'title': 'A Multi-Axis Annotation Scheme for Event Temporal Relations',\n",
       "  'authorId': '3333257',\n",
       "  'authorName': 'Qiang Ning',\n",
       "  'abstract': 'Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '7fcb4d25625b5a25ff99332c3ba09f9adabc7927',\n",
       "  'title': 'Neural Machine Translation for English–Kazakh with Morphological Segmentation and Synthetic Data',\n",
       "  'authorId': '144514048',\n",
       "  'authorName': 'Antonio Toral',\n",
       "  'abstract': 'This paper presents the systems submitted by the University of Groningen to the English– Kazakh language pair (both translation directions) for the WMT 2019 news translation task. We explore the potential benefits of (i) morphological segmentation (both unsupervised and rule-based), given the agglutinative nature of Kazakh, (ii) data from two additional languages (Turkish and Russian), given the scarcity of English–Kazakh data and (iii) synthetic data, both for the source and for the target language. Our best submissions ranked second for Kazakh→English and third for English→Kazakh in terms of the BLEU automatic evaluation metric.',\n",
       "  'year': 2019,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '9880c8e9ce969e002cbd9e49ed5d1d830445492b',\n",
       "  'title': 'Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter',\n",
       "  'authorId': '1380557800',\n",
       "  'authorName': 'Boaz Shmueli',\n",
       "  'abstract': 'Datasets with induced emotion labels are scarce but of utmost importance for many NLP tasks. We present a new, automated method for collecting texts along with their induced reaction labels. The method exploits the online use of reaction GIFs, which capture complex affective states. We show how to augment the data with induced emotion and induced sentiment labels. We use our method to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K tweets. We provide baselines for three new tasks, including induced sentiment prediction and multilabel classification of induced emotions. Our method and dataset open new research opportunities in emotion detection and affective computing.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd0f60803987f248cfe3fd3f1e7a1fea16a6577f8',\n",
       "  'title': 'DisSim: A Discourse-Aware Syntactic Text Simplification Framework for English and German',\n",
       "  'authorId': '7193142',\n",
       "  'authorName': 'C. Niklaus',\n",
       "  'abstract': 'We introduce DisSim, a discourse-aware sentence splitting framework for English and German whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the coherence structure of the input and, hence, its interpretability for downstream tasks.',\n",
       "  'year': 2019,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': 'dd056780c52519a79b7c096528025d1719402d15',\n",
       "  'title': 'Do Speakers Produce Discourse Connectives Rationally?',\n",
       "  'authorId': '2550052',\n",
       "  'authorName': 'Frances Yung',\n",
       "  'abstract': 'A number of different discourse connectives can be used to mark the same discourse relation, but it is unclear what factors affect connective choice. One recent account is the Rational Speech Acts theory, which predicts that speakers try to maximize the informativeness of an utterance such that the listener can interpret the intended meaning correctly. Existing prior work uses referential language games to test the rational account of speakers’ production of concrete meanings, such as identification of objects within a picture. Building on the same paradigm, we design a novel Discourse Continuation Game to investigate speakers’ production of abstract discourse relations. Experimental results reveal that speakers significantly prefer a more informative connective, in line with predictions of the RSA model.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': '1d97b7aaf42c798225283987a1cd7df09c5d9b31',\n",
       "  'title': 'What Makes Reading Comprehension Questions Easier?',\n",
       "  'authorId': '2673984',\n",
       "  'authorName': 'Saku Sugawara',\n",
       "  'abstract': 'A challenge in creating a dataset for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. We then manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '1b0cf0cededba48d2fea32cdcf407906c61cf14f',\n",
       "  'title': 'ECNU at SemEval-2017 Task 4: Evaluating Effective Features on Machine Learning Methods for Twitter Message Polarity Classification',\n",
       "  'authorId': '7743242',\n",
       "  'authorName': 'Yunxiao Zhou',\n",
       "  'abstract': 'This paper reports our submission to subtask A of task 4 (Sentiment Analysis in Twitter, SAT) in SemEval 2017, i.e., Message Polarity Classification. We investigated several traditional Natural Language Processing (NLP) features, domain specific features and word embedding features together with supervised machine learning methods to address this task. Officially released results showed that our system ranked above average.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': '945058efb3bbcf071f3b7cce8d5e9dd0b6348098',\n",
       "  'title': 'Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning',\n",
       "  'authorId': '2441459',\n",
       "  'authorName': 'Xiangrong Zeng',\n",
       "  'abstract': 'The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works didn’t consider the extraction order of relational facts in a sentence. In this paper we argue that the extraction order is important in this task. To take the extraction order into consideration, we apply the reinforcement learning into a sequence-to-sequence model. The proposed model could generate relational facts freely. Widely conducted experiments on two public datasets demonstrate the efficacy of the proposed method.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6da4b231148bf26677233d1f778d08a5d26f4313',\n",
       "  'title': 'TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference',\n",
       "  'authorId': '50816334',\n",
       "  'authorName': 'Deming Ye',\n",
       "  'abstract': 'Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs’ inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'aaab0a85ca3ef96a88111f892644f39e7eb0ec47',\n",
       "  'title': 'Tilde’s Machine Translation Systems for WMT 2017',\n",
       "  'authorId': '2161780',\n",
       "  'authorName': 'Marcis Pinnis',\n",
       "  'abstract': 'The paper describes Tilde’s EnglishLatvian and Latvian-English machine translation systems for the WMT 2017 shared task in news translation. Both constrained and unconstrained systems are described. Our constrained systems were ranked as the best performing systems according to the automatic evaluation results. The paper gives details to how we pre-processed training data, the NMT system architecture that we used for training the NMT models, the SMT systems and their usage in NMT-SMT hybrid system configurations.',\n",
       "  'year': 2018,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '61c3c1d251c8d00a76c7513e06b24692d0b74284',\n",
       "  'title': 'Errudite: Scalable, Reproducible, and Testable Error Analysis',\n",
       "  'authorId': '35232494',\n",
       "  'authorName': 'Tongshuang Sherry Wu',\n",
       "  'abstract': 'Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a8b3a3e0719ceb880cd5d96626e3bac0229b5958',\n",
       "  'title': 'Phrase Table Induction Using In-Domain Monolingual Data for Domain Adaptation in Statistical Machine Translation',\n",
       "  'authorId': '35150510',\n",
       "  'authorName': 'Benjamin Marie',\n",
       "  'abstract': 'We present a new framework to induce an in-domain phrase table from in-domain monolingual data that can be used to adapt a general-domain statistical machine translation system to the targeted domain. Our method first compiles sets of phrases in source and target languages separately and generates candidate phrase pairs by taking the Cartesian product of the two phrase sets. It then computes inexpensive features for each candidate phrase pair and filters them using a supervised classifier in order to induce an in-domain phrase table. We experimented on the language pair English–French, both translation directions, in two domains and obtained consistently better results than a strong baseline system that uses an in-domain bilingual lexicon. We also conducted an error analysis that showed the induced phrase tables proposed useful translations, especially for words and phrases unseen in the parallel data used to train the general-domain baseline system.',\n",
       "  'year': 2017,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '2b7ba48f605834ad75aae11fa2197975580e660a',\n",
       "  'title': 'Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis',\n",
       "  'authorId': '33516663',\n",
       "  'authorName': 'Hiroki Ouchi',\n",
       "  'abstract': 'In transductive learning, an unlabeled test set is used for model training. Although this setting deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, wherein the texts to be processed are known in advance. However, despite its practical advantages, transductive learning is underexplored in natural language processing. Here we conduct an empirical study of transductive learning for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-of-the-art neural models in in-domain and out-of-domain settings.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'c90851f7bf0c4a54073a4155642622965e6fdb93',\n",
       "  'title': 'Cross-Pair Text Representations for Answer Sentence Selection',\n",
       "  'authorId': '143836647',\n",
       "  'authorName': 'K. Tymoshenko',\n",
       "  'abstract': 'High-level semantics tasks, e.g., paraphrasing, textual entailment or question answering, involve modeling of text pairs. Before the emergence of neural networks, this has been mostly performed using intra-pair features, which incorporate similarity scores or rewrite rules computed between the members within the same pair. In this paper, we compute scalar products between vectors representing similarity between members of different pairs, in place of simply using a single vector for each pair. This allows us to obtain a representation specific to any pair of pairs, which delivers the state of the art in answer sentence selection. Most importantly, our approach can outperform much more complex algorithms based on neural networks.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a43518c48a2e9bdf0ff95206fc88b0051221b3db',\n",
       "  'title': 'Segmentation and UR Acquisition with UR Constraints',\n",
       "  'authorId': '89742701',\n",
       "  'authorName': 'Max Nelson',\n",
       "  'abstract': 'This paper presents a model that treats segmentation and underlying representation acquisition as parallel, interacting processes. A probability distribution over mappings from underlying to surface representations is defined using a Maximum Entropy grammar which weights a set of underlying representation constraints (URCs) (Apoussidou, 2007; Pater et al., 2012). URCs are induced from observed surface strings and used to generate candidates. Structural ambiguity arising from the comparison of segmented outputs to unsegmented surface strings is handled with Expectation Maximization (Dempster et al., 1977; Jarosz, 2013). The model successfully learns a simple voicing assimilation rule and segmentation via correspondences between surface phones and input meanings. The trained grammar is also able to segment novel forms affixed with familiar morphemes.',\n",
       "  'year': 2019,\n",
       "  'venue': ''},\n",
       " {'paperId': '851ddebb2bb435fd57e728a030658c62d28d4709',\n",
       "  'title': 'Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent',\n",
       "  'authorId': '1737616',\n",
       "  'authorName': 'D. Litman',\n",
       "  'abstract': \"While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues. This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation. We compare the performance of two versions of TOOT (literal and cooperative), by having users carry out a set of tasks with each version. By using hypothesis testing methods, we show that a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT's cooperative rather than literal strategy contributes to greater performance.\",\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'bd2d006ad6a4b120c3bf024685993649cd5c01c1',\n",
       "  'title': 'Semantic Query Expansion for Arabic Information Retrieval',\n",
       "  'authorId': '39145389',\n",
       "  'authorName': 'Ashraf Y. Mahgoub',\n",
       "  'abstract': 'Traditional keyword based search is found to have some limitations. Such as word sense ambiguity, and the query intent ambiguity which can hurt the precision. Semantic search uses the contextual meaning of terms in addition to the semantic matching techniques in order to overcome these limitations. This paper introduces a query expansion approach using an ontology built from Wikipedia pages in addition to other thesaurus to improve search accuracy for Arabic language. Our approach outperformed the traditional keyword based approach in terms of both F-score and NDCG measures.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ANLP@EMNLP'},\n",
       " {'paperId': 'ee9ec0a5d989c28892661a803bc6a197d59b0162',\n",
       "  'title': 'QCMUQ$@$QALB-2015 Shared Task: Combining Character level MT and Error-tolerant Finite-State Recognition for Arabic Spelling Correction',\n",
       "  'authorId': '2063374',\n",
       "  'authorName': 'Houda Bouamor',\n",
       "  'abstract': 'We describe the CMU-Q and QCRI’s joint efforts in building a spelling correction system for Arabic in the QALB 2015 Shared Task. Our system is based on a hybrid pipeline that combines rule-based linguistic techniques with statistical methods using language modeling and machine translation, as well as an error-tolerant finite-state automata method. We trained and tested our spelling corrector using the dataset provided by the shared task organizers. Our system outperforms the baseline system and yeilds better correction quality with an F-score of 68.12 on L1test-2015 testset and 38.90 on the L2-test2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ANLP@ACL'},\n",
       " {'paperId': 'b58abb51a5dae7fb753be4535e468a6f1f07f873',\n",
       "  'title': 'Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation',\n",
       "  'authorId': '1785372925',\n",
       "  'authorName': 'Tianlu Wang',\n",
       "  'abstract': 'Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '966a38882be844dbf7e8b15478e1bdf3c75ef8a6',\n",
       "  'title': 'A Closer Look at How Fine-tuning Changes BERT',\n",
       "  'authorId': '2036642',\n",
       "  'authorName': 'Yichu Zhou',\n",
       "  'abstract': 'Given the prevalence of pre-trained contextualized representations in today’s NLP, there have been many efforts to understand what information they contain, and why they seem to be universally successful. The most common approach to use these representations involves fine-tuning them for an end task. Yet, how fine-tuning changes the underlying embedding space is less studied. In this work, we study the English BERT family and use two probing techniques to analyze how fine-tuning changes the space. We hypothesize that fine-tuning affects classification performance by increasing the distances between examples associated with different labels. We confirm this hypothesis with carefully designed experiments on five different NLP tasks. Via these experiments, we also discover an exception to the prevailing wisdom that “fine-tuning always improves performance”. Finally, by comparing the representations before and after fine-tuning, we discover that fine-tuning does not introduce arbitrary changes to representations; instead, it adjusts the representations to downstream tasks while largely preserving the original spatial structure of the data points.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '212d2715aee9fbefe140685b088b789d6c8277b0',\n",
       "  'title': 'Similarity of Semantic Relations',\n",
       "  'authorId': '1689647',\n",
       "  'authorName': 'Peter D. Turney',\n",
       "  'abstract': 'There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.',\n",
       "  'year': 2006,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': 'b8de808afbf302c663c20c6bd1628bc4c394a5e4',\n",
       "  'title': '2002: Another Score',\n",
       "  'authorId': '2246473',\n",
       "  'authorName': 'D. G. Hays',\n",
       "  'abstract': 'Twenty years is a long time to spend in prison, but it is a short time in intellectual history. In the few years Just prior to the foundation of this Association, we had come from remarkably complex but nevertheless rather superficial analysis of text as strings of characters or, perhaps, lexlcal units to programs for parsing that operated on complex grammatical symbols but according to rather simple general principles; the programs could be independent of the language. And at the moment of foundation, we had--In the words of D. R. Swanson--run up against the stone wall of semantics. No one at the time could say whether it was the wall of a prison yard or another step in the old intellectual pyramid.',\n",
       "  'year': 1982,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '410b8baa4a53c01ad1d963fba687a38d8ef5823e',\n",
       "  'title': 'FrameIt: Ontology Discovery for Noisy User-Generated Text',\n",
       "  'authorId': '3310951',\n",
       "  'authorName': 'Dan Iter',\n",
       "  'abstract': 'A common need of NLP applications is to extract structured data from text corpora in order to perform analytics or trigger an appropriate action. The ontology defining the structure is typically application dependent and in many cases it is not known a priori. We describe the FrameIt System that provides a workflow for (1) quickly discovering an ontology to model a text corpus and (2) learning an SRL model that extracts the instances of the ontology from sentences in the corpus. FrameIt exploits data that is obtained in the ontology discovery phase as weak supervision data to bootstrap the SRL model and then enables the user to refine the model with active learning. We present empirical results and qualitative analysis of the performance of FrameIt on three corpora of noisy user-generated text.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NUT@EMNLP'},\n",
       " {'paperId': '04cea2d855c5bbc5e7ddeed47a3ad8ecc54d4f5c',\n",
       "  'title': 'Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle',\n",
       "  'authorId': '2714199',\n",
       "  'authorName': 'Yikang Shen',\n",
       "  'abstract': 'Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing, and syntactic generalization tests while using fewer parameters than other models.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'dfff07f198d7f86436028787252059d83a97a503',\n",
       "  'title': 'Hitachi at SemEval-2020 Task 3: Exploring the Representation Spaces of Transformers for Human Sense Word Similarity',\n",
       "  'authorId': '1379579811',\n",
       "  'authorName': 'Terufumi Morishita',\n",
       "  'abstract': 'In this paper, we present our system for SemEval-2020 task 3, Predicting the (Graded) Effect of Context in Word Similarity. Due to the unsupervised nature of the task, we concentrated on inquiring about the similarity measures induced by different layers of different pre-trained Transformer-based language models, which can be good approximations of the human sense of word similarity. Interestingly, our experiments reveal a language-independent characteristic: the middle to upper layers of Transformer-based language models can induce good approximate similarity measures. Finally, our system was ranked 1st on the Slovenian part of Subtask1 and 2nd on the Croatian part of both Subtask1 and Subtask2.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': '1fb985c6624bede346e05f9ef16aa4505731c330',\n",
       "  'title': 'DORB: Dynamically Optimizing Multiple Rewards with Bandits',\n",
       "  'authorId': '10721120',\n",
       "  'authorName': 'Ramakanth Pasunuru',\n",
       "  'abstract': 'Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements. Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly. While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards. Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time. Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains. We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation, including on an unseen-test transfer setup. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b0216b8dcd9fcdf2801477573de93ab9a383b42f',\n",
       "  'title': 'DOCENT: Learning Self-Supervised Entity Representations from Large Document Collections',\n",
       "  'authorId': '51199981',\n",
       "  'authorName': 'Yury Zemlyanskiy',\n",
       "  'abstract': 'This paper explores learning rich self-supervised entity representations from large amounts of associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, question answering, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of context to include any available text related to an entity. This enables a new class of powerful, high-capacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision. We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities – strategies we compare experimentally on downstream tasks in the TV-Movies domain, such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results, our models match or outperform competitive baselines, sometimes with little or no fine-tuning, and are also able to scale to very large corpora. Finally, we make our datasets and pre-trained models publicly available. This includes Reviews2Movielens, mapping the ~1B word corpus of Amazon movie reviews (He and McAuley, 2016) to MovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions with natural language queries and corresponding community recommendations.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '43946aa78c80b6d7e6ffff837bdf4cff85f6a935',\n",
       "  'title': 'Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation',\n",
       "  'authorId': '3375999',\n",
       "  'authorName': 'Varun Gangal',\n",
       "  'abstract': 'Multiple different responses are often plausible for a given open domain dialog context. Prior work has shown the importance of having multiple valid reference responses for meaningful and robust automated evaluations. In such cases, common practice has been to collect more human written references. However, such collection can be expensive, time consuming, and not easily scalable. Instead, we propose a novel technique for automatically expanding a human generated reference to a set of candidate references. We fetch plausible references from knowledge sources, and adapt them so that they are more fluent in context of the dialog instance in question. More specifically, we use (1) a commonsense knowledge base to elicit a large number of plausible reactions given the dialog history (2) relevant instances retrieved from dialog corpus, using similar past as well as future contexts. We demonstrate that our automatically expanded reference sets lead to large improvements in correlations of automated metrics with human ratings of system outputs for DailyDialog dataset. 1',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'd1e6e66b181b5912c101a19d345e56a0c5c28bba',\n",
       "  'title': 'Linguistically Aware Information Retrieval: Providing Input Enrichment for Second Language Learners',\n",
       "  'authorId': '3458900',\n",
       "  'authorName': 'Maria Chinkina',\n",
       "  'abstract': 'How can second language teachers retrieve texts that are rich in terms of the grammatical constructions to be taught, but also address the content of interest to the learners? We developed an Information Retrieval system that identifies the 87 grammatical constructions spelled out in the official English language curriculum of schools in Baden-W¨ urttemberg (Germany) and reranks the search results based on the selected (de)prioritization of grammatical forms. In combination with a visualization of the characteristics of the search results, the approach effectively supports teachers in prioritizing those texts that provide the targeted forms. The approach facilitates systematic input enrichment for language learners as a complement to the established notion of input enhancement: while input enrichment aims at richly representing the selected forms and categories in a text, input enhancement targets their presentation to make them more salient and support noticing.',\n",
       "  'year': 2016,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': 'a559f85b244410f5b3b29d74ae9ed5fcffb40f01',\n",
       "  'title': 'The Covert Helps Parse the Overt',\n",
       "  'authorId': '2108145384',\n",
       "  'authorName': 'Xun Zhang',\n",
       "  'abstract': 'This paper is concerned with whether deep syntactic information can help surface parsing, with a particular focus on empty categories. We design new algorithms to produce dependency trees in which empty elements are allowed, and evaluate the impact of information about empty category on parsing overt elements. Such information is helpful to reduce the approximation error in a structured parsing model, but increases the search space for inference and accordingly the estimation error. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing.',\n",
       "  'year': 2017,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': '02aede33cd8dea1f5c16148adbba56bf0980a544',\n",
       "  'title': 'LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation',\n",
       "  'authorId': '46479974',\n",
       "  'authorName': 'Pankaj Gupta',\n",
       "  'abstract': 'Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) LISA: “How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response” (2) Example2pattern: “How the saliency patterns look like for each category in the data according to the network in decision making”. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BlackboxNLP@EMNLP'},\n",
       " {'paperId': '7e2977078a52bb20cc85ff737748e62028d3370e',\n",
       "  'title': 'Linköping: Cubic-Time Graph Parsing with a Simple Scoring Scheme',\n",
       "  'authorId': '40462390',\n",
       "  'authorName': 'Marco Kuhlmann',\n",
       "  'abstract': 'We turn the Eisner algorithm for parsing to projective dependency trees into a cubictime algorithm for parsing to a restricted class of directed graphs. To extend the algorithm into a data-driven parser, we combine it with an edge-factored feature model and online learning. We report and discuss results on the SemEval-2014 Task 8 data sets (Oepen et al., 2014).',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '1c342a10c9acc745e58bc880fd8c5f3757af6eb1',\n",
       "  'title': 'Conditioning, but on Which Distribution? Grammatical Gender in German Plural Inflection',\n",
       "  'authorId': '14310569',\n",
       "  'authorName': 'Kate McCurdy',\n",
       "  'abstract': 'Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This suggests that the neural models are not an effective cognitive model of German plural formation.',\n",
       "  'year': 2020,\n",
       "  'venue': 'CMCL'},\n",
       " {'paperId': '966f28e435e6a5315e3d7918d3cdf2269f7fa3c8',\n",
       "  'title': 'Automatic Discovery of Part-Whole Relations',\n",
       "  'authorId': '2469966',\n",
       "  'authorName': 'R. Girju',\n",
       "  'abstract': 'An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper presents a supervised, semantically intensive, domain independent approach for the automatic detection of part-whole relations in text. First an algorithm is described that identifies lexico-syntactic patterns that encode part-whole relations. A difficulty is that these patterns also encode other semantic relations, and a learning method is necessary to discriminate whether or not a pattern contains a part-whole relation. A large set of training examples have been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents. Classification rules have been generated this way for different patterns such as genitives, noun compounds, and noun phrases containing prepositional phrases to extract part-whole relations from them. The applicability of these rules has been tested on a test corpus obtaining an overall average precision of 80.95% and recall of 75.91%. The results demonstrate the importance of word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clarification rules apply to different patterns.',\n",
       "  'year': 2006,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': 'f87afd98fce8e25f149e06e36ce66bb4402d9ca9',\n",
       "  'title': 'Active learning for deep semantic parsing',\n",
       "  'authorId': '145378739',\n",
       "  'authorName': 'Long Duong',\n",
       "  'abstract': 'Semantic parsing requires training data that is expensive and slow to collect. We apply active learning to both traditional and “overnight” data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection. We propose several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '7a9790754922e27bef815f93d46ba87097103050',\n",
       "  'title': 'GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model',\n",
       "  'authorId': '22317545',\n",
       "  'authorName': 'Qile Zhu',\n",
       "  'abstract': 'Discovering the latent topics within texts has been a fundamental task for many applications. However, conventional topic models suffer different problems in different settings. The Latent Dirichlet Allocation (LDA) may not work well for short texts due to the data sparsity (i.e. the sparse word co-occurrence patterns in short documents). The Biterm Topic Model (BTM) learns topics by modeling the word-pairs named biterms in the whole corpus. This assumption is very strong when documents are long with rich topic information and do not exhibit the transitivity of biterms. In this paper, we propose a novel way called GraphBTM to represent biterms as graphs and design a Graph Convolutional Networks (GCNs) with residual connections to extract transitive features from biterms. To overcome the data sparsity of LDA and the strong assumption of BTM, we sample a fixed number of documents to form a mini-corpus as a sample. We also propose a dataset called All News extracted from 15 news publishers, in which documents are much longer than 20 Newsgroups. We present an amortized variational inference method for GraphBTM. Our method generates more coherent topics compared with previous approaches. Experiments show that the sampling strategy improves performance by a large margin.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '36b2f33d76801711784ef13fc7bd1f94a8e2967b',\n",
       "  'title': 'Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning',\n",
       "  'authorId': '1921742',\n",
       "  'authorName': 'Yifan Gao',\n",
       "  'abstract': 'Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning (SML) approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data. Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions. Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions. Code will be public to facilitate the research along this line.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '61e28ecbe968f0285d82bad39a4bb5d33e3f9aa1',\n",
       "  'title': 'Identifying Political Sentiment between Nation States with Social Media',\n",
       "  'authorId': '1729918',\n",
       "  'authorName': 'Nathanael Chambers',\n",
       "  'abstract': 'This paper describes an approach to largescale modeling of sentiment analysis for the social sciences. The goal is to model relations between nation states through social media. Many cross-disciplinary applications of NLP involve making predictions (such as predicting political elections), but this paper instead focuses on a model that is applicable to broader analysis. Do citizens express opinions in line with their home country’s formal relations? When opinions diverge over time, what is the cause and can social media serve to detect these changes? We describe several learning algorithms to study how the populace of a country discusses foreign nations on Twitter, ranging from state-of-theart contextual sentiment analysis to some required practical learners that filter irrelevant tweets. We evaluate on standard sentiment evaluations, but we also show strong correlations with two public opinion polls and current international alliance relationships. We conclude with some political science use cases.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'dd9d593d95cf5090007fabb91118cec8fb78c408',\n",
       "  'title': 'Perturbation CheckLists for Evaluating NLG Evaluation Metrics',\n",
       "  'authorId': '145338991',\n",
       "  'authorName': 'Ananya B. Sai',\n",
       "  'abstract': 'Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics. Our templates and code are available at https://iitmnlp.github.io/EvalEval/',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '355b66a65aee97822eb7404183ee72b18cb648de',\n",
       "  'title': 'Reordering Examples Helps during Priming-based Few-Shot Learning',\n",
       "  'authorId': '36212180',\n",
       "  'authorName': 'Sawan Kumar',\n",
       "  'abstract': 'The ability to learn from limited data, or fewshot learning, is a desirable and often critical requirement for NLP systems. While many existing methods do poorly at learning from a handful of examples, large pretrained language models have recently been shown to be efficient few-shot learners. One approach to few-shot learning, which does not require finetuning of model parameters, is to augment the language model’s input with priming text which is typically constructed using task specific descriptions and examples. In this work, we further explore priming-based few-shot learning, with focus on using examples as prompts. We show that presenting examples in the right order is key for generalization. We introduce PERO (Prompting with Examples in the Right Order), where we formulate few-shot learning as search over the set of permutations of the training examples. We show that PERO can learn to generalize efficiently using as few as 10 examples, in contrast to existing approaches. While the newline token is a natural choice for separating the examples in the prompt, we show that learning a new separator token can potentially provide further gains in performance. We demonstrate the effectiveness of the proposed method on the tasks of sentiment classification, natural language inference and fact retrieval. Finally, we analyze the learned prompts to reveal novel insights, including the idea that two training examples in the right order alone can provide competitive performance for sentiment classification and natural language inference.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '6edd30e48cab6428329c17c2c9669b7fa9a4c206',\n",
       "  'title': 'From Disjoint Sets to Parallel Data to Train Seq2Seq Models for Sentiment Transfer',\n",
       "  'authorId': '1755187',\n",
       "  'authorName': 'P. Cavalin',\n",
       "  'abstract': 'We present a method for creating parallel data to train Seq2Seq neural networks for sentiment transfer. Most systems for this task, which can be viewed as monolingual machine translation (MT), have relied on unsupervised methods, such as Generative Adversarial Networks (GANs)-inspired approaches, for coping with the lack of parallel corpora. Given that the literature shows that Seq2Seq methods have been consistently outperforming unsupervised methods in MT-related tasks, in this work we exploit the use of semantic similarity computation for converting non-parallel data onto a parallel corpus. That allows us to train a transformer neural network for the sentiment transfer task, and compare its performance against unsupervised approaches. With experiments conducted on two well-known public datasets, i.e. Yelp and Amazon, we demonstrate that the proposed methodology outperforms existing unsupervised methods very consistently in fluency, and presents competitive results in terms of sentiment conversion and content preservation. We believe that this works opens up an opportunity for seq2seq neural networks to be better exploited in problems for which they have not been applied owing to the lack of parallel training data.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '556983a574e322fd7d01c6a8193b3785c97a8905',\n",
       "  'title': 'A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code',\n",
       "  'authorId': '145727842',\n",
       "  'authorName': 'Nadezhda Chirkova',\n",
       "  'abstract': 'There is an emerging interest in the application of natural language processing models to source code processing tasks. One of the major problems in applying deep learning to software engineering is that source code often contains a lot of rare identifiers, resulting in huge vocabularies. We propose a simple, yet effective method, based on identifier anonymization, to handle out-of-vocabulary (OOV) identifiers. Our method can be treated as a preprocessing step and, therefore, allows for easy implementation. We show that the proposed OOV anonymization method significantly improves the performance of the Transformer in two code processing tasks: code completion and bug fixing.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'fe3bd7c7b36cc563c1bd54189b963e3244b74580',\n",
       "  'title': 'Neural Discourse Structure for Text Categorization',\n",
       "  'authorId': '40608686',\n",
       "  'authorName': 'Yangfeng Ji',\n",
       "  'abstract': 'We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '22ded5cef081ea5ec7b449d865aeb00583dae1b9',\n",
       "  'title': 'Portable, layer-wise task performance monitoring for NLP models',\n",
       "  'authorId': '2327884',\n",
       "  'authorName': 'Thomas Lippincott',\n",
       "  'abstract': 'There is a long-standing interest in understanding the internal behavior of neural networks. Deep neural architectures for natural language processing (NLP) are often accompanied by explanations for their effectiveness, from general observations (e.g. RNNs can represent unbounded dependencies in a sequence) to specific arguments about linguistic phenomena (early layers encode lexical information, deeper layers syntactic). The recent ascendancy of DNNs is fueling efforts in the NLP community to explore these claims. Previous work has tended to focus on easily-accessible representations like word or sentence embeddings, with deeper structure requiring more ad hoc methods to extract and examine. In this work, we introduce Vivisect, a toolkit that aims at a general solution for broad and fine-grained monitoring in the major DNN frameworks, with minimal change to research patterns.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BlackboxNLP@EMNLP'},\n",
       " {'paperId': '71d4e703201c98c9c9b44079600e289f234f09f2',\n",
       "  'title': 'Enhancing Authorship Attribution By Utilizing Syntax Tree Profiles',\n",
       "  'authorId': '2452336',\n",
       "  'authorName': 'Michael Tschuggnall',\n",
       "  'abstract': 'The aim of modern authorship attribution approaches is to analyze known authors and to assign authorships to previously unseen and unlabeled text documents based on various features. In this paper we present a novel feature to enhance current attribution methods by analyzing the grammar of authors. To extract the feature, a syntax tree of each sentence of a document is calculated, which is then split up into length-independent patterns using pq-grams. The mostly used pq-grams are then used to compose sample profiles of authors that are compared with the profile of the unlabeled document by utilizing various distance metrics and similarity scores. An evaluation using three different and independent data sets reveals promising results and indicate that the grammar of authors is a significant feature to enhance modern authorship attribution methods.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '7798372125ef228e672e1f3e50509c77a00e97f5',\n",
       "  'title': 'Tracking And Understanding Public Reaction During COVID-19: Saudi Arabia As A Use Case',\n",
       "  'authorId': '3456325',\n",
       "  'authorName': 'Aseel Addawood',\n",
       "  'abstract': 'The coronavirus disease of 2019 (COVID-19) has had a huge impact on economies and societies around the world. While governments are taking extreme measures to reduce the spread of the virus, people are being affected by these new measures. With restrictions like lockdowns and social distancing, it has become important to understand the emotional response of the public towards the pandemic. In this paper, we study the reaction of Saudi Arabian citizens towards the pandemic. We utilize a collection of Arabic tweets that were sent during 2020, primarily through hashtags that originated in Saudi Arabia. Our results showed that people had maintained a positive reaction towards the pandemic. This positive reaction was at its highest at the beginning of the COVID-19 crisis and started to decline as time passed. Overall, the results showed that people were highly supportive of each other through this pandemic. This research can help researchers and policymakers in understanding the emotional effect of a pandemic on societies.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NLP4COVID@EMNLP'},\n",
       " {'paperId': 'e20531a7084a6b2c2bfee7c8cf911752841e5910',\n",
       "  'title': 'Multilingual Event Linking to Wikidata',\n",
       "  'authorId': '51132476',\n",
       "  'authorName': 'Adithya Pratapa',\n",
       "  'abstract': 'We present a task of multilingual linking of events to a knowledge base. We automatically compile a large-scale dataset for this task, comprising of 1.8M mentions across 44 languages referring to over 10.9K events from Wikidata. We propose two variants of the event linking task: 1) multilingual, where event descriptions are from the same language as the mention, and 2) crosslingual, where all event descriptions are in English. On the two proposed tasks, we compare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and multilingual adaptations of the biencoder and crossencoder architectures from BLINK (Wu et al., 2020). In our experiments on the two task variants, we find both biencoder and crossencoder models significantly outperform the BM25+ baseline. Our results also indicate that the crosslingual task is in general more challenging than the multilingual task. To test the out-of-domain generalization of the proposed linking systems, we additionally create a Wikinews-based evaluation set. We present qualitative analysis highlighting various aspects captured by the proposed dataset, including the need for temporal reasoning over context and tackling diverse event descriptions across languages.',\n",
       "  'year': 2022,\n",
       "  'venue': 'MIA'},\n",
       " {'paperId': '9635e3c008f7bfa80638ade7134a8fb0ef1b37e1',\n",
       "  'title': 'You should evaluate your language model on marginal likelihood over tokenisations',\n",
       "  'authorId': '3421987',\n",
       "  'authorName': 'Kris Cao',\n",
       "  'abstract': 'Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'e68e29ea60c3803c5b425bb959a42e2376819bd4',\n",
       "  'title': 'Levenshtein Training for Word-level Quality Estimation',\n",
       "  'authorId': '35218254',\n",
       "  'authorName': 'Shuoyang Ding',\n",
       "  'abstract': 'We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human post-editing data. We also propose heuristics to construct reference labels that are compatible with subword-level finetuning and inference. Results on WMT 2020 QE shared task dataset show that our proposed method has superior data efficiency under the data-constrained setting and competitive performance under the unconstrained setting.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b08583bb2e5cc8414749b2700da952cd699bd8a2',\n",
       "  'title': 'A Straightforward Approach to Narratologically Grounded Character Identification',\n",
       "  'authorId': '51185813',\n",
       "  'authorName': 'Labiba Jahan',\n",
       "  'abstract': 'One of the most fundamental elements of narrative is character: if we are to understand a narrative, we must be able to identify the characters of that narrative. Therefore, character identification is a critical task in narrative natural language understanding. Most prior work has lacked a narratologically grounded definition of character, instead relying on simplified or implicit definitions that do not capture essential distinctions between characters and other referents in narratives. In prior work we proposed a preliminary definition of character that was based in clear narratological principles: a character is an animate entity that is important to the plot. Here we flesh out this concept, demonstrate that it can be reliably annotated (0.78 Cohen’s κ), and provide annotations of 170 narrative texts, drawn from 3 different corpora, containing 1,347 character co-reference chains and 21,999 non-character chains that include 3,937 animate chains. Furthermore, we have shown that a supervised classifier using a simple set of easily computable features can effectively identify these characters (overall F1 of 0.90). A detailed error analysis shows that character identification is first and foremost affected by co-reference quality, and further, that the shorter a chain is the harder it is to effectively identify as a character. We release our code and data for the benefit of other researchers',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': 'cd8308ab5f8cd7808311a5981ea7e29c7db57480',\n",
       "  'title': 'Which Step Do I Take First? Troubleshooting with Bayesian Models',\n",
       "  'authorId': '1767336',\n",
       "  'authorName': 'Annie Louis',\n",
       "  'abstract': 'Online discussion forums and community question-answering websites provide one of the primary avenues for online users to share information. In this paper, we propose text mining techniques which aid users navigate troubleshooting-oriented data such as questions asked on forums and their suggested solutions. We introduce Bayesian generative models of the troubleshooting data and apply them to two interrelated tasks: (a) predicting the complexity of the solutions (e.g., plugging a keyboard in the computer is easier compared to installing a special driver) and (b) presenting them in a ranked order from least to most complex. Experimental results show that our models are on par with human performance on these tasks, while outperforming baselines based on solution length or readability.',\n",
       "  'year': 2015,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': '4e210f80135c9bf257610b49cd46e9f131baba60',\n",
       "  'title': 'Asking too much? The rhetorical role of questions in political discourse',\n",
       "  'authorId': '3051097',\n",
       "  'authorName': 'Justine Zhang',\n",
       "  'abstract': 'Questions play a prominent role in social interactions, performing rhetorical functions that go beyond that of simple informational exchange. The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor. While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied. In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role. By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discourse—such as the bifurcation in questioning behavior between government and opposition parties—and reveals new insights into the effects of a legislator’s tenure and political career ambitions.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '1d32e792d54bac1f87e9857b7d55f01ed0ab4065',\n",
       "  'title': 'CAiRE in DialDoc21: Data Augmentation for Information Seeking Dialogue System',\n",
       "  'authorId': '38524906',\n",
       "  'authorName': 'Etsuko Ishii',\n",
       "  'abstract': 'Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the task and thus achieve promising performance. In DialDoc21 competition, our system achieved 74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU score in subtask 2. Empirical analysis is provided to explain the effectiveness of our approaches.',\n",
       "  'year': 2021,\n",
       "  'venue': 'DIALDOC'},\n",
       " {'paperId': '2ac6d769ffa8e4966229eb3686c7ba02a946dbd5',\n",
       "  'title': 'Enhanced Universal Dependency Parsing with Second-Order Inference and Mixture of Training Data',\n",
       "  'authorId': '47120498',\n",
       "  'authorName': 'Xinyu Wang',\n",
       "  'abstract': 'This paper presents the system used in our submission to the IWPT 2020 Shared Task. Our system is a graph-based parser with second-order inference. For the low-resource Tamil corpora, we specially mixed the training data of Tamil with other languages and significantly improved the performance of Tamil. Due to our misunderstanding of the submission requirements, we submitted graphs that are not connected, which makes our system only rank 6th over 10 teams. However, after we fixed this problem, our system is 0.6 ELAS higher than the team that ranked 1st in the official results.',\n",
       "  'year': 2020,\n",
       "  'venue': 'IWPT'},\n",
       " {'paperId': '84a97e7ce30751c83de2953ed2b658146ac164ab',\n",
       "  'title': 'Transfer Learning for Health-related Twitter Data',\n",
       "  'authorId': '8696363',\n",
       "  'authorName': 'A. Dirkson',\n",
       "  'abstract': 'Transfer learning is promising for many NLP applications, especially in tasks with limited labeled data. This paper describes the methods developed by team TMRLeiden for the 2019 Social Media Mining for Health Applications (SMM4H) Shared Task. Our methods use state-of-the-art transfer learning methods to classify, extract and normalise adverse drug effects (ADRs) and to classify personal health mentions from health-related tweets. The code and fine-tuned models are publicly available.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task'},\n",
       " {'paperId': 'f630bbd08dc16e8e61deef8183eaf80d03590d28',\n",
       "  'title': 'Word Reordering and a Dynamic Programming Beam Search Algorithm for Statistical Machine Translation',\n",
       "  'authorId': '2324070',\n",
       "  'authorName': 'C. Tillmann',\n",
       "  'abstract': 'In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP). The search algorithm uses the translation model presented in Brown et al. (1993). Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. Word reordering restrictions especially useful for the translation direction German to English are presented. The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article.',\n",
       "  'year': 2003,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': '8495df711f45b8e4345502f04b797aa2abd7f54b',\n",
       "  'title': 'Generating High Quality Proposition Banks for Multilingual Semantic Role Labeling',\n",
       "  'authorId': '2403712',\n",
       "  'authorName': 'A. Akbik',\n",
       "  'abstract': 'Semantic role labeling (SRL) is crucial to natural language understanding as it identifies the predicate-argument structure in text with semantic labels. Unfortunately, resources required to construct SRL models are expensive to obtain and simply do not exist for most languages. In this paper, we present a two-stage method to enable the construction of SRL models for resourcepoor languages by exploiting monolingual SRL and multilingual parallel data. Experimental results show that our method outperforms existing methods. We use our method to generate Proposition Banks with high to reasonable quality for 7 languages in three language families and release these resources to the research community.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '8adfc29b52a277760203c9acf024f6f690591bf9',\n",
       "  'title': 'Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation',\n",
       "  'authorId': '2347956',\n",
       "  'authorName': 'Mikel Artetxe',\n",
       "  'abstract': 'Following the recent success of word embeddings, it has been argued that there is no such thing as an ideal representation for words, as different models tend to capture divergent and often mutually incompatible aspects like semantics/syntax and similarity/relatedness. In this paper, we show that each embedding model captures more information than directly apparent. A linear transformation that adjusts the similarity order of the model without any external resource can tailor it to achieve better results in those aspects, providing a new perspective on how embeddings encode divergent linguistic information. In addition, we explore the relation between intrinsic and extrinsic evaluation, as the effect of our transformations in downstream tasks is higher for unsupervised systems than for supervised ones.',\n",
       "  'year': 2018,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': 'd8b6e965b771e3d0707dd8caa57224a0dfbb886e',\n",
       "  'title': 'Harvesting Paragraph-level Question-Answer Pairs from Wikipedia',\n",
       "  'authorId': '13728923',\n",
       "  'authorName': 'X. Du',\n",
       "  'abstract': 'We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '823956ee7b994735f3605f426a71e7f85d86f1d4',\n",
       "  'title': 'Unsupervised Semantic Frame Induction using Triclustering',\n",
       "  'authorId': '2065795',\n",
       "  'authorName': 'Dmitry Ustalov',\n",
       "  'abstract': 'We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a078d53c1eff50123e2b065276663de539a40aa1',\n",
       "  'title': 'Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering',\n",
       "  'authorId': '22244290',\n",
       "  'authorName': 'Kaixin Ma',\n",
       "  'abstract': 'Non-extractive commonsense QA remains a challenging AI task, as it requires systems to reason about, synthesize, and gather disparate pieces of information, in order to generate responses to queries. Recent approaches on such tasks show increased performance, only when models are either pre-trained with additional information or when domain-specific heuristics are used, without any special consideration regarding the knowledge resource type. In this paper, we perform a survey of recent commonsense QA methods and we provide a systematic analysis of popular knowledge resources and knowledge-integration methods, across benchmarks from multiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for knowledge integration and that the degree of domain overlap, between knowledge bases and datasets, plays a crucial role in determining model success.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a5aef1c9282cb05b9e007343b8a35e60088aa58a',\n",
       "  'title': 'Automatically Extracting Topical Components for a Response-to-Text Writing Assessment',\n",
       "  'authorId': '143974800',\n",
       "  'authorName': 'Zahra Rahimi',\n",
       "  'abstract': 'We investigate automatically extracting multiword topical components to replace information currently provided by experts that is used to score the Evidence dimension of a writing in response to text assessment. Our goal is to reduce the amount of expert effort and improve the scalability of an automatic scoring system. Experimental results show that scoring performance using automatically extracted data-driven topical components is promising.',\n",
       "  'year': 2016,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': 'a82f3f87ec6a36e5653bf6ffc658f4115630375d',\n",
       "  'title': 'Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: CL-SciSumm, LaySumm and LongSumm',\n",
       "  'authorId': '2386804',\n",
       "  'authorName': 'Muthu Kumar Chandrasekaran',\n",
       "  'abstract': 'We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm, LaySumm and LongSumm. We report on each of the tasks, which received 18 submissions in total, with some submissions addressing two or three of the tasks. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SDP'},\n",
       " {'paperId': '6b5e31d5f231864f6f0f8d156e0d996c7e9812b7',\n",
       "  'title': 'An automated medical scribe for documenting clinical encounters',\n",
       "  'authorId': '1411201408',\n",
       "  'authorName': 'Greg P. Finley',\n",
       "  'abstract': 'A medical scribe is a clinical professional who charts patient–physician encounters in real time, relieving physicians of most of their administrative burden and substantially increasing productivity and job satisfaction. We present a complete implementation of an automated medical scribe. Our system can serve either as a scalable, standardized, and economical alternative to human scribes; or as an assistive tool for them, providing a first draft of a report along with a convenient means to modify it. This solution is, to our knowledge, the first automated scribe ever presented and relies upon multiple speech and language technologies, including speaker diarization, medical speech recognition, knowledge extraction, and natural language generation.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '7c94fd1f86a769cdf0b70c57be7d34d489cc6fb7',\n",
       "  'title': 'Consistent unsupervised estimators for anchored PCFGs',\n",
       "  'authorId': '143615270',\n",
       "  'authorName': 'Alexander Clark',\n",
       "  'abstract': 'Abstract Learning probabilistic context-free grammars (PCFGs) from strings is a classic problem in computational linguistics since Horning (1969). Here we present an algorithm based on distributional learning that is a consistent estimator for a large class of PCFGs that satisfy certain natural conditions including being anchored (Stratos et al., 2016). We proceed via a reparameterization of (top–down) PCFGs that we call a bottom–up weighted context-free grammar. We show that if the grammar is anchored and satisfies additional restrictions on its ambiguity, then the parameters can be directly related to distributional properties of the anchoring strings; we show the asymptotic correctness of a naive estimator and present some simulations using synthetic data that show that algorithms based on this approach have good finite sample behavior.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SCIL'},\n",
       " {'paperId': '1df3b34859896cad950e15ff1a7179f00b98b7e7',\n",
       "  'title': 'Language Generation for Multimedia Healthcare Briefings',\n",
       "  'authorId': '145590324',\n",
       "  'authorName': 'K. McKeown',\n",
       "  'abstract': 'This paper identifies issues for language generation that arose in developing a multimedia interface to healthcare data that includes coordinated speech, text and graphics. In order to produce brief speech for time-pressured caregivers, the system both combines related information into a single sentence and uses abbreviated references in speech when an unambiguous textual reference is also used. Finally, due to the temporal nature of the speech, the language generation module needs to communicate information about the ordering and duration of references to other temporal media, such as graphics, in order to allow for coordination between media.',\n",
       "  'year': 1997,\n",
       "  'venue': 'ANLP'},\n",
       " {'paperId': 'd1a3f5d3fc32f818f45fdcac8e84bfe940e21217',\n",
       "  'title': 'Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation',\n",
       "  'authorId': '1747849',\n",
       "  'authorName': 'Ivan Vulic',\n",
       "  'abstract': 'Existing approaches to automatic VerbNet-style verb classification are heavily dependent on feature engineering and therefore limited to languages with mature NLP pipelines. In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages. To the best of our knowledge, this is the first study which demonstrates how the architectures for learning word embeddings can be applied to this challenging syntactic-semantic task. Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet. A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using vector dimensions as features for learning verb classes. Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '82a0d4cd32a689e9c32f808ab8014c86c6e054e6',\n",
       "  'title': 'Syntactic methods for negation detection in radiology reports in Spanish',\n",
       "  'authorId': '2292216',\n",
       "  'authorName': 'Viviana Cotik',\n",
       "  'abstract': 'Identification of the certainty of events is an important text mining problem. In particular, biomedical texts report medical conditions or findings that might be factual, hedged or negated. Identification of negation and its scope over a term of interest determines whether a finding is reported and is a challenging task. Not much work has been performed for Spanish in this domain. \\nIn this work we introduce different algorithms developed to determine if a term of interest is under the scope of negation in radiology reports written in Spanish. \\nThe methods include syntactic techniques based in rules derived from PoS tagging patterns, constituent tree patterns and dependency tree patterns, and an adaption of NegEx, a well known rule-based negation detection algorithm (Chapman et al., 2001a). All methods outperform a simple dictionary lookup algorithm developed as baseline. NegEx and the PoS tagging pattern method obtain the best results with 0.92 F1.',\n",
       "  'year': 2016,\n",
       "  'venue': 'BioNLP@ACL'},\n",
       " {'paperId': 'a444f6278355830f4c285286a353b216419acee0',\n",
       "  'title': 'LINA: Identifying Comparable Documents from Wikipedia',\n",
       "  'authorId': '145967807',\n",
       "  'authorName': 'E. Morin',\n",
       "  'abstract': 'This paper describes the LINA system for the BUCC 2015 shared track. Following (Enright and Kondrak, 2007), our system identify comparable documents by collecting counts of hapax words. We extend this method by filtering out document pairs sharing target documents using pigeonhole reasoning and cross-lingual information .',\n",
       "  'year': 2015,\n",
       "  'venue': 'BUCC@ACL/IJCNLP'},\n",
       " {'paperId': 'fc1022b1bcc67f191be74f388d5160177342a37b',\n",
       "  'title': 'Bitextor’s participation in WMT’16: shared task on document alignment',\n",
       "  'authorId': '1399506397',\n",
       "  'authorName': 'M. Esplà-Gomis',\n",
       "  'abstract': 'This paper describes the participation of Prompsit Language Engineering and the Universitat d’Alacant in the shared task on document alignment at the First Conference on Machine Translation (WMT 2016). Two systems have been submitted, corresponding to two different versions of the tool Bitextor: the last stable release, version 4.1, and the newest one, version 5.0. The paper describes the main features of each version of the tool and discusses the results obtained on the data sets published for the shared task.',\n",
       "  'year': 2016,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '07f147581ba39eaf82aa9717ccdff5d5f9916d00',\n",
       "  'title': 'An Empirical Study of Contextual Data Augmentation for Japanese Zero Anaphora Resolution',\n",
       "  'authorId': '1666006598',\n",
       "  'authorName': 'Ryuto Konno',\n",
       "  'abstract': 'One critical issue of zero anaphora resolution (ZAR) is the scarcity of labeled data. This study explores how effectively this problem can be alleviated by data augmentation. We adopt a state-of-the-art data augmentation method, called the contextual data augmentation (CDA), that generates labeled training instances using a pretrained language model. The CDA has been reported to work well for several other natural language processing tasks, including text classification and machine translation. This study addresses two underexplored issues on CDA, that is, how to reduce the computational cost of data augmentation and how to ensure the quality of the generated data. We also propose two methods to adapt CDA to ZAR: [MASK]-based augmentation and linguistically-controlled masking. Consequently, the experimental results on Japanese ZAR show that our methods contribute to both the accuracy gainand the computation cost reduction. Our closer analysis reveals that the proposed method can improve the quality of the augmented training data when compared to the conventional CDA.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': 'e5e43a5d56b8e73e6a6d4fb5b9dbd766ee20380b',\n",
       "  'title': 'Reducing Sparsity Improves the Recognition of Implicit Discourse Relations',\n",
       "  'authorId': '22319255',\n",
       "  'authorName': 'Junyi Jessy Li',\n",
       "  'abstract': 'The earliest work on automatic detection of implicit discourse relations relied on lexical features. More recently, researchers have demonstrated that syntactic features are superior to lexical features for the task. In this paper we re-examine the two classes of state of the art representations: syntactic production rules and word pair features. In particular, we focus on the need to reduce sparsity in instance representation, demonstrating that different representation choices even for the same class of features may exacerbate sparsity issues and reduce performance. We present results that clearly reveal that lexicalization of the syntactic features is necessary for good performance. We introduce a novel, less sparse, syntactic representation which leads to improvement in discourse relation recognition. Finally, we demonstrate that classifiers trained on different representations, especially lexical ones, behave rather differently and thus could likely be combined in future systems.',\n",
       "  'year': 2014,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': 'ecda782e203e0b350d65e35b029843ec45abc1f2',\n",
       "  'title': 'Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method',\n",
       "  'authorId': '1728955',\n",
       "  'authorName': 'Ching-Yun Chang',\n",
       "  'abstract': 'Linguistic steganography is concerned with hiding information in natural language text. One of the major transformations used in linguistic steganography is synonym substitution. However, few existing studies have studied the practical application of this approach. In this article we propose two improvements to the use of synonym substitution for encoding hidden bits of information. First, we use the Google n-gram corpus for checking the applicability of a synonym in context, and we evaluate this method using data from the SemEval lexical substitution task and human annotated data. Second, we address the problem that arises from words with more than one sense, which creates a potential ambiguity in terms of which bits are represented by a particular word. We develop a novel method in which words are the vertices in a graph, synonyms are linked by edges, and the bits assigned to a word are determined by a vertex coding algorithm. This method ensures that each word represents a unique sequence of bits, without cutting out large numbers of synonyms, and thus maintains a reasonable embedding capacity.',\n",
       "  'year': 2014,\n",
       "  'venue': 'Computational Linguistics'},\n",
       " {'paperId': 'd8b3ceefeb1307d569e24a607a35947910edfdc5',\n",
       "  'title': 'Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings',\n",
       "  'authorId': '2132856',\n",
       "  'authorName': 'Philippa Shoemark',\n",
       "  'abstract': 'Word embeddings are increasingly used for the automatic detection of semantic change; yet, a robust evaluation and systematic comparison of the choices involved has been lacking. We propose a new evaluation framework for semantic change detection and find that (i) using the whole time series is preferable over only comparing between the first and last time points; (ii) independently trained and aligned embeddings perform better than continuously trained embeddings for long time periods; and (iii) that the reference point for comparison matters. We also present an analysis of the changes detected on a large Twitter dataset spanning 5.5 years.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'e323f3e86cd766b50ade5d722af810f1ce3d2664',\n",
       "  'title': 'Obligation and Prohibition Extraction Using Hierarchical RNNs',\n",
       "  'authorId': '10783142',\n",
       "  'authorName': 'Ilias Chalkidis',\n",
       "  'abstract': 'We consider the task of detecting contractual obligations and prohibitions. We show that a self-attention mechanism improves the performance of a BILSTM classifier, the previous state of the art for this task, by allowing it to focus on indicative tokens. We also introduce a hierarchical BILSTM, which converts each sentence to an embedding, and processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers surrounding sentences, because the hierarchical model has a broader discourse view.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '477093dda32ccea94a39bc9db056e3973e19eb9d',\n",
       "  'title': 'How Much of Enhanced UD Is Contained in UD?',\n",
       "  'authorId': '20493598',\n",
       "  'authorName': 'Adam Ek',\n",
       "  'abstract': 'In this paper, we present the submission of team CLASP to the IWPT 2020 Shared Task on parsing enhanced universal dependencies. We develop a tree-to-graph transformation algorithm based on dependency patterns. This algorithm can transform gold UD trees to EUD graphs with an ELAS score of 81.55 and a EULAS score of 96.70. These results show that much of the information needed to construct EUD graphs from UD trees are present in the UD trees. Coupled with a standard UD parser, the method applies to the official test data and yields and ELAS score of 67.85 and a EULAS score is 80.18.',\n",
       "  'year': 2020,\n",
       "  'venue': 'IWPT'},\n",
       " {'paperId': '926c8baf01d87468c892e6661ed8d06bf853649f',\n",
       "  'title': 'An Encoder-decoder Approach to Predicting Causal Relations in Stories',\n",
       "  'authorId': '3316824',\n",
       "  'authorName': 'Melissa Roemmele',\n",
       "  'abstract': 'We address the task of predicting causally related events in stories according to a standard evaluation framework, the Choice of Plausible Alternatives (COPA). We present a neural encoder-decoder model that learns to predict relations between adjacent sequences in stories as a means of modeling causality. We explore this approach using different methods for extracting and representing sequence pairs as well as different model architectures. We also compare the impact of different training datasets on our model. In particular, we demonstrate the usefulness of a corpus not previously applied to COPA, the ROCStories corpus. While not state-of-the-art, our results establish a new reference point for systems evaluated on COPA, and one that is particularly informative for future neural-based approaches.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': 'fe433d6c26e4a5abb9eff26cacee0da79e6bfe2d',\n",
       "  'title': 'Neural Architectures for Multilingual Semantic Parsing',\n",
       "  'authorId': '32406168',\n",
       "  'authorName': 'Raymond Hendy Susanto',\n",
       "  'abstract': 'In this paper, we address semantic parsing in a multilingual context. We train one multilingual model that is capable of parsing natural language sentences from multiple different languages into their corresponding formal semantic representations. We extend an existing sequence-to-tree model to a multi-task learning framework which shares the decoder for generating semantic representations. We report evaluation results on the multilingual GeoQuery corpus and introduce a new multilingual version of the ATIS corpus.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '54f2fd5606562691cdea3062fc6b5463f4f86750',\n",
       "  'title': 'A Probabilistic Corpus-Driven Model for Lexical-Functional Analysis',\n",
       "  'authorId': '1764192',\n",
       "  'authorName': 'R. Bod',\n",
       "  'abstract': 'We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical-Functional Grammar (LFG). We start by summarizing the original DOP model for tree representations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings.',\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ff65f089d7abe1222182cb72e60c33dc145a1888',\n",
       "  'title': 'JCTDHS at SemEval-2019 Task 5: Detection of Hate Speech in Tweets using Deep Learning Methods, Character N-gram Features, and Preprocessing Methods',\n",
       "  'authorId': '1401813543',\n",
       "  'authorName': 'Yaakov Hacohen-Kerner',\n",
       "  'abstract': 'In this paper, we describe our submissions to SemEval-2019 contest. We tackled subtask A - “a binary classification where systems have to predict whether a tweet with a given target (women or immigrants) is hateful or not hateful”, a part of task 5 “Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)”. Our system JCTDHS (Jerusalem College of Technology Detects Hate Speech) was developed for tweets written in English. We applied various supervised ML methods, various combinations of n-gram features using the TF-IDF scheme and. In addition, we applied various combinations of eight basic preprocessing methods. Our best submission was a special bidirectional RNN, which was ranked at the 11th position out of 68 submissions.',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'e5cbdfe8e90dfa0ddcccc832a0b5b2ae7e47bd53',\n",
       "  'title': 'Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning',\n",
       "  'authorId': '1941442',\n",
       "  'authorName': 'Jianpeng Cheng',\n",
       "  'abstract': 'Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'f9e3d3379c4796214e67fcaee250275c4c8cdd05',\n",
       "  'title': 'LitWay, Discriminative Extraction for Different Bio-Events',\n",
       "  'authorId': '40144368',\n",
       "  'authorName': 'Chen Li',\n",
       "  'abstract': 'Even a simple biological phenomenon may introduce a complex network of molecular interactions. Scientific literature is one of the trustful resources delivering knowledge of these networks. We propose LitWay, a system for extracting semantic relations from texts. LitWay utilizes a hybrid method that combines both a rule-based method and a machine learning-based method. It is tested on the SeeDev task of BioNLP-ST 2016, achieves the state-of-the-art performance with the F-score of 43.2%, ranking first of all participating teams. To further reveal the linguistic characteristics of each event, we test the system solely with syntactic rules or machine learning, and different combinations of two methods. We find that it is difficult for one method to achieve good performance for all semantic relation types due to the complication of bio-events in the literatures.',\n",
       "  'year': 2016,\n",
       "  'venue': 'BioNLP'},\n",
       " {'paperId': 'd8857a0490b4dc4d2ec7957ca8baae0cd3bec684',\n",
       "  'title': 'Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots',\n",
       "  'authorId': '49046540',\n",
       "  'authorName': 'Cao Liu',\n",
       "  'abstract': 'Conventional chatbots focus on two-party response generation, which simplifies the real dialogue scene. In this paper, we strive toward a novel task of Response Generation on Multi-Party Chatbot (RGMPC), where the generated responses heavily rely on the interlocutors’ roles (e.g., speaker and addressee) and their utterances. Unfortunately, complex interactions among the interlocutors’ roles make it challenging to precisely capture conversational contexts and interlocutors’ information. Facing this challenge, we present a response generation model which incorporates Interlocutor-aware Contexts into Recurrent Encoder-Decoder frameworks (ICRED) for RGMPC. Specifically, we employ interactive representations to capture dialogue contexts for different interlocutors. Moreover, we leverage an addressee memory to enhance contextual interlocutor information for the target addressee. Finally, we construct a corpus for RGMPC based on an existing open-access dataset. Automatic and manual evaluations demonstrate that the ICRED remarkably outperforms strong baselines.',\n",
       "  'year': 2019,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': '0c68d7d153bb56e4637d6aee051d87580e05fd5b',\n",
       "  'title': 'Detecting East Asian Prejudice on Social Media',\n",
       "  'authorId': '2737827',\n",
       "  'authorName': 'Bertie Vidgen',\n",
       "  'abstract': 'During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ALW'},\n",
       " {'paperId': '297c5602666aeed136e0897c2cd0020d49414830',\n",
       "  'title': 'Pointing to Subwords for Generating Function Names in Source Code',\n",
       "  'authorId': '47808167',\n",
       "  'authorName': 'Shogo Fujita',\n",
       "  'abstract': 'We tackle the task of automatically generating a function name from source code. Existing generators face difficulties in generating low-frequency or out-of-vocabulary subwords. In this paper, we propose two strategies for copying low-frequency or out-of-vocabulary subwords in inputs. Our best performing model showed an improvement over the conventional method in terms of our modified F1 and accuracy on the Java-small and Java-large datasets.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '806dc5c64d3a65f89e0f26ff9f51bb029c6908b2',\n",
       "  'title': 'A Multi-Level Attention Model for Evidence-Based Fact Checking',\n",
       "  'authorId': '2782010',\n",
       "  'authorName': 'C. Kruengkrai',\n",
       "  'abstract': 'Evidence-based fact checking aims to verify the truthfulness of a claim against evidence extracted from textual sources. Learning a representation that effectively captures relations between a claim and evidence can be challenging. Recent state-of-the-art approaches have developed increasingly sophisticated models based on graph structures. We present a simple model that can be trained on sequence structures. Our model enables inter-sentence attentions at different levels and can benefit from joint training. Results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that our model outperforms the graphbased approaches and yields 1.09% and 1.42% improvements in label accuracy and FEVER score, respectively, over the best published model.1',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'a3e09be944f2658918c97791c23206afaccd9052',\n",
       "  'title': 'Sentiment Aware Neural Machine Translation',\n",
       "  'authorId': '152358188',\n",
       "  'authorName': 'Chenglei Si',\n",
       "  'abstract': 'Sentiment ambiguous lexicons refer to words where their polarity depends strongly on con- text. As such, when the context is absent, their translations or their embedded sentence ends up (incorrectly) being dependent on the training data. While neural machine translation (NMT) has achieved great progress in recent years, most systems aim to produce one single correct translation for a given source sentence. We investigate the translation variation in two sentiment scenarios. We perform experiments to study the preservation of sentiment during translation with three different methods that we propose. We conducted tests with both sentiment and non-sentiment bearing contexts to examine the effectiveness of our methods. We show that NMT can generate both positive- and negative-valent translations of a source sentence, based on a given input sentiment label. Empirical evaluations show that our valence-sensitive embedding (VSE) method significantly outperforms a sequence-to-sequence (seq2seq) baseline, both in terms of BLEU score and ambiguous word translation accuracy in test, given non-sentiment bearing contexts.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'cfef9993443f08f246fdea42a468918df63ed4f8',\n",
       "  'title': 'Weakly Supervised Part-of-speech Tagging Using Eye-tracking Data',\n",
       "  'authorId': '38094253',\n",
       "  'authorName': 'Maria Barrett',\n",
       "  'abstract': 'For many of the world’s languages, there are no or very few linguistically annotated resources. On the other hand, raw text, and often also dictionaries, can be harvested from the web for many of these languages, and part-of-speech taggers can be trained with these resources. At the same time, previous research shows that eye-tracking data, which can be obtained without explicit annotation, contains clues to part-of-speech information. In this work, we bring these two ideas together and show that given raw text, a dictionary, and eye-tracking data obtained from naive participants reading text, we can train a weakly supervised PoS tagger using a second-order HMM with maximum entropy emissions. The best model use type-level ag-gregates of eye-tracking data and signiﬁ-cantly outperforms a baseline that does not have access to eye-tracking data.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ffd0b74c9760a25806912cdbfb14cebac69a34c6',\n",
       "  'title': 'Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction',\n",
       "  'authorId': '49224924',\n",
       "  'authorName': 'Federico Bianchi',\n",
       "  'abstract': 'We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a composition function are learned from user data only. We show how the resulting semantics for noun phrases exhibits compositional properties while being fully learnable without any explicit labelling. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as word2vec and BERT.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '6a0388c46f2aff013343fdafaaffacf56a315915',\n",
       "  'title': 'The Social Impact of Natural Language Processing',\n",
       "  'authorId': '2022288',\n",
       "  'authorName': 'Dirk Hovy',\n",
       "  'abstract': 'Medical sciences have long since established an ethics code for experiments, to minimize the risk of harm to subjects. Natural language processing (NLP) used to involve mostly anonymous corpora, with the goal of enriching linguistic analysis, and was therefore unlikely to raise ethical concerns. As NLP becomes increasingly wide-spread and uses more data from social media, however, the situation has changed: the outcome of NLP experiments and applications can now have a direct effect on individual users’ lives. Until now, the discourse on this topic in the field has not followed the technological development, while public discourse was often focused on exaggerated dangers. This position paper tries to take back the initiative and start a discussion. We identify a number of social implications of NLP and discuss their ethical significance, as well as ways to address them.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c3cfe631890f175cdeaf228b4c78d055bc4d988f',\n",
       "  'title': 'Minimizing Annotation Effort via Max-Volume Spectral Sampling',\n",
       "  'authorId': '3171632',\n",
       "  'authorName': 'A. Quattoni',\n",
       "  'abstract': 'We address the annotation data bottleneck for sequence classiﬁcation. Speciﬁcally we ask the question: if one has a budget of N annotations, which samples should we select for annotation? The solution we propose looks for diversity in the selected sample, by maximiz-ing the amount of information that is useful for the learning algorithm, or equivalently by minimizing the redundancy of samples in the selection. This is formulated in the context of spectral learning of recurrent functions for sequence classiﬁcation. Our method represents unlabeled data in the form of a Hankel matrix, and uses the notion of spectral max-volume to ﬁnd a compact sub-block from which annotation samples are drawn. Experiments on sequence classiﬁcation conﬁrm that our spectral sampling strategy is in fact efﬁcient and yields good models.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '87cddd89d6e902b76f6e33e3798166bc6cb4b58f',\n",
       "  'title': 'Corpus based Amharic sentiment lexicon generation',\n",
       "  'authorId': '1805994426',\n",
       "  'authorName': 'Girma Neshir Alemneh',\n",
       "  'abstract': 'Sentiment classification is an active research area with several applications including analysis of political opinions, classifying comments, movie reviews, news reviews and product reviews. To employ rule based sentiment classification, we require sentiment lexicons. However, manual construction of sentiment lexicon is time consuming and costly for resource-limited languages. To bypass manual development time and costs, we tried to build Amharic Sentiment Lexicons relying on corpus based approach. The intention of this approach is to handle sentiment terms specific to Amharic language from Amharic Corpus. Small set of seed terms are manually prepared from three parts of speech such as noun, adjective and verb. We developed algorithms for constructing Amharic sentiment lexicons automatically from Amharic news corpus. Corpus based approach is proposed relying on the word co-occurrence distributional embedding including frequency based embedding (i.e. Positive Point-wise Mutual Information PPMI). Using PPMI with threshold value of 100 and 200, we got corpus based Amharic Sentiment lexicons of size 1811 and 3794 respectively by expanding 519 seeds. Finally, the lexicon generated in corpus based approach is evaluated.',\n",
       "  'year': 2020,\n",
       "  'venue': 'WINLP'},\n",
       " {'paperId': 'cb143ecaf720544f07e32a8f58be825c49105f1d',\n",
       "  'title': 'Multi-Predicate Semantic Role Labeling',\n",
       "  'authorId': '3292660',\n",
       "  'authorName': 'Haitong Yang',\n",
       "  'abstract': 'The current approaches to Semantic Role Labeling (SRL) usually perform role classification for each predicate separately and the interaction among individual predicate’s role labeling is ignored if there is more than one predicate in a sentence. In this paper, we prove that different predicates in a sentence could help each other during SRL. In multi-predicate role labeling, there are mainly two key points: argument identification and role labeling of the arguments shared by multiple predicates. To address these issues, in the stage of argument identification, we propose novel predicate-related features which help remove many argument identification errors; in the stage of argument classification, we adopt a discriminative reranking approach to perform role classification of the shared arguments, in which a large set of global features are proposed. We conducted experiments on two standard benchmarks: Chinese PropBank and English PropBank. The experimental results show that our approach can significantly improve SRL performance, especially in Chinese PropBank.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '2e0fab543b49b315d29b0d81e8f958707456f69e',\n",
       "  'title': 'PuzzLing Machines: A Challenge on Learning From Small Data',\n",
       "  'authorId': '7655033',\n",
       "  'authorName': 'Gözde Gül Sahin',\n",
       "  'abstract': 'Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students. These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions. Solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills. Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages. We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected. We hope that this benchmark, available at https://ukplab.github.io/PuzzLing-Machines/, inspires further efforts towards a new paradigm in NLP—one that is grounded in human-like reasoning and understanding.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '4f7f84e95d020f9ce03ad2465cf9546a429441fe',\n",
       "  'title': 'From Sentiment Annotations to Sentiment Prediction through Discourse Augmentation',\n",
       "  'authorId': '2024784636',\n",
       "  'authorName': 'Patrick Huber',\n",
       "  'abstract': 'Sentiment analysis, especially for long documents, plausibly requires methods capturing complex linguistics structures. To accommodate this, we propose a novel framework to exploit task-related discourse for the task of sentiment analysis. More specifically, we are combining the large-scale, sentiment-dependent MEGA-DT treebank with a novel neural architecture for sentiment prediction, based on a hybrid TreeLSTM hierarchical attention model. Experiments show that our framework using sentiment-related discourse augmentations for sentiment prediction enhances the overall performance for long documents, even beyond previous approaches using well-established discourse parsers trained on human annotated data. We show that a simple ensemble approach can further enhance performance by selectively using discourse, depending on the document length.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '5e05fb7bc137fdad765b654e7178248bfb4b3a64',\n",
       "  'title': 'Predicting Romanian Stress Assignment',\n",
       "  'authorId': '3094242',\n",
       "  'authorName': 'A. Ciobanu',\n",
       "  'abstract': 'We train and evaluate two models for Romanian stress prediction: a baseline model which employs the consonant-vowel structure of the words and a cascaded model with averaged perceptron training consisting of two sequential models ‐ one for predicting syllable boundaries and another one for predicting stress placement. We show in this paper that Romanian stress is predictable, though not deterministic, by using data-driven machine learning techniques.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '07ded4cf00095d91e8689a0a52d9e20eb64aca0b',\n",
       "  'title': 'MuTual: A Dataset for Multi-Turn Dialogue Reasoning',\n",
       "  'authorId': '152496687',\n",
       "  'authorName': 'Leyang Cui',\n",
       "  'abstract': 'Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams. Compared to previous benchmarks for non-task oriented dialogue systems, MuTual is much more challenging since it requires a model that be able to handle various reasoning problems. Empirical results show that state-of-the-art methods only reach 71%, which is far behind human performance of 94%, indicating that there is ample room for improving reasoning ability.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd7cb4532a1f09c5e7846e8b6746cbf526443c1a7',\n",
       "  'title': 'MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision',\n",
       "  'authorId': '2024784636',\n",
       "  'authorName': 'Patrick Huber',\n",
       "  'abstract': 'The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment-annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '88a72a9bc844ceb809f0fcf94ae3a0ba69e558de',\n",
       "  'title': 'Importance-based Neuron Allocation for Multilingual Neural Machine Translation',\n",
       "  'authorId': '145106559',\n",
       "  'authorName': 'W. Xie',\n",
       "  'abstract': 'Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'f6ec1fa2def777848485f13e4f9fcda04d6b9330',\n",
       "  'title': 'A Unified Neural Architecture for Joint Dialog Act Segmentation and Recognition in Spoken Dialog System',\n",
       "  'authorId': '46887780',\n",
       "  'authorName': 'Tianyu Zhao',\n",
       "  'abstract': 'In spoken dialog systems (SDSs), dialog act (DA) segmentation and recognition provide essential information for response generation. A majority of previous works assumed ground-truth segmentation of DA units, which is not available from automatic speech recognition (ASR) in SDS. We propose a unified architecture based on neural networks, which consists of a sequence tagger for segmentation and a classifier for recognition. The DA recognition model is based on hierarchical neural networks to incorporate the context of preceding sentences. We investigate sharing some layers of the two components so that they can be trained jointly and learn generalized features from both tasks. An evaluation on the Switchboard Dialog Act (SwDA) corpus shows that the jointly-trained models outperform independently-trained models, single-step models, and other reported results in DA segmentation, recognition, and joint tasks.',\n",
       "  'year': 2018,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '9bc1c0956bf72207ee73204138c9d5066dce8c4e',\n",
       "  'title': 'Named Entity Recognition in the Romanian Legal Domain',\n",
       "  'authorId': '1723144442',\n",
       "  'authorName': 'V. Pais',\n",
       "  'abstract': 'Recognition of named entities present in text is an important step towards information extraction and natural language understanding. This work presents a named entity recognition system for the Romanian legal domain. The system makes use of the gold annotated LegalNERo corpus. Furthermore, the system combines multiple distributional representations of words, including word embeddings trained on a large legal domain corpus. All the resources, including the corpus, model and word embeddings are open sourced. Finally, the best system is available for direct usage in the RELATE platform.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NLLP'},\n",
       " {'paperId': 'dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5',\n",
       "  'title': 'Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?',\n",
       "  'authorId': '2003628072',\n",
       "  'authorName': 'Arij Riabi',\n",
       "  'abstract': 'Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set- tings.',\n",
       "  'year': 2021,\n",
       "  'venue': 'WNUT'},\n",
       " {'paperId': '4b3e69a7736f4a83914bb8ae76d175c3316b6fcd',\n",
       "  'title': 'Improved Topic Representations of Medical Documents to Assist COVID-19 Literature Exploration',\n",
       "  'authorId': '1881453937',\n",
       "  'authorName': 'Yulia Otmakhova',\n",
       "  'abstract': 'Efficient discovery and exploration of biomedical literature has grown in importance in the context of the COVID-19 pandemic, and topicbased methods such as latent Dirichlet allocation (LDA) are a useful tool for this purpose. In this study we compare traditional topic models based on word tokens with topic models based on medical concepts, and propose several ways to improve topic coherence and specificity.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NLP4COVID@EMNLP'},\n",
       " {'paperId': '0bb5618c74d4de1cb0bba04c06814ffcbcc18a18',\n",
       "  'title': 'Enhancing Transformer with Sememe Knowledge',\n",
       "  'authorId': '49889860',\n",
       "  'authorName': 'Yuhui Zhang',\n",
       "  'abstract': 'While large-scale pretraining has achieved great success in many NLP tasks, it has not been fully studied whether external linguistic knowledge can improve data-driven models. In this work, we introduce sememe knowledge into Transformer and propose three sememe-enhanced Transformer models. Sememes, by linguistic definition, are the minimum semantic units of language, which can well represent implicit semantic meanings behind words. Our experiments demonstrate that introducing sememe knowledge into Transformer can consistently improve language modeling and downstream tasks. The adversarial test further demonstrates that sememe knowledge can substantially improve model robustness.',\n",
       "  'year': 2020,\n",
       "  'venue': 'REPL4NLP'},\n",
       " {'paperId': '41b220365fb3fcd4ab050b3b975db5413c958503',\n",
       "  'title': 'Generalizing Unmasking for Short Texts',\n",
       "  'authorId': '9537652',\n",
       "  'authorName': 'Janek Bevendorff',\n",
       "  'abstract': 'Authorship verification is the problem of inferring whether two texts were written by the same author. For this task, unmasking is one of the most robust approaches as of today with the major shortcoming of only being applicable to book-length texts. In this paper, we present a generalized unmasking approach which allows for authorship verification of texts as short as four printed pages with very high precision at an adjustable recall tradeoff. Our generalized approach therefore reduces the required material by orders of magnitude, making unmasking applicable to authorship cases of more practical proportions. The new approach is on par with other state-of-the-art techniques that are optimized for texts of this length: it achieves accuracies of 75–80%, while also allowing for easy adjustment to forensic scenarios that require higher levels of confidence in the classification.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '8828c7db08c0baa1de735fb070e3a68d33d6353a',\n",
       "  'title': 'Semantics, Discourse and Statistical Machine Translation',\n",
       "  'authorId': '2694222',\n",
       "  'authorName': 'Deyi Xiong',\n",
       "  'abstract': 'In the past decade, statistical machine translation (SMT) has been advanced from word-based SMT to phraseand syntax-based SMT. Although this advancement produces significant improvements in BLEU scores, crucial meaning errors and lack of cross-sentence connections at discourse level still hurt the quality of SMT-generated translations. More recently, we have witnessed two active movements in SMT research: one towards combining semantics and SMT in attempt to generate not only grammatical but also meaningpreserved translations, and the other towards exploring discourse knowledge for document-level machine translation in order to capture intersentence dependencies. The emergence of semantic SMT are due to the combination of two factors: the necessity of semantic modeling in SMT and the renewed interest of designing models tailored to relevant NLP/SMT applications in the semantics community. The former is represented by recent numerous studies on exploring word sense disambiguation, semantic role labeling, bilingual semantic representations as well as semantic evaluation for SMT. The latter is reflected in CoNLL shared tasks, SemEval and SenEval exercises in recent years. The need of capturing cross-sentence dependencies for document-level SMT triggers the resurgent interest of modeling translation from the perspective of discourse. Discourse phenomena, such as coherent relations, discourse topics, lexical cohesion that are beyond the scope of conventional sentence-level n-grams, have been recently considered and explored in the context of SMT. This tutorial aims at providing a timely and combined introduction of such recent work along these two trends as discourse is inherently connected with semantics. The tutorial has three parts. The first part critically reviews the phraseand syntax-based SMT. The second part is devoted to the lines of research oriented to semantic SMT, including a brief introduction of semantics, lexical and shallow semantics tailored to SMT, semantic representations in SMT, semantically motivated evaluation as well as advanced topics on deep semantic learning for SMT. The third part is dedicated to recent work on SMT with discourse, including a brief review on discourse studies from linguistics and computational viewpoints, discourse research from monolingual to multilingual, discourse-based SMT and a few advanced topics. The tutorial is targeted for researchers in the SMT, semantics and discourse communities. In particular, the expected audience comes from two groups: 1) Researchers and students in the SMT community who want to design cutting-edge models and algorithms for semantic SMT with various semantic knowledge and representations, and who would like to advance SMT from sentence-bysentence translation to document-level translation with discourse information; 2) Researchers and students from the semantics and discourse community who are interested in developing models and methods and adapting them to SMT.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '41c4495e3bb9ceaba4aa2e6e809d24d7ef6263d6',\n",
       "  'title': 'FrameNet-assisted Noun Compound Interpretation',\n",
       "  'authorId': '32003673',\n",
       "  'authorName': 'Girishkumar Ponkiya',\n",
       "  'abstract': 'Given a noun compound (NC), we address the problem of predicting the appropriate semantic label linking the constituents of the NC. This problem is called Noun Compound Interpretation (NCI). We use FrameNet as a semantic label repository. For example, given the noun compound (board approval), we predict the frame (DENY OR GRANT PERMISSION, as per FrameNet) as appropriate and the semantic role of the modifier word (AUTHORITY) as the semantic label linking board and approval; the resulting label is DENY OR GRANT PERMISSION:AUTHORITY. Our semantic label repository is very large (≈ 11k labels) compared to the NC data available for training (approx 1900). Thus, learning in this case, especially for unseen semantic labels, is hard. We propose to solve this problem by predicting semantic labels in a continuous label embedding space, which is novel. This embedding space is created by learning label embeddings using the FrameNet data. The embeddings are then used to train two separate models – one for predicting Frames and the other for FEs. As the label embedding space captures the semantics of the labels, using these embeddings enables generalizing well on unseen labels, thus achieving zero-shot learning. Our preliminary investigations show that the proposed approach performs well for unseen labels, achieving 5% and 2% points improvements over baselines for the frame and FE prediction, respectively. The study shows the promise of the use of continuous space embeddings for noun compound interpretation and points to the need for further investigation.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'c9329d720d99e8b901115893d77d29039f9a818e',\n",
       "  'title': 'Exploring Asymmetric Clustering for Statistical Language Modeling',\n",
       "  'authorId': '1800422',\n",
       "  'authorName': 'Jianfeng Gao',\n",
       "  'abstract': 'The n-gram model is a stochastic model, which predicts the next word (predicted word) given the previous words (conditional words) in a word sequence. The cluster n-gram model is a variant of the n-gram model in which similar words are classified in the same cluster. It has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words. This is the basis of the asymmetric cluster model (ACM) discussed in our study. In this paper, we first present a formal definition of the ACM. We then describe in detail the methodology of constructing the ACM. The effectiveness of the ACM is evaluated on a realistic application, namely Japanese Kana-Kanji conversion. Experimental results show substantial improvements of the ACM in comparison with classical cluster models and word n-gram models at the same model size. Our analysis shows that the high-performance of the ACM lies in the asymmetry of the model.',\n",
       "  'year': 2002,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '2bb4b6b876581c0c21ef58d3c38809715173ca7a',\n",
       "  'title': 'PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation',\n",
       "  'authorId': '2152348673',\n",
       "  'authorName': 'Wei Zhu',\n",
       "  'abstract': 'This paper describes the models designated for the MEDIQA 2019 shared tasks by the team PANLP. We take advantages of the recent advances in pre-trained bidirectional transformer language models such as BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019b). We find that pre-trained language models can significantly outperform traditional deep learning models. Transfer learning from the NLI task to the RQE task is also experimented, which proves to be useful in improving the results of fine-tuning MT-DNN large. A knowledge distillation process is implemented, to distill the knowledge contained in a set of models and transfer it into an single model, whose performance turns out to be comparable with that obtained by the ensemble of that set of models. Finally, for test submissions, model ensemble and a re-ranking process are implemented to boost the performances. Our models participated in all three tasks and ranked the 1st place for the RQE task, and the 2nd place for the NLI task, and also the 2nd place for the QA task.',\n",
       "  'year': 2019,\n",
       "  'venue': 'BioNLP@ACL'},\n",
       " {'paperId': '5ba79e88b37a084026ddc9ed2875a9dbe156aed4',\n",
       "  'title': 'Feature Optimization for Constituent Parsing via Neural Networks',\n",
       "  'authorId': '40296541',\n",
       "  'authorName': 'Zhiguo Wang',\n",
       "  'abstract': 'The performance of discriminative constituent parsing relies crucially on feature engineering, and effective features usually have to be carefully selected through a painful manual process. In this paper, we propose to automatically learn a set of effective features via neural networks. Specifically, we build a feedforward neural network model, which takes as input a few primitive units (words, POS tags and certain contextual tokens) from the local context, induces the feature representation in the hidden layer and makes parsing predictions in the output layer. The network simultaneously learns the feature representation and the prediction model parameters using a back propagation algorithm. By pre-training the model on a large amount of automatically parsed data, and then fine-tuning on the manually annotated Treebank data, our parser achieves the highest F1 score at 86.6% on Chinese Treebank 5.1, and a competitive F1 score at 90.7% on English Treebank. More importantly, our parser generalizes well on cross-domain test sets, where we significantly outperform Berkeley parser by 3.4 points on average for Chinese and 2.5 points for English.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '97d5e1a7ed819f6f10b3a1dfacd98ea2a0ae4954',\n",
       "  'title': 'Fair Evaluation in Concept Normalization: a Large-scale Comparative Analysis for BERT-based Models',\n",
       "  'authorId': '2617496',\n",
       "  'authorName': 'E. Tutubalina',\n",
       "  'abstract': 'Linking of biomedical entity mentions to various terminologies of chemicals, diseases, genes, adverse drug reactions is a challenging task, often requiring non-syntactic interpretation. A large number of biomedical corpora and state-of-the-art models have been introduced in the past five years. However, there are no general guidelines regarding the evaluation of models on these corpora in single- and cross-terminology settings. In this work, we perform a comparative evaluation of various benchmarks and study the efficiency of state-of-the-art neural architectures based on Bidirectional Encoder Representations from Transformers (BERT) for linking of three entity types across three domains: research abstracts, drug labels, and user-generated texts on drug therapy in English. We have made the source code and results available at https://github.com/insilicomedicine/Fair-Evaluation-BERT.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '798c61b2b985e918a74b9aa154e6bc3f01040763',\n",
       "  'title': 'Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence',\n",
       "  'authorId': '145902734',\n",
       "  'authorName': 'Jian Guan',\n",
       "  'abstract': 'Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '020a72f25ef5b6a76ec1ccde0a58ab6680e367db',\n",
       "  'title': 'An Improved Approach for Semantic Graph Composition with CCG',\n",
       "  'authorId': '145079952',\n",
       "  'authorName': 'Austin Blodgett',\n",
       "  'abstract': 'This paper builds on previous work using Combinatory Categorial Grammar (CCG) to derive a transparent syntax-semantics interface for Abstract Meaning Representation (AMR) parsing. We define new semantics for the CCG combinators that is better suited to deriving AMR graphs. In particular, we define relation-wise alternatives for the application and composition combinators: these require that the two constituents being combined overlap in one AMR relation. We also provide a new semantics for type raising, which is necessary for certain constructions. Using these mechanisms, we suggest an analysis of eventive nouns, which present a challenge for deriving AMR graphs. Our theoretical analysis will facilitate future work on robust and transparent AMR parsing using CCG.',\n",
       "  'year': 2019,\n",
       "  'venue': 'IWCS'},\n",
       " {'paperId': 'a93cb72c5936d6eb5e8d14ff2994cb72ef50cc69',\n",
       "  'title': 'A Simple Bayesian Modelling Approach to Event Extraction from Twitter',\n",
       "  'authorId': '1725992',\n",
       "  'authorName': 'Deyu Zhou',\n",
       "  'abstract': 'With the proliferation of social media sites, social streams have proven to contain the most up-to-date information on current events. Therefore, it is crucial to extract events from the social streams such as tweets. However, it is not straightforward to adapt the existing event extraction systems since texts in social media are fragmented and noisy. In this paper we propose a simple and yet effective Bayesian model, called Latent Event Model (LEM), to extract structured representation of events from social media. LEM is fully unsupervised and does not require annotated data for training. We evaluate LEM on a Twitter corpus. Experimental results show that the proposed model achieves 83% in F-measure, and outperforms the state-of-the-art baseline by over 7%.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '5e413c1c7c84e7c7e3b79b6abc461fcb6f595ef2',\n",
       "  'title': 'Evidence-based Fact-Checking of Health-related Claims',\n",
       "  'authorId': '2692127',\n",
       "  'authorName': 'Mourad Sarrouti',\n",
       "  'abstract': 'The task of verifying the truthfulness of claims in textual documents, or fact-checking, has received signiﬁcant attention in recent years. Many existing evidence-based fact-checking datasets contain synthetic claims and the models trained on these data might not be able to verify real-world claims. Partic-ularly few studies addressed evidence-based fact-checking of health-related claims that re-quire medical expertise or evidence from the scientiﬁc literature. In this paper, we introduce H EALTH V ER , a new dataset for evidence-based fact-checking of health-related claims that allows to study the validity of real-world claims by evaluating their truthfulness against scientiﬁc articles. Using a three-step data creation method, we ﬁrst retrieved real-world claims from snippets returned by a search engine for questions about COVID-19. Then we automatically retrieved and re-ranked relevant scientiﬁc papers using a T5 relevance-based model. Finally, the relations between each evidence statement and the associated claim were manually annotated as S UPPORT , R EFUTE and N EUTRAL . To validate the created dataset of 14,330 evidence-claim pairs, we developed baseline models based on pretrained language models. Our experiments showed that training deep learning models on real-world medical claims greatly improves performance compared to models trained on synthetic and open-domain claims. Our results and manual analysis suggest that H EALTH V ER Evidence: Recent research results suggest that bats or pan- golins might be the original hosts for the virus based on comparative studies using its genomic sequences. Claims with labels:',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b29e05badbf3c08997b0231cc1685651b7bff339',\n",
       "  'title': 'LingJing at SemEval-2022 Task 1: Multi-task Self-supervised Pre-training for Multilingual Reverse Dictionary',\n",
       "  'authorId': '2156072001',\n",
       "  'authorName': 'Bin Li',\n",
       "  'abstract': 'This paper introduces the approach of Team LingJing’s experiments on SemEval-2022 Task 1 Comparing Dictionaries and Word Embeddings (CODWOE). This task aims at comparing two types of semantic descriptions and including two sub-tasks: the definition modeling and reverse dictionary track. Our team focuses on the reverse dictionary track and adopts the multi-task self-supervised pre-training for multilingual reverse dictionaries. Specifically, the randomly initialized mDeBERTa-base model is used to perform multi-task pre-training on the multilingual training datasets. The pre-training step is divided into two stages, namely the MLM pre-training stage and the contrastive pre-training stage. The experimental results show that the proposed method has achieved good performance in the reverse dictionary track, where we rank the 1-st in the Sgns targets of the EN and RU languages. All the experimental codes are open-sourced at https://github.com/WENGSYX/Semeval.',\n",
       "  'year': 2022,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': 'e99a259299d4d555ee4c354f2095ab4401369c82',\n",
       "  'title': 'SciREX: A Challenge Dataset for Document-Level Information Extraction',\n",
       "  'authorId': '49837811',\n",
       "  'authorName': 'Sarthak Jain',\n",
       "  'abstract': 'Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'f0f77fe7fc62ec627db40aa8deb40de06cfe8be4',\n",
       "  'title': 'Exploring Controllable Text Generation Techniques',\n",
       "  'authorId': '9358910',\n",
       "  'authorName': 'Shrimai Prabhumoye',\n",
       "  'abstract': 'Neural controllable text generation is an important area gaining attention due to its plethora of applications. Although there is a large body of prior work in controllable text generation, there is no unifying theme. In this work, we provide a new schema of the pipeline of the generation process by classifying it into five modules. The control of attributes in the generation process requires modification of these modules. We present an overview of different techniques used to perform the modulation of these modules. We also provide an analysis on the advantages and disadvantages of these techniques. We further pave ways to develop new architectures based on the combination of the modules described in this paper.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '8b2623e2d1f939a7cf169ac8b81e4bde615c85c4',\n",
       "  'title': 'ECNU: Multi-level Sentiment Analysis on Twitter Using Traditional Linguistic Features and Word Embedding Features',\n",
       "  'authorId': '2109086217',\n",
       "  'authorName': 'Zhihua Zhang',\n",
       "  'abstract': 'This paper reports our submission to task 10 (Sentiment Analysis on Tweet, SAT) (Rosenthal et al., 2015) in SemEval 2015 , which contains five subtasks, i.e., contextual polarity disambiguation (subtask A: expressionlevel), message polarity classification (subtask B: message-level), topic-based message polarity classification and detecting trends towards a topic (subtask C and D: topic-level), and determining sentiment strength of twitter terms (subtask E: term-level). For the first four subtasks, we built supervised models using traditional features and word embedding features to perform sentiment polarity classification. For subtask E, we first expanded the training data with the aid of external sentiment lexicons and then built a regression model to estimate the sentiment strength. Despite the simplicity of features, our systems rank above the average.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '0ccedf9d1d996627a914ff1037d3aadb957b2c09',\n",
       "  'title': 'Lexical Normalization of User-Generated Medical Text',\n",
       "  'authorId': '8696363',\n",
       "  'authorName': 'A. Dirkson',\n",
       "  'abstract': 'In the medical domain, user-generated social media text is increasingly used as a valuable complementary knowledge source to scientific medical literature. The extraction of this knowledge is complicated by colloquial language use and misspellings. Yet, lexical normalization of such data has not been addressed properly. This paper presents an unsupervised, data-driven spelling correction module for medical social media. Our method outperforms state-of-the-art spelling correction and can detect mistakes with an F0.5 of 0.888. Additionally, we present a novel corpus for spelling mistake detection and correction on a medical patient forum.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task'},\n",
       " {'paperId': 'a7862e14b4c20cefd6dc4f611f8aa866fabf130b',\n",
       "  'title': 'Learning to Solve Arithmetic Word Problems with Verb Categorization',\n",
       "  'authorId': '81007822',\n",
       "  'authorName': 'Mohammad Javad Hosseini',\n",
       "  'abstract': 'This paper presents a novel approach to learning to solve simple arithmetic word problems. Our system, ARIS, analyzes each of the sentences in the problem statement to identify the relevant variables and their values. ARIS then maps this information into an equation that represents the problem, and enables its (trivial) solution as shown in Figure 1. The paper analyzes the arithmetic-word problems “genre”, identifying seven categories of verbs used in such problems. ARIS learns to categorize verbs with 81.2% accuracy, and is able to solve 77.7% of the problems in a corpus of standard primary school test questions. We report the first learning results on this task without reliance on predefined templates and make our data publicly available. 1',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6f30ba347243db23f939b3fa279b985f9c7fd036',\n",
       "  'title': 'Y’all should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts',\n",
       "  'authorId': '2157025',\n",
       "  'authorName': 'Gabriel Stanovsky',\n",
       "  'abstract': 'Distinguishing between singular and plural “you” in English is a challenging task which has potential for downstream applications, such as machine translation or coreference resolution. While formal written English does not distinguish between these cases, other languages (such as Spanish), as well as other dialects of English (via phrases such as “y’all”), do make this distinction. We make use of this to obtain distantly-supervised labels for the task on a large-scale in two domains. Following, we train a model to distinguish between the single/plural ‘you’, finding that although in-domain training achieves reasonable accuracy (≥ 77%), there is still a lot of room for improvement, especially in the domain-transfer scenario, which proves extremely challenging. Our code and data are publicly available.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'ccbd5a0c3dd4c589526647147125f80b0799c0a7',\n",
       "  'title': 'Automatic Question Answering for Medical MCQs: Can It go Further than Information Retrieval?',\n",
       "  'authorId': '34903252',\n",
       "  'authorName': 'L. Ha',\n",
       "  'abstract': 'We present a novel approach to automatic question answering that does not depend on the performance of an information retrieval (IR) system and does not require that the training data come from the same source as the questions. We evaluate the system performance on a challenging set of university-level medical science multiple-choice questions. Best performance is achieved when combining a neural approach with an IR approach, both of which work independently. Unlike previous approaches, the system achieves statistically significant improvement over the random guess baseline even for questions that are labeled as challenging based on the performance of baseline solvers.',\n",
       "  'year': 2019,\n",
       "  'venue': 'RANLP'},\n",
       " {'paperId': '4eb8ac1f091befb9eebb60d5b80fdac5667fd758',\n",
       "  'title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers',\n",
       "  'authorId': '3130583',\n",
       "  'authorName': 'Georgios P. Spithourakis',\n",
       "  'abstract': 'Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over non-hierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '35bac08d23a2e28221f03c5b3b273f1a49ea8b7a',\n",
       "  'title': 'Embeddings in Natural Language Processing: Theory and Advances in Vector Representations of Meaning',\n",
       "  'authorId': '144918133',\n",
       "  'authorName': 'Marcos Garcia',\n",
       "  'abstract': 'Word vector representations have a long tradition in several research fields, such as cognitive science or computational linguistics. They have been used to represent the meaning of various units of natural languages, including, among others, words, phrases, and sentences. Before the deep learning tsunami, count-based vector space models had been successfully used in computational linguistics to represent the semantics of natural languages. However, the rise of neural networks in NLP popularized the use of word embeddings, which are now applied as pre-trained vectors in most machine learning architectures. This book, written by Mohammad Taher Pilehvar and Jose Camacho-Collados, provides a comprehensive and easy-to-read review of the theory and advances in vector models for NLP, focusing specially on semantic representations and their applications. It is a great introduction to different types of embeddings and the background and motivations behind them. In this sense, the authors adequately present the most relevant concepts and approaches that have been used to build vector representations. They also keep track of the most recent advances of this vibrant and fast-evolving area of research, discussing cross-lingual representations and current language models based on the Transformer. Therefore, this is a useful book for researchers interested in computational methods for semantic representations and artificial intelligence. Although some basic knowledge of machine learning may be necessary to follow a few topics, the book includes clear illustrations and explanations, which make it accessible to a wide range of readers. Apart from the preface and the conclusions, the book is organized into eight chapters. In the first two, the authors introduce some of the core ideas of NLP and artificial neural networks, respectively, discussing several concepts that will be useful throughout the book. Then, Chapters 3 to 6 present different types of vector representations at the lexical level (word embeddings, graph embeddings, sense embeddings, and contextualized embeddings), followed by a brief chapter (7) about sentence and document embeddings. For each specific topic, the book includes methods and data sets to assess the quality of the embeddings. Finally, Chapter 8 raises ethical issues involved',\n",
       "  'year': 2021,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': '8cfcbe3a58be39092ec2d1ef86585179bf7654e0',\n",
       "  'title': 'Chunk-Based Bi-Scale Decoder for Neural Machine Translation',\n",
       "  'authorId': '2111824520',\n",
       "  'authorName': 'Hao Zhou',\n",
       "  'abstract': 'In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same time-scale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '26278111e60c9b50780b9ae37ec27a46bbe31dc5',\n",
       "  'title': 'KAFK at SemEval-2020 Task 12: Checkpoint Ensemble of Transformers for Hate Speech Classification',\n",
       "  'authorId': '2006187174',\n",
       "  'authorName': 'K. Das',\n",
       "  'abstract': 'This paper presents the approach of Team KAFK for the English edition of SemEval-2020 Task 12. We use checkpoint ensembling to create ensembles of BERT-based transformers and show that it can improve the performance of classification systems. We explore attention mask dropout to mitigate for the poor constructs of social media texts. Our classifiers scored macro-f1 of 0.909, 0.551 and 0.616 for subtasks A, B and C respectively. The code is publicly released online.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': 'c39fc494cb84b260eaa33cceda2b76512f3701a5',\n",
       "  'title': 'Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?',\n",
       "  'authorId': '3023068',\n",
       "  'authorName': 'Abhilasha Ravichander',\n",
       "  'abstract': 'Privacy plays a crucial role in preserving democratic ideals and personal autonomy. The dominant legal approach to privacy in many jurisdictions is the “Notice and Choice” paradigm, where privacy policies are the primary instrument used to convey information to users. However, privacy policies are long and complex documents that are difficult for users to read and comprehend. We discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '202a841653cf86787c57bf14e47d3ce0654c16f6',\n",
       "  'title': 'KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding',\n",
       "  'authorId': '48691049',\n",
       "  'authorName': 'Jiyeon Ham',\n",
       "  'abstract': 'Natural language inference (NLI) and semantic textual similarity (STS) are key tasks in natural language understanding (NLU). Although several benchmark datasets for those tasks have been released in English and a few other languages, there are no publicly available NLI or STS datasets in the Korean language. Motivated by this, we construct and release new datasets for Korean NLI and STS, dubbed KorNLI and KorSTS, respectively. Following previous approaches, we machine-translate existing English training sets and manually translate development and test sets into Korean. To accelerate research on Korean NLU, we also establish baselines on KorNLI and KorSTS. Our datasets are publicly available at https://github.com/kakaobrain/KorNLUDatasets.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '3bdcc99f45b58e4ddf4ffde5f58bea1ddada2744',\n",
       "  'title': 'Conversational Image Editing: Incremental Intent Identification in a New Dialogue Task',\n",
       "  'authorId': '2175808',\n",
       "  'authorName': 'R. Manuvinakurike',\n",
       "  'abstract': 'We present “conversational image editing”, a novel real-world application domain combining dialogue, visual information, and the use of computer vision. We discuss the importance of dialogue incrementality in this task, and build various models for incremental intent identification based on deep learning and traditional classification algorithms. We show how our model based on convolutional neural networks outperforms models based on random forests, long short term memory networks, and conditional random fields. By training embeddings based on image-related dialogue corpora, we outperform pre-trained out-of-the-box embeddings, for intention identification tasks. Our experiments also provide evidence that incremental intent processing may be more efficient for the user and could save time in accomplishing tasks.',\n",
       "  'year': 2018,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '55cda1dfff6fe34b5c9336290964dc2ae9ef7284',\n",
       "  'title': 'Lexicon Stratification for Translating Out-of-Vocabulary Words',\n",
       "  'authorId': '145317727',\n",
       "  'authorName': 'Yulia Tsvetkov',\n",
       "  'abstract': 'A language lexicon can be divided into four main strata, depending on origin of words: core vocabulary words, fullyand partiallyassimilated foreign words, and unassimilated foreign words (or transliterations). This paper focuses on translation of fullyand partially-assimilated foreign words, called “borrowed words”. Borrowed words (or loanwords) are content words found in nearly all languages, occupying up to 70% of the vocabulary. We use models of lexical borrowing in machine translation as a pivoting mechanism to obtain translations of out-of-vocabulary loanwords in a lowresource language. Our framework obtains substantial improvements (up to 1.6 BLEU) over standard baselines.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '21f27a2528345c09a812406cc441ee84a967873a',\n",
       "  'title': 'Vision-Language Pretraining: Current Trends and the Future',\n",
       "  'authorId': '2801949',\n",
       "  'authorName': 'Aishwarya Agrawal',\n",
       "  'abstract': 'In the last few years, there has been an increased interest in building multimodal (vision-language) models that are pretrained on larger but noisier datasets where the two modalities (e.g., image and text) loosely correspond to each other (e.g., Lu et al., 2019; Radford et al., 2021). Given a task (such as visual question answering), these models are then often fine-tuned on task-specific supervised datasets. (e.g., Lu et al., 2019; Chen et al.,2020; Tan and Bansal, 2019; Li et al., 2020a,b). In addition to the larger pretraining datasets, the transformer architecture (Vaswani et al., 2017) and in particular self-attention applied to two modalities are responsible for the impressive performance of the recent pretrained models on downstream tasks (Hendricks et al., 2021). In this tutorial, we focus on recent vision-language pretraining paradigms. Our goal is to first provide the background on image–language datasets, benchmarks, and modeling innovations before the multimodal pretraining area. Next we discuss the different family of models used for vision-language pretraining, highlighting their strengths and shortcomings. Finally, we discuss the limits of vision-language pretraining through statistical learning, and the need for alternative approaches such as causal representation learning.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '6690164c2c0c65b246df1f0af6bb4b21476303fc',\n",
       "  'title': 'TWEETSUM: Event oriented Social Summarization Dataset',\n",
       "  'authorId': '1724097',\n",
       "  'authorName': 'Ruifang He',\n",
       "  'abstract': 'With social media becoming popular, a vast of short and noisy messages are produced by millions of users when a hot event happens. Developing social summarization systems becomes more and more critical for people to quickly grasp core and essential information. However, the publicly available and high-quality large scale social summarization dataset is rare. Constructing such corpus is not easy and very expensive since short texts have very complex social characteristics. In this paper, we construct TWEETSUM, a new event-oriented dataset for social summarization. The original data is collected from twitter and contains 12 real world hot events with a total of 44,034 tweets and 11,240 users. Each event has four expert summaries, and we also have the annotation quality evaluation. In addition, we collect additional social signals (i.e. user relations, hashtags and user profiles) and further establish user relation network for each event. Besides the detailed dataset description, we show the performance of several typical extractive summarization methods on TWEETSUM to establish baselines. For further researches, we will release this dataset to the public.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '58ed96cd372732937859168fcf92c1395d049c1e',\n",
       "  'title': 'Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection',\n",
       "  'authorId': '34086979',\n",
       "  'authorName': 'Giovanni Da San Martino',\n",
       "  'abstract': 'We present the shared task on Fine-Grained Propaganda Detection, which was organized as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were two subtasks. FLC is a fragment-level task that asks for the identification of propagandist text fragments in a news article and also for the prediction of the specific propaganda technique used in each such fragment (18-way classification task). SLC is a sentence-level binary classification task asking to detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at http://propaganda.qcri.org/nlp4if-shared-task/.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '0d0522810ea580e76ecba16e9ba59913aa829197',\n",
       "  'title': 'Learning Bilingual Sentiment-Specific Word Embeddings without Cross-lingual Supervision',\n",
       "  'authorId': '2115389088',\n",
       "  'authorName': 'Yanlin Feng',\n",
       "  'abstract': 'Word embeddings learned in two languages can be mapped to a common space to produce Bilingual Word Embeddings (BWE). Unsupervised BWE methods learn such a mapping without any parallel data. However, these methods are mainly evaluated on tasks of word translation or word similarity. We show that these methods fail to capture the sentiment information and do not perform well enough on cross-lingual sentiment analysis. In this work, we propose UBiSE (Unsupervised Bilingual Sentiment Embeddings), which learns sentiment-specific word representations for two languages in a common space without any cross-lingual supervision. Our method only requires a sentiment corpus in the source language and pretrained monolingual word embeddings of both languages. We evaluate our method on three language pairs for cross-lingual sentiment analysis. Experimental results show that our method outperforms previous unsupervised BWE methods and even supervised BWE methods. Our method succeeds for a distant language pair English-Basque.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '1dfae23c8825e5afcf1c422bbad950d4b9f6fbcd',\n",
       "  'title': 'Structure-Sharing in Lexical Representation',\n",
       "  'authorId': '3209288',\n",
       "  'authorName': 'D. Flickinger',\n",
       "  'abstract': \"The lexicon now plays a central role in our implementation of a Head-driven Phrase Structure Grammar (HPSG), given the massive relocation into the lexicon of linguistic information that was carried by the phrase structure rules in the old GPSG system. HPSG's grammar contains fewer than twenty (very general) rules; its predecessor required over 350 to achieve roughly the same coverage. This simplification of the grammar is made possible by an enrichment of the structure and content of lexical entries, using both inheritance mechanisms and lexical rules to represent the linguistic information in a general and efficient form. We will argue that our mechanisms for structure-sharing not only provide the ability to express important linguistic generalization about the lexicon, but also make possible an efficient, readily modifiable implementation that we find quite adequate for continuing development of a large natural language system.\",\n",
       "  'year': 1985,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '0ba00f9835803ab7511f96cf0f3225489edad0e6',\n",
       "  'title': 'CIRCSIM-Tutor: An Intelligent Tutoring System Using Natural Language Dialogue',\n",
       "  'authorId': '2186744',\n",
       "  'authorName': 'M. Evens',\n",
       "  'abstract': 'CIRCSIM-Tutor version 2, a dialogue-based intelligent tutoring system (ITS), is nearly five years old. It conducts a conversation with a student to help the student learn to solve a class of problems in cardiovascular physiology dealing with the regulation of blood pressure. It uses natural language for both input and output, and can handle a variety of syntactic constructions and lexical items, including sentence fragments and misspelled words.',\n",
       "  'year': 1997,\n",
       "  'venue': 'ANLP'},\n",
       " {'paperId': '00cc08c90bc4ae7d3523e4dad2ca3a8fafc8501a',\n",
       "  'title': 'Embeddings for Word Sense Disambiguation: An Evaluation Study',\n",
       "  'authorId': '2676143',\n",
       "  'authorName': 'Ignacio Iacobacci',\n",
       "  'abstract': 'Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture semantic information from massive amounts of textual content. As a result, many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models. In this work, we study how word embeddings can be used in Word Sense Disambiguation, one of the oldest tasks in Natural Language Processing and Artificial Intelligence. We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture, and perform a deep analysis of how different parameters affect performance. We show how a WSD system that makes use of word embeddings alone, if designed properly, can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '643ea30d0e488e216dd32f4bffb8476da0ac9355',\n",
       "  'title': 'A Comparison of Head Transducers and Transfer for a Limited Domain Translation Application',\n",
       "  'authorId': '1767307',\n",
       "  'authorName': 'H. Alshawi',\n",
       "  'abstract': 'We compare the effectiveness of two related machine translation models applied to the same limited-domain task. One is a transfer model with monolingual head automata for analysis and generation; the other is a direct transduction model based on bilingual head transducers. We conclude that the head transducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort.',\n",
       "  'year': 1997,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e88ea5108f4bb9c596ee359994fcc94158c3b101',\n",
       "  'title': 'Unsupervised Knowledge Selection for Dialogue Generation',\n",
       "  'authorId': '30917866',\n",
       "  'authorName': 'Xiuyi Chen',\n",
       "  'abstract': 'Knowledge selection is an important and challenging task which could provide the appropriate knowledge for informative dialogue generation. However, the needed gold knowledge label is difficult to collect in reality. In this paper, we study knowledge selection for dialogue generation in the unsupervised scenario and propose a novel Distilled Distant Supervision Loss (DDSL) to supervise knowledge selection when the gold knowledge label is unknown. Specifically, we first obtain an oracle knowledge label via distant supervision and then leverage knowledge distillation to alleviate the noisy labeling problem of distant supervision. Furthermore, we propose a pretraining-finetuning strategy to deal with the mismatch knowledge selection problem that models tend to select the mismatched knowledge for dialogue generation in the unsupervised setting and will cause the degeneration of knowledge-aware decoder. Experiments on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '7145dae1bfa95c614b6d0ff8338a58c56b31405e',\n",
       "  'title': 'Bag-of-Words as Target for Neural Machine Translation',\n",
       "  'authorId': '8093340',\n",
       "  'authorName': 'Shuming Ma',\n",
       "  'abstract': 'A sentence can be translated into more than one correct sentences. However, most of the existing neural machine translation models only use one of the correct translations as the targets, and the other correct sentences are punished as the incorrect sentences in the training stage. Since most of the correct translations for one sentence share the similar bag-of-words, it is possible to distinguish the correct translations from the incorrect ones by the bag-of-words. In this paper, we propose an approach that uses both the sentences and the bag-of-words as targets in the training stage, in order to encourage the model to generate the potentially correct sentences that are not appeared in the training set. We evaluate our model on a Chinese-English translation dataset, and experiments show our model outperforms the strong baselines by the BLEU score of 4.55.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e174d2d7401087175d35342d78504386127e26b4',\n",
       "  'title': 'Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction',\n",
       "  'authorId': '2848048',\n",
       "  'authorName': 'Jekaterina Novikova',\n",
       "  'abstract': 'Recognition of social signals, coming from human facial expressions or prosody of human speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users’ impressions of the robot after a conversation. We find that happiness in the user’s recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that the facial expression emotional features and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learning-based adaptive human-robot dialogue systems.',\n",
       "  'year': 2017,\n",
       "  'venue': 'RoboNLP@ACL'},\n",
       " {'paperId': '732a2f3532e41e05465e2b3a9b5eebc2ceb483c1',\n",
       "  'title': 'Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques',\n",
       "  'authorId': '38716503',\n",
       "  'authorName': 'Kundan Krishna',\n",
       "  'abstract': 'Following each patient visit, physicians draft long semi-structured clinical summaries called SOAP notes. While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout. In this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. After exploring a spectrum of methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an algorithm that (i) extracts important utterances relevant to each summary section; (ii) clusters together related utterances; and then (iii) generates one summary sentence per cluster. Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators. For reproducibility, we demonstrate similar benefits on the publicly available AMI dataset. Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'febd7b2917eb7c372dfec325a396d5ddc77916a6',\n",
       "  'title': 'A Study on Efficiency, Accuracy and Document Structure for Answer Sentence Selection',\n",
       "  'authorId': '3457102',\n",
       "  'authorName': 'Daniele Bonadiman',\n",
       "  'abstract': 'An essential task of most Question Answering (QA) systems is to re-rank the set of answer candidates, i.e., Answer Sentence Selection (AS2). These candidates are typically sentences either extracted from one or more documents preserving their natural order or retrieved by a search engine. Most state-of-the-art approaches to the task use huge neural models, such as BERT, or complex attentive architectures. In this paper, we argue that by exploiting the intrinsic structure of the original rank together with an effective word-relatedness encoder, we achieve the highest accuracy among the cost-efficient models, with two orders of magnitude fewer parameters than the current state of the art. Our model takes 9.5 seconds to train on the WikiQA dataset, i.e., very fast in comparison with the 18 minutes required by a standard BERT-base fine-tuning.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '205b9f4891a2ead886604f161a44b3aed483609a',\n",
       "  'title': 'Conditional Structure versus Conditional Estimation in NLP Models',\n",
       "  'authorId': '38666915',\n",
       "  'authorName': 'D. Klein',\n",
       "  'abstract': 'This paper separates conditional parameter estimation, which consistently raises test set accuracy on statistical NLP tasks, from conditional model structures, such as the conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy. Error analysis on part-of-speech tagging shows that the actual tagging errors made by the conditionally structured model derive not only from label bias, but also from other ways in which the independence assumptions of the conditional model structure are unsuited to linguistic sequences. The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work.',\n",
       "  'year': 2002,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '567a5d8e6731d6cfebc1a7f6b42eb889cc028ec0',\n",
       "  'title': 'Book Review: Design Patterns in Fluid Construction Grammar edited by Luc Steels',\n",
       "  'authorId': '145254207',\n",
       "  'authorName': 'Nathan Schneider',\n",
       "  'abstract': 'In computational modeling of natural language phenomena, there are at least three modes of research. The currently dominant statistical paradigm typically prioritizes instance coverage: Data-driven methods seek to use as much information observed in data as possible in order to generalize linguistic analyses to unseen instances. A second approach prioritizes detailed description of grammatical phenomena, that is, forming and defending theories with a focus on a small number of instances. A third approach might be called integrative: Rather than addressing phenomena in isolation, different approaches are brought together to address multiple challenges in a unified framework, and the behavior of the system is demonstrated with a small number of instances. Design Patterns in Fluid Construction Grammar (DPFCG) exemplifies the third approach, introducing a linguistic formalism called Fluid Construction Grammar (FCG) that addresses parsing, production, and learning in a single computational framework. The book emphasizes grammar-engineering, following broad-coverage descriptive paradigms that can be traced back to Generalized Phrase Structure Grammar In all of these cases, a formal meta-framework allows computational linguists to formalize their hypotheses and intuitions about a language\\'s grammatical behavior and then explore how these representational choices affect the processing of natural language utterances. Many of the aforemen-tioned approaches have engendered large-scale platforms that can be used and reused to provide formal description of grammars for different languages, such as Par-Gram for LFG (Butt et al. 2002) and the LinGO Grammar Matrix for HPSG (Bender, Flickinger, and Oepen 2002). FCG offers a similar grammar engineering framework that follows the principles of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG treats constructions as the basic units of grammatical organization in language. The constructions are viewed as learned associations between form (e.g., sounds, morphemes, syntactic phrases) and function (semantics, pragmatics, discourse meaning, etc.). CxG does not impose a strict separation between lexicon and grammar—indeed, it is perhaps best known as treating semi-productive idioms like \" the X-er, the Y-er \" and \" X let alone Y \" on equal footing with lexemes and \" core \" syntactic patterns (Fillmore,',\n",
       "  'year': 2013,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': 'c3f91f90f45e935f8ec369172584ea69c6b405c7',\n",
       "  'title': 'COMMIT at SemEval-2016 Task 5: Sentiment Analysis with Rhetorical Structure Theory',\n",
       "  'authorId': '2339044',\n",
       "  'authorName': 'K. Schouten',\n",
       "  'abstract': 'This paper reports our submission to the Aspect-Based Sentiment Analysis task of SemEval 2016. It covers the prediction of sentiment for a given set of aspects (e.g., subtask 1, slot 2) for the English language using discourse analysis. To that end, a discourse parser implementing the Rhetorical Structure Theory is employed and the resulting information is used to determine the context of each aspect, as well as to compute the expressed sentiment in that context by weighing the discourse relations between words. While discourse analysis yields high level linguistic information that can be used to better predict sentiment, the proposed algorithm does not yet stack up to the high-performing machine learning approaches that are commonly exploited for this task.',\n",
       "  'year': 2016,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'eb0024439858af7cc951ce2efa5a6533c3781799',\n",
       "  'title': 'WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models',\n",
       "  'authorId': '2090357303',\n",
       "  'authorName': 'Benjamin Minixhofer',\n",
       "  'abstract': 'Large pretrained language models (LMs) have become the central building block of many NLP applications. Training these models requires ever more computational resources and most of the existing models are trained on English text only. It is exceedingly expensive to train these models in other languages. To alleviate this problem, we introduce a novel method – called WECHSEL – to efficiently and effectively transfer pretrained LMs to new languages. WECHSEL can be applied to any model which uses subword-based tokenization and learns an embedding for each subword. The tokenizer of the source model (in English) is replaced with a tokenizer in the target language and token embeddings are initialized such that they are semantically similar to the English tokens by utilizing multilingual static word embeddings covering English and the target language. We use WECHSEL to transfer the English RoBERTa and GPT-2 models to four languages (French, German, Chinese and Swahili). We also study the benefits of our method on very low-resource languages. WECHSEL improves over proposed methods for cross-lingual parameter transfer and outperforms models of comparable size trained from scratch with up to 64x less training effort. Our method makes training large language models for new languages more accessible and less damaging to the environment. We make our code and models publicly available.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '30bbf8871b84b8e51f14a2ccf0e3a54386e2c05b',\n",
       "  'title': 'ECNU: Expression- and Message-level Sentiment Orientation Classification in Twitter Using Multiple Effective Features',\n",
       "  'authorId': '2130418538',\n",
       "  'authorName': 'Jiang Zhao',\n",
       "  'abstract': 'Microblogging websites (such as Twitter, Facebook) are rich sources of data for opinion mining and sentiment analysis. In this paper, we describe our approaches used for sentiment analysis in twitter (task 9) organized in SemEval 2014. This task tries to determine whether the sentiment orientations conveyed by the whole tweets or pieces of tweets are positive, negative or neutral. To solve this problem, we extracted several simple and basic features considering the following aspects: surface text, syntax, sentiment score and twitter characteristic. Then we exploited these features to build a classifier using SVM algorithm. Despite the simplicity of features, our systems rank above the average.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'ac41d132999661f6b59db46a28086d4b6707654a',\n",
       "  'title': 'Diagnosing Vision-and-Language Navigation: What Really Matters',\n",
       "  'authorId': '51439692',\n",
       "  'authorName': 'Wanrong Zhu',\n",
       "  'abstract': 'Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machines’ performance and human benchmarks. Moreover, the agents’ inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how the agents perceive the multimodal input is under-studied and needs investigation. In this work, we conduct a series of diagnostic experiments to unveil agents’ focus during navigation. Results show that indoor navigation agents refer to both object and direction tokens when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and poorly understand the object tokens. Transformer-based agents acquire a better cross-modal understanding of objects and display strong numerical reasoning ability than non-Transformer-based agents. When it comes to vision-and-language alignments, many models claim that they can align object tokens with specific visual targets. We find unbalanced attention on the vision and text input and doubt the reliability of such cross-modal alignments.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '3fcf6389cd09ce5235aede81e131cc6555cdca83',\n",
       "  'title': 'Active Learning for Coreference Resolution using Discrete Annotation',\n",
       "  'authorId': '46708422',\n",
       "  'authorName': 'Belinda Z. Li',\n",
       "  'abstract': 'We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget. In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour. Future work can use our annotation protocol to effectively develop coreference models for new domains. Our code is publicly available.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1420aa3a7722e9fdad5fb2ea68a326a794ccf0fa',\n",
       "  'title': 'Author Commitment and Social Power: Automatic Belief Tagging to Infer the Social Context of Interactions',\n",
       "  'authorId': '3331141',\n",
       "  'authorName': 'Vinodkumar Prabhakaran',\n",
       "  'abstract': 'Understanding how social power structures affect the way we interact with one another is of great interest to social scientists who want to answer fundamental questions about human behavior, as well as to computer scientists who want to build automatic methods to infer the social contexts of interactions. In this paper, we employ advancements in extra-propositional semantics extraction within NLP to study how author commitment reflects the social context of an interactions. Specifically, we investigate whether the level of commitment expressed by individuals in an organizational interaction reflects the hierarchical power structures they are part of. We find that subordinates use significantly more instances of non-commitment than superiors. More importantly, we also find that subordinates attribute propositions to other agents more often than superiors do — an aspect that has not been studied before. Finally, we show that enriching lexical features with commitment labels captures important distinctions in social meanings.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'd74d92b130669536458289fb5f3d2526d3340c6a',\n",
       "  'title': 'IIT(BHU)–IIITH at CoNLL–SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection',\n",
       "  'authorId': '1785252',\n",
       "  'authorName': 'Abhishek Sharma',\n",
       "  'abstract': 'This paper describes the systems submitted by IIT (BHU), Varanasi/IIIT Hyderabad (IITBHU–IIITH) for Task 1 of CoNLL– SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection (Cotterell et al., 2018). The task is to generate the inflected form given a lemma and set of morphological features. The systems are evaluated on over 100 distinct languages and three different resource settings (low, medium and high). We formulate the task as a sequence to sequence learning problem. As most of the characters in inflected form are copied from the lemma, we use Pointer-Generator Network (See et al., 2017) which makes it easier for the system to copy characters from the lemma. PointerGenerator Network also helps in dealing with out-of-vocabulary characters during inference. Our best performing system stood 4th among 28 systems, 3rd among 23 systems and 4th among 23 systems for the low, medium and high resource setting respectively.',\n",
       "  'year': 2018,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': 'ea3e18c7b10a137d495054682c055a80b5be768c',\n",
       "  'title': 'Findings of the 2019 Conference on Machine Translation (WMT19)',\n",
       "  'authorId': '2934336',\n",
       "  'authorName': 'Loïc Barrault',\n",
       "  'abstract': 'This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.',\n",
       "  'year': 2019,\n",
       "  'venue': 'WMT'},\n",
       " {'paperId': '5082a1a13daea5c7026706738f8528391a1e6d59',\n",
       "  'title': 'A Neural Attention Model for Abstractive Sentence Summarization',\n",
       "  'authorId': '2531268',\n",
       "  'authorName': 'Alexander M. Rush',\n",
       "  'abstract': 'Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '3d22f972448a2336677ae6ff2877fae010c7dfa2',\n",
       "  'title': 'What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?',\n",
       "  'authorId': '2066004861',\n",
       "  'authorName': 'Marc Tanti',\n",
       "  'abstract': 'Image captioning has evolved into a core task for Natural Language Generation and has also proved to be an important testbed for deep learning approaches to handling multimodal representations. Most contemporary approaches rely on a combination of a convolutional network to handle image features, and a recurrent network to encode linguistic information. The latter is typically viewed as the primary “generation” component. Beyond this high-level characterisation, a CNN+RNN model supports a variety of architectural designs. The dominant model in the literature is one in which visual features encoded by a CNN are “injected” as part of the linguistic encoding process, driving the RNN’s linguistic choices. By contrast, it is possible to envisage an architecture in which visual and linguistic features are encoded separately, and merged at a subsequent stage. In this paper, we address two related questions: (1) Is direct injection the best way of combining multimodal information, or is a late merging alternative better for the image captioning task? (2) To what extent should a recurrent network be viewed as actually generating, rather than simply encoding, linguistic information?',\n",
       "  'year': 2017,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': '1f0e1657063ea38cf225eaf1c1187ae7b2e4a0e0',\n",
       "  'title': 'Increasing Robustness to Spurious Correlations using Forgettable Examples',\n",
       "  'authorId': '3261470',\n",
       "  'authorName': 'Yadollah Yaghoobzadeh',\n",
       "  'abstract': 'Neural NLP models tend to rely on spurious correlations between labels and input features to perform their tasks. Minority examples, i.e., examples that contradict the spurious correlations present in the majority of data points, have been shown to increase the out-of-distribution generalization of pre-trained language models. In this paper, we first propose using example forgetting to find minority examples without prior knowledge of the spurious correlations present in the dataset. Forgettable examples are instances either learned and then forgotten during training or never learned. We show empirically how these examples are related to minorities in our training sets. Then, we introduce a new approach to robustify models by fine-tuning our models twice, first on the full training data and second on the minorities only. We obtain substantial improvements in out-of-distribution generalization when applying our approach to the MNLI, QQP and FEVER datasets.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': 'f0a498014c4ef67c0b72ceb18d95e0d25087fd57',\n",
       "  'title': 'Neural Machine Translation via Binary Code Prediction',\n",
       "  'authorId': '143800179',\n",
       "  'authorName': 'Yusuke Oda',\n",
       "  'abstract': 'In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.',\n",
       "  'year': 2017,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'b02df8174726c42938fe9dba52658eeadc6e07e4',\n",
       "  'title': 'Building a Large Knowledge Base for a Natural Language System',\n",
       "  'authorId': '3187096',\n",
       "  'authorName': 'Jerry R. Hobbs',\n",
       "  'abstract': 'A sophisticated natural language system requires a large knowledge base. A methodology is described for constructing one in a principled way. Facts are selected for the knowledge base by determining what facts are linguistically presupposed by a text in the domain of interest. The facts are sorted into clusters, and within each cluster they are organized according to their logical dependencies. Finally, the facts are encoded as predicate calculus axioms.',\n",
       "  'year': 1984,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '323d8e106f7e806ef362eecb432afb3b31d07a05',\n",
       "  'title': 'Weakly supervised learning of allomorphy',\n",
       "  'authorId': '2614548',\n",
       "  'authorName': 'Miikka Silfverberg',\n",
       "  'abstract': 'Most NLP resources that offer annotations at the word segment level provide morphological annotation that includes features indicating tense, aspect, modality, gender, case, and other inflectional information. Such information is rarely aligned to the relevant parts of the words—i.e. the allomorphs, as such annotation would be very costly. These unaligned weak labelings are commonly provided by annotated NLP corpora such as treebanks in various languages. Although they lack alignment information, the presence/absence of labels at the word level is also consistent with the amount of supervision assumed to be provided to L1 and L2 learners. In this paper, we explore several methods to learn this latent alignment between parts of word forms and the grammatical information provided. All the methods under investigation favor hypotheses regarding allomorphs of morphemes that re-use a small inventory, i.e. implicitly minimize the number of allomorphs that a morpheme can be realized as. We show that the provided information offers a significant advantage for both word segmentation and the learning of allomorphy.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SWCN@EMNLP'},\n",
       " {'paperId': 'e191b7eca9cf259abab3905dab1a54d1be091a7a',\n",
       "  'title': 'Designing a Symbolic Intermediate Representation for Neural Surface Realization',\n",
       "  'authorId': '40895152',\n",
       "  'authorName': 'H. Elder',\n",
       "  'abstract': 'Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction. This work focuses on designing a symbolic intermediate representation to be used in multi-stage neural generation with the intention of reducing the frequency of failed outputs. We show that surface realization from this intermediate representation is of high quality and when the full system is applied to the E2E dataset it outperforms the winner of the E2E challenge. Furthermore, by breaking out the surface realization step from typically end-to-end neural systems, we also provide a framework for non-neural based content selection and planning systems to potentially take advantage of semi-supervised pretraining of neural surface realization models.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation'},\n",
       " {'paperId': '67ab35563a9deb7cf6fb2bbaae55bef28e238601',\n",
       "  'title': 'Multi-Task Supervised Pretraining for Neural Domain Adaptation',\n",
       "  'authorId': '38351132',\n",
       "  'authorName': 'Sara Meftah',\n",
       "  'abstract': 'Two prevalent transfer learning approaches are used in recent works to improve neural networks performance for domains with small amounts of annotated data: Multi-task learning which involves training the task of interest with related auxiliary tasks to exploit their underlying similarities, and Mono-task fine-tuning, where the weights of the model are initialized with the pretrained weights of a large-scale labeled source domain and then fine-tuned with labeled data of the target domain (domain of interest). In this paper, we propose a new approach which takes advantage from both approaches by learning a hierarchical model trained across multiple tasks from a source domain, and is then fine-tuned on multiple tasks of the target domain. Our experiments on four tasks applied to the social media domain show that our proposed approach leads to significant improvements on all tasks compared to both approaches.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SOCIALNLP'},\n",
       " {'paperId': '917c63f2186119166b3379f5d2816bb1a2f39b09',\n",
       "  'title': 'DEMix Layers: Disentangling Domains for Modular Language Modeling',\n",
       "  'authorId': '40895369',\n",
       "  'authorName': 'Suchin Gururangan',\n",
       "  'abstract': 'We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'ac5637276be614f5c89bd8fe2ed82f412568eba1',\n",
       "  'title': 'UIO-Lien: Entailment Recognition using Minimal Recursion Semantics',\n",
       "  'authorId': '5742176',\n",
       "  'authorName': 'Elisabeth Lien',\n",
       "  'abstract': 'In this paper we present our participation in the Semeval 2014 task “Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment”. Our results demonstrate that using generic tools for semantic analysis is a viable option for a system that recognizes textual entailment. The invested effort in developing such tools allows us to build systems for reasoning that do not require training.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '20b506a82d2a36a9badf26a2d42a88d6700992cb',\n",
       "  'title': 'NILC at CWI 2018: Exploring Feature Engineering and Feature Learning',\n",
       "  'authorId': '145188798',\n",
       "  'authorName': 'N. Hartmann',\n",
       "  'abstract': 'This paper describes the results of NILC team at CWI 2018. We developed solutions following three approaches: (i) a feature engineering method using lexical, n-gram and psycholinguistic features, (ii) a shallow neural network method using only word embeddings, and (iii) a Long Short-Term Memory (LSTM) language model, which is pre-trained on a large text corpus to produce a contextualized word vector. The feature engineering method obtained our best results for the classification task and the LSTM model achieved the best results for the probabilistic classification task. Our results show that deep neural networks are able to perform as well as traditional machine learning methods using manually engineered features for the task of complex word identification in English.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': '14bf3c2241a9d3ce27cceff9542c7c36638fcaea',\n",
       "  'title': 'Combining Segmenter and Chunker for Chinese Word Segmentation',\n",
       "  'authorId': '1749558',\n",
       "  'authorName': 'Masayuki Asahara',\n",
       "  'abstract': 'Our proposed method is to use a Hidden Markov Model-based word segmenter and a Support Vector Machine-based chunker for Chinese word segmentation. Firstly, input sentences are analyzed by the Hidden Markov Model-based word segmenter. The word segmenter produces n-best word candidates together with some class information and confidence measures. Secondly, the extracted words are broken into character units and each character is annotated with the possible word class and the position in the word, which are then used as the features for the chunker. Finally, the Support Vector Machine-based chunker brings character units together into words so as to determine the word boundaries.',\n",
       "  'year': 2003,\n",
       "  'venue': 'SIGHAN'},\n",
       " {'paperId': 'c272e9a3968d59fa15fe4ea3e09d012da876d571',\n",
       "  'title': 'The Chinese Remainder Theorem for Compact, Task-Precise, Efficient and Secure Word Embeddings',\n",
       "  'authorId': '3401712',\n",
       "  'authorName': 'Patricia Thaine',\n",
       "  'abstract': 'The growing availability of powerful mobile devices and other edge devices, together with increasing regulatory and security concerns about the exchange of personal information across networks of these devices has challenged the Computational Linguistics community to develop methods that are at once fast, space-efficient, accurate and amenable to secure encoding schemes such as homomorphic encryption. Inspired by recent work that restricts floating point precision to speed up neural network training in hardware-based SIMD, we have developed a method for compressing word vector embeddings into integers using the Chinese Reminder Theorem that speeds up addition by up to 48.27% and at the same time compresses GloVe word embedding libraries by up to 25.86%. We explore the practicality of this simple approach by investigating the trade-off between precision and performance in two NLP tasks: compositional semantic relatedness and opinion target sentiment classification. We find that in both tasks, lowering floating point number precision results in negligible changes to performance.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '7fb7552df9870eae0df495636f54d0a773b660e9',\n",
       "  'title': 'Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data',\n",
       "  'authorId': '1747849',\n",
       "  'authorName': 'Ivan Vulic',\n",
       "  'abstract': 'We propose the first probabilistic approach to modeling cross-lingual semantic similarity (CLSS) in context which requires only comparable data. The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts (e.g., crosslingual topics obtained by a multilingual topic model). These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources. Word meaning is represented as a probability distribution over the latent concepts, and a change in meaning is represented as a change in the distribution over these latent concepts. We present new models that modulate the isolated out-ofcontext word representations with contextual knowledge. Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of crosslingual semantic similarity.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '33c831326bb47b2ba2031fd7213b6918d23eb01e',\n",
       "  'title': 'The Impact of Positional Encodings on Multilingual Compression',\n",
       "  'authorId': '24881798',\n",
       "  'authorName': 'Vinit Ravishankar',\n",
       "  'abstract': 'In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs. We first show that surprisingly, while these modifications tend to improve monolingual language models, none of them result in better multilingual language models. We then answer why that is: sinusoidal encodings were explicitly designed to facilitate compositionality by allowing linear projections over arbitrary time steps. Higher variances in multilingual training distributions requires higher compression, in which case, compositionality becomes indispensable. Learned absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal embeddings in multilingual settings, but more complex positional encoding architectures lack the inductive bias to effectively learn cross-lingual alignment. In other words, while sinusoidal positional encodings were designed for monolingual applications, they are particularly useful in multilingual language models.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5b0a0aff06999e36d0908dcd1375530d4e0ab6ea',\n",
       "  'title': 'Bootstrapping a Romanian Corpus for Medical Named Entity Recognition',\n",
       "  'authorId': '31546117',\n",
       "  'authorName': 'Maria Mitrofan',\n",
       "  'abstract': 'Named Entity Recognition (NER) is an important component of natural language processing (NLP), with applicability in the biomedical domain, enabling knowledge discovery from medical texts. Due to the fact that for the Romanian language there are only a few linguistic resources specific to the biomedical domain, we have created a sub-corpus specific to this domain. In this paper we present a newly developed Romanian sub-corpus for medical domain NER, which is a valuable asset for the field of biomedical text processing. We provide a description of the sub-corpus, statistics about data-composition and we evaluate an automatic NER tool on the newly created resource.',\n",
       "  'year': 2017,\n",
       "  'venue': 'RANLP'},\n",
       " {'paperId': '8b25ca2fcceb3ad47e3a552d122e25c841088676',\n",
       "  'title': 'WSD-games: a Game-Theoretic Algorithm for Unsupervised Word Sense Disambiguation',\n",
       "  'authorId': '2124993',\n",
       "  'authorName': 'Rocco Tripodi',\n",
       "  'abstract': 'In this paper we present an unsupervised approach to word sense disambiguation based on evolutionary game theory. In our algorithm each word to be disambiguated is represented as a node on a graph and each sense as a class. The algorithm performs a consistent class assignment of senses according to the similarity information of each word with the others, so that similar words are constrained to similar classes. The dynamics of the system are formulated in terms of a non-cooperative multiplayer game, where the players are the data points to decide their class memberships and equilibria correspond to consistent labeling of the data.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '7b8e1f4b6686bd771f3b9498dc0729c655058796',\n",
       "  'title': 'DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion',\n",
       "  'authorId': '2765914',\n",
       "  'authorName': 'Zhen Han',\n",
       "  'abstract': 'There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose Dy- ERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'ead4e2a3a4d3b32458645d0da4ee369b4858432f',\n",
       "  'title': 'Information Status Distinctions and Referring Expressions: An Empirical Study of References to People in News Summaries',\n",
       "  'authorId': '1722590',\n",
       "  'authorName': 'Advaith Siddharthan',\n",
       "  'abstract': 'Although there has been much theoretical work on using various information status distinctions to explain the form of references in written text, there have been few studies that attempt to automatically learn these distinctions for generating references in the context of computer-regenerated text. In this article, we present a model for generating references to people in news summaries that incorporates insights from both theory and a corpus analysis of human written summaries. In particular, our model captures how two properties of a person referred to in the summary—familiarity to the reader and global salience in the news story—affect the content and form of the initial reference to that person in a summary. We demonstrate that these two distinctions can be learned from a typical input for multi-document summarization and that they can be used to make regeneration decisions that improve the quality of extractive summaries.',\n",
       "  'year': 2011,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': '379e9d5bba8a617b3114bd5b562b14aa6abc5282',\n",
       "  'title': 'Natural SQL: Making SQL Easier to Infer from Natural Language Specifications',\n",
       "  'authorId': '120624606',\n",
       "  'authorName': 'Yujian Gan',\n",
       "  'abstract': 'Addressing the mismatch between natural language descriptions and the corresponding SQL queries is a key challenge for text-to-SQL translation. To bridge this gap, we propose an SQL intermediate representation (IR) called Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities of SQL, while it simplifies the queries as follows: (1) dispensing with operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are usually hard to find counterparts for in the text descriptions; (2) removing the need for nested subqueries and set operators; and (3) making schema linking easier by reducing the required number of schema items. On Spider, a challenging textto-SQL benchmark that contains complex and nested SQL queries, we demonstrate that NatSQL outperforms other IRs, and significantly improves the performance of several previous SOTA models. Furthermore, for existing models that do not support executable SQL generation, NatSQL easily enables them to generate executable SQL queries, and achieves the new state-of-the-art execution accuracy 1.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '5e35895fc4731858f0b286cb5a1613a819cc2367',\n",
       "  'title': 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text',\n",
       "  'authorId': '40910779',\n",
       "  'authorName': 'Koustuv Sinha',\n",
       "  'abstract': 'The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by the classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model’s ability for systematic generalization by evaluating on held-out combinations of logical rules, and allows us to evaluate a model’s robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs—with the graph-based model exhibiting both stronger generalization and greater robustness.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '9ee37a0be298149f0e5a1c9b5ac8188411811823',\n",
       "  'title': 'Simultaneous Interpretation Utilizing Example-based Incremental Transfer',\n",
       "  'authorId': '35227506',\n",
       "  'authorName': 'Hideki Mima',\n",
       "  'abstract': 'This paper describes a practical method of automatic simultaneous interpretation utilizing an example-based incremental transfer mechanism. We primarily show how incremental translation is achieved in the context of an example-based framework. We then examine the type of translation examples required for a simultaneous interpretation to create naturally communicative dialogs. Finally, we propose a scheme for automatic simultaneous interpretation exploiting this example-based incremental translation mechanism. Preliminary experimentation analyzing the performance of our example-based incremental translation mechanism leads us to believe that the proposed scheme can be utilized to achieve a practical simultaneous interpretation system.',\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '7ee7e580798e9177dc8ff9bb94c895cd3d4ab780',\n",
       "  'title': 'Annotation and Classification of Sentence-level Revision Improvement',\n",
       "  'authorId': '2501945',\n",
       "  'authorName': 'T. Afrin',\n",
       "  'abstract': 'Studies of writing revisions rarely focus on revision quality. To address this issue, we introduce a corpus of between-draft revisions of student argumentative essays, annotated as to whether each revision improves essay quality. We demonstrate a potential usage of our annotations by developing a machine learning model to predict revision improvement. With the goal of expanding training data, we also extract revisions from a dataset edited by expert proofreaders. Our results indicate that blending expert and non-expert revisions increases model performance, with expert data particularly important for predicting low-quality revisions.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': 'ad1cf7f34d394b5daaee3eea266b7ad44b4f46eb',\n",
       "  'title': 'Can Humor Prediction Datasets be used for Humor Generation? Humorous Headline Generation via Style Transfer',\n",
       "  'authorId': '47433471',\n",
       "  'authorName': 'Orion Weller',\n",
       "  'abstract': 'Understanding and identifying humor has been increasingly popular, as seen by the number of datasets created to study humor. However, one area of humor research, humor generation, has remained a difficult task, with machine generated jokes failing to match human-created humor. As many humor prediction datasets claim to aid in generative tasks, we examine whether these claims are true. We focus our experiments on the most popular dataset, included in the 2020 SemEval’s Task 7, and teach our model to take normal text and “translate” it into humorous text. We evaluate our model compared to humorous human generated headlines, finding that our model is preferred equally in A/B testing with the human edited versions, a strong success for humor generation, and is preferred over an intelligent random baseline 72% of the time. We also show that our model is assumed to be human written comparable with that of the human edited headlines and is significantly better than random, indicating that this dataset does indeed provide potential for future humor generation systems.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FIGLANG'},\n",
       " {'paperId': 'fea152fa2b8954be765bb10245b90e07de265fd9',\n",
       "  'title': 'The Effect of Negators, Modals, and Degree Adverbs on Sentiment Composition',\n",
       "  'authorId': '2886725',\n",
       "  'authorName': 'Svetlana Kiritchenko',\n",
       "  'abstract': 'Negators, modals, and degree adverbs can significantly affect the sentiment of the words they modify. Often, their impact is modeled with simple heuristics; although, recent work has shown that such heuristics do not capture the true sentiment of multi-word phrases. We created a dataset of phrases that include various negators, modals, and degree adverbs, as well as their combinations. Both the phrases and their constituent content words were annotated with real-valued scores of sentiment association. Using phrasal terms in the created dataset, we analyze the impact of individual modifiers and the average effect of the groups of modifiers on overall sentiment. We find that the effect of modifiers varies substantially among the members of the same group. Furthermore, each individual modifier can affect sentiment words in different ways. Therefore, solutions based on statistical learning seem more promising than fixed hand-crafted rules on the task of automatic sentiment prediction.',\n",
       "  'year': 2016,\n",
       "  'venue': 'WASSA@NAACL-HLT'},\n",
       " {'paperId': '56fcf886de43d431590c5617546f75c4c16f3ad4',\n",
       "  'title': 'EU-BRIDGE MT: Combined Machine Translation',\n",
       "  'authorId': '35307070',\n",
       "  'authorName': 'Markus Freitag',\n",
       "  'abstract': 'This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two European language pairs, German→English and English→German. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems.',\n",
       "  'year': 2014,\n",
       "  'venue': 'WMT@ACL'},\n",
       " {'paperId': '717a0f8badc5adaf60bbe96b0720bc10fe83fc22',\n",
       "  'title': 'Aligning English Strings with Abstract Meaning Representation Graphs',\n",
       "  'authorId': '3158273',\n",
       "  'authorName': 'Nima Pourdamghani',\n",
       "  'abstract': 'We align pairs of English sentences and corresponding Abstract Meaning Representations (AMR), at the token level. Such alignments will be useful for downstream extraction of semantic interpretation and generation rules. Our method involves linearizing AMR structures and performing symmetrized EM training. We obtain 86.5% and 83.1% alignment F score on development and test sets.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '1a7ca0620507645ce9f4baa1f5a7c42705d3ed74',\n",
       "  'title': 'Applying Occam’s Razor to Transformer-Based Dependency Parsing: What Works, What Doesn’t, and What is Really Necessary',\n",
       "  'authorId': '49948196',\n",
       "  'authorName': 'Stefan Grünewald',\n",
       "  'abstract': 'The introduction of pre-trained transformer-based contextualized word embeddings has led to considerable improvements in the accuracy of graph-based parsers for frameworks such as Universal Dependencies (UD). However, previous works differ in various dimensions, including their choice of pre-trained language models and whether they use LSTM layers. With the aims of disentangling the effects of these choices and identifying a simple yet widely applicable architecture, we introduce STEPS, a new modular graph-based dependency parser. Using STEPS, we perform a series of analyses on the UD corpora of a diverse set of languages. We find that the choice of pre-trained embeddings has by far the greatest impact on parser performance and identify XLM-R as a robust choice across the languages in our study. Adding LSTM layers provides no benefits when using transformer-based embeddings. A multi-task training setup outputting additional UD features may contort results. Taking these insights together, we propose a simple but widely applicable parser architecture and configuration, achieving new state-of-the-art results (in terms of LAS) for 10 out of 12 diverse languages.',\n",
       "  'year': 2020,\n",
       "  'venue': 'IWPT'},\n",
       " {'paperId': 'b9653d6eb79c142e81105bfa300735611c243b5a',\n",
       "  'title': 'Neural sequence modelling for learner error prediction',\n",
       "  'authorId': '144725847',\n",
       "  'authorName': 'Zheng Yuan',\n",
       "  'abstract': 'This paper describes our use of two recurrent neural network sequence models: sequence labelling and sequence-to-sequence models, for the prediction of future learner errors in our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We show that these two models capture complementary information as combining them improves performance. Furthermore, the same network architecture and group of features can be used directly to build competitive prediction models in all three language tracks, demonstrating that our approach generalises well across languages.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': '13107a42f6e4a0d9d50113f74f25885b88c0872a',\n",
       "  'title': 'Multi-grained Chinese Word Segmentation with Weakly Labeled Data',\n",
       "  'authorId': '2057184863',\n",
       "  'authorName': 'Chen Gong',\n",
       "  'abstract': 'In contrast with the traditional single-grained word segmentation (SWS), where a sentence corresponds to a single word sequence, multi-grained Chinese word segmentation (MWS) aims to segment a sentence into multiple word sequences to preserve all words of different granularities. Due to the lack of manually annotated MWS data, previous work train and tune MWS models only on automatically generated pseudo MWS data. In this work, we further take advantage of the rich word boundary information in existing SWS data and naturally annotated data from dictionary example (DictEx) sentences, to advance the state-of-the-art MWS model based on the idea of weak supervision. Particularly, we propose to accommodate two types of weakly labeled data for MWS, i.e., SWS data and DictEx data by employing a simple yet competitive graph-based parser with local loss. Besides, we manually annotate a high-quality MWS dataset according to our newly compiled annotation guideline, consisting of over 9,000 sentences from two types of texts, i.e., canonical newswire (NEWS) and non-canonical web (BAIKE) data for better evaluation. Detailed evaluation shows that our proposed model with weakly labeled data significantly outperforms the state-of-the-art MWS model by 1.12 and 5.97 on NEWS and BAIKE data in F1.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '2f8f7aac687886705c05bdf82e23036a1cc121c8',\n",
       "  'title': 'Edinburgh’s End-to-End Multilingual Speech Translation System for IWSLT 2021',\n",
       "  'authorId': '48335426',\n",
       "  'authorName': 'Biao Zhang',\n",
       "  'abstract': 'This paper describes Edinburgh’s submissions to the IWSLT2021 multilingual speech translation (ST) task. We aim at improving multilingual translation and zero-shot performance in the constrained setting (without using any extra training data) through methods that encourage transfer learning and larger capacity modeling with advanced neural components. We build our end-to-end multilingual ST model based on Transformer, integrating techniques including adaptive speech feature selection, language-specific modeling, multi-task learning, deep and big Transformer, sparsified linear attention and root mean square layer normalization. We adopt data augmentation using machine translation models for ST which converts the zero-shot problem into a zero-resource one. Experimental results show that these methods deliver substantial improvements, surpassing the official baseline by > 15 average BLEU and outperforming our cascading system by > 2 average BLEU. Our final submission achieves competitive performance (runner up).',\n",
       "  'year': 2021,\n",
       "  'venue': 'IWSLT'},\n",
       " {'paperId': '6780df84221b088b63ca45e81e7becea79732c88',\n",
       "  'title': 'Duluth: Word Sense Discrimination in the Service of Lexicography',\n",
       "  'authorId': '2001885',\n",
       "  'authorName': 'Ted Pedersen',\n",
       "  'abstract': 'This paper describes the Duluth systems that participated in Task 15 of SemEval 2015. The goal of the task was to automatically construct dictionary entries (via a series of three subtasks). Our systems participated in subtask 2, which involved automatically clustering the contexts in which a target word occurs into its different senses. Our results are consistent with previous word sense induction and discrimination findings, where it proves difficult to beat a baseline algorithm that assigns all instances of a target word to a single sense. However, our method of predicting the number of senses automatically fared quite well.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'c5be5422855e1e6cef989d10c9b30f1d318d3457',\n",
       "  'title': 'From Argumentation Mining to Stance Classification',\n",
       "  'authorId': '1690185',\n",
       "  'authorName': 'Parinaz Sobhani',\n",
       "  'abstract': 'Argumentation mining and stance classification were recently introduced as interesting tasks in text mining. In this paper, a novel framework for argument tagging based on topic modeling is proposed. Unlike other machine learning approaches for argument tagging which often require large set of labeled data, the proposed model is minimally supervised and merely a one-to-one mapping between the pre-defined argument set and the extracted topics is required. These extracted arguments are subsequently exploited for stance classification. Additionally, a manuallyannotated corpus for stance classification and argument tagging of online news comments is introduced and made available. Experiments on our collected corpus demonstrate the benefits of using topic-modeling for argument tagging. We show that using Non-Negative Matrix Factorization instead of Latent Dirichlet Allocation achieves better results for argument classification, close to the results of a supervised classifier. Furthermore, the statistical model that leverages automatically-extracted arguments as features for stance classification shows promising results.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ArgMining@HLT-NAACL'},\n",
       " {'paperId': '20679774bdaf52586e16d07c604861ac92ba25a8',\n",
       "  'title': 'Formal Aspects and Parsing Issues of Dependency Theory',\n",
       "  'authorId': '152862473',\n",
       "  'authorName': 'V. Lombardo',\n",
       "  'abstract': 'The paper investigates the problem of providing a formal device for the dependency approach to syntax, and to link it with a parsing model. After reviewing the basic tenets of the paradigm and the few existing mathematical results, we describe a dependency formalism which is able to deal with long-distance dependencies. Finally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results.',\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '531a7f2c659787165df4fd5b4580590b953448e4',\n",
       "  'title': 'The E2E Dataset: New Challenges For End-to-End Generation',\n",
       "  'authorId': '2848048',\n",
       "  'authorName': 'Jekaterina Novikova',\n",
       "  'abstract': 'This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '661f3174766c8aeba31b3cc0d4fcdf7d44a3bad5',\n",
       "  'title': 'DoQA - Accessing Domain-Specific FAQs via Conversational QA',\n",
       "  'authorId': '1602998334',\n",
       "  'authorName': 'Jon Ander Campos',\n",
       "  'abstract': 'The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing. Compared to previous work, DoQA comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain. In addition, we introduce a more realistic information retrieval (IR) scenario where the system needs to find the answer in any of the FAQ documents. The results of an existing, strong, system show that, thanks to transfer learning from a Wikipedia QA dataset and fine tuning on a single FAQ domain, it is possible to build high quality conversational QA systems for FAQs without in-domain training data. The good results carry over into the more challenging IR scenario. In both cases, there is still ample room for improvement, as indicated by the higher human upperbound.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e35357ac461a669fe7e4b877ee1fad0dfda26303',\n",
       "  'title': 'Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings',\n",
       "  'authorId': '26161085',\n",
       "  'authorName': 'Kalpesh Krishna',\n",
       "  'abstract': 'Style transfer is the task of rewriting a sentence into a target style while approximately preserving content. While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al. 2021) has attempted “few-shot” style transfer using only 3-10 sentences at inference for style extraction. In this work we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available. We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim. We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages. Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob. We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simplification, gender neutralization, text anonymization) all without retraining the model. Finally, we find model evaluation to be difficult due to the lack of datasets and metrics for many languages. To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages, and use this data to design our automatic evaluations.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'e95734df7202270975fa5fd2e901b43cec5c5e83',\n",
       "  'title': 'Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations',\n",
       "  'authorId': '31498163',\n",
       "  'authorName': 'Dipendra Kumar Misra',\n",
       "  'abstract': 'Semantic parsing from denotations faces two key challenges in model training: (1) given only the denotations (e.g., answers), search for good candidate semantic parses, and (2) choose the best model update algorithm. We propose effective and general solutions to each of them. Using policy shaping, we bias the search procedure towards semantic parses that are more compatible to the text, which provide better supervision signals for training. In addition, we propose an update equation that generalizes three different families of learning algorithms, which enables fast model exploration. When experimented on a recently proposed sequential question answering dataset, our framework leads to a new state-of-the-art model that outperforms previous work by 5.0% absolute on exact match accuracy.',\n",
       "  'year': 2018,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'f639ce400d4def7bed2b75e474613e05a993b594',\n",
       "  'title': 'Multilingual and Cross-Lingual Complex Word Identification',\n",
       "  'authorId': '3084761',\n",
       "  'authorName': 'Seid Muhie Yimam',\n",
       "  'abstract': 'Complex Word Identification (CWI) is an important task in lexical simplification and text accessibility. Due to the lack of CWI datasets, previous works largely depend on Simple English Wikipedia and edit histories for obtaining ‘gold standard’ annotations, which are of doubtable quality, and limited only to English. We collect complex words/phrases (CP) for English, German and Spanish, annotated by both native and non-native speakers, and propose language independent features that can be used to train multilingual and cross-lingual CWI models. We show that the performance of cross-lingual CWI systems (using a model trained on one language and applying it on the other languages) is comparable to the performance of monolingual CWI systems.',\n",
       "  'year': 2017,\n",
       "  'venue': 'RANLP'},\n",
       " {'paperId': '54a5940311053912360670c4b6e6a2925df2de12',\n",
       "  'title': 'Orthography Engineering in Grammatical Framework',\n",
       "  'authorId': '114753335',\n",
       "  'authorName': 'Krasimir Angelov',\n",
       "  'abstract': 'Orthography is an integral part of language but in grammar engineering it is often ignored, simplified or just delegated to external tools. We present new extensions to Grammatical Framework, which allow one and the same formalism to describe both orthography, syntax and morphology. These extensions are also easily generalizable to other formalisms.',\n",
       "  'year': 2015,\n",
       "  'venue': ''},\n",
       " {'paperId': '4713199775e52eaf6424680549a43a3cf72af2d0',\n",
       "  'title': 'Predicting Second Language Learner Successes and Mistakes by Means of Conjunctive Features',\n",
       "  'authorId': '2785507',\n",
       "  'authorName': 'Yves Bestgen',\n",
       "  'abstract': 'This paper describes the system developed by the Centre for English Corpus Linguistics for the 2018 Duolingo SLAM challenge. It aimed at predicting the successes and mistakes of second language learners on each of the words that compose the exercises they answered. Its main characteristic is to include conjunctive features, built by combining word ngrams with metadata about the user and the exercise. It achieved a relatively good performance, ranking fifth out of 15 systems. Complementary analyses carried out to gauge the contribution of the different sets of features to the performance confirmed the usefulness of the conjunctive features for the SLAM task.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': 'f0a4000286dcd6ec3fd955ebab988bae3eac4a4d',\n",
       "  'title': 'QUality Estimation from ScraTCH (QUETCH): Deep Learning for Word-level Translation Quality Estimation',\n",
       "  'authorId': '3422710',\n",
       "  'authorName': 'Julia Kreutzer',\n",
       "  'abstract': 'This paper describes the system submitted by the University of Heidelberg to the Shared Task on Word-level Quality Estimation at the 2015 Workshop on Statistical Machine Translation. The submitted system combines a continuous space deep neural network, that learns a bilingual feature representation from scratch, with a linear combination of the manually defined baseline features provided by the task organizers. A combination of these orthogonal information sources shows significant improvements over the combined systems, and produces very competitive F1-scores for predicting word-level translation quality.',\n",
       "  'year': 2015,\n",
       "  'venue': 'WMT@EMNLP'},\n",
       " {'paperId': 'bb3425318de7eed5641cda147d61c9a057b9d054',\n",
       "  'title': 'Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks',\n",
       "  'authorId': '2363901',\n",
       "  'authorName': 'Rabeeh Karimi Mahabadi',\n",
       "  'abstract': 'State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/rabeehk/hyperformer.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'df6dadfd989b5623c928cad17809b1ff3fff081e',\n",
       "  'title': 'Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate',\n",
       "  'authorId': '2022649',\n",
       "  'authorName': 'Christo Kirov',\n",
       "  'abstract': 'Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter in 1988, Pinker and Prince presented a comprehensive rebuttal of many of Rumelhart and McClelland’s claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince’s criticisms without requiring any simplification of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a reexamination of their utility in linguistic and cognitive modeling.',\n",
       "  'year': 2018,\n",
       "  'venue': 'TACL'},\n",
       " {'paperId': 'bddc5714f44740d57ba2e8f7c244ee3ecdb52748',\n",
       "  'title': 'YNU_DYX at SemEval-2019 Task 9: A Stacked BiLSTM for Suggestion Mining Classification',\n",
       "  'authorId': '148387344',\n",
       "  'authorName': 'Yunxia Ding',\n",
       "  'abstract': 'In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A: Suggestion Mining from Online Reviews and Forums. We use Word2Vec to learn the distributed representations from sentences. This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context. We perform an ensemble to improve the effectiveness of our model. Our official submission results achieve an F1-score 0.5659.',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '5e0cca43a1c4fc0f1fc03764fc2dab59efeba7a5',\n",
       "  'title': 'Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations',\n",
       "  'authorId': '2111946280',\n",
       "  'authorName': 'Saif M. Mohammad',\n",
       "  'abstract': 'Disparities in authorship and citations across genders can have substantial adverse consequences not just on the disadvantaged gender, but also on the field of study as a whole. In this work, we examine female first author percentages and the citations to their papers in Natural Language Processing. We find that only about 29% of first authors are female and only about 25% of last authors are female. Notably, this percentage has not improved since the mid 2000s. We also show that, on average, female first authors are cited less than male first authors, even when controlling for experience and area of research. We hope that recording citation and participation gaps across demographic groups will improve awareness of gender gaps and encourage more inclusiveness and fairness in research.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '7ae0d87d9bd764626bcf07f0d9d7f5e8270f9f00',\n",
       "  'title': 'What Can We Learn from Noun Substitutions in Revision Histories?',\n",
       "  'authorId': '148178070',\n",
       "  'authorName': 'Talita Anthonio',\n",
       "  'abstract': 'In community-edited resources such as wikiHow, sentences are subject to revisions on a daily basis. Recent work has shown that resulting improvements over time can be modelled computationally, assuming that each revision contributes to the improvement. We take a closer look at a subset of such revisions, for which we attempt to improve a computational model and validate in how far the assumption that ‘revised means better’ actually holds. The subset of revisions considered here are noun substitutions, which often involve interesting semantic relations, including synonymy, antonymy and hypernymy. Despite the high semantic relatedness, we find that a supervised classifier can distinguish the revised version of a sentence from an original version with an accuracy close to 70%, when taking context into account. In a human annotation study, we observe that annotators identify the revised sentence as the ‘better version’ with similar performance. Our analysis reveals a fair agreement among annotators when a revision improves fluency. In contrast, noun substitutions that involve other lexical-semantic relationships are often perceived as being equally good or tend to cause disagreements. While these findings are also reflected in classification scores, a comparison of results shows that our model fails in cases where humans can resort to factual knowledge or intuitions about the required level of specificity.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '96bd3982f8c6eff4b4b48cd2aedf0adbe304bb72',\n",
       "  'title': 'Analyzing Sentence Fusion in Abstractive Summarization',\n",
       "  'authorId': '50827114',\n",
       "  'authorName': 'Logan Lebanoff',\n",
       "  'abstract': 'While recent work in abstractive summarization has resulted in higher scores in automatic metrics, there is little understanding on how these systems combine information taken from multiple document sentences. In this paper, we analyze the outputs of five state-of-the-art abstractive summarizers, focusing on summary sentences that are formed by sentence fusion. We ask assessors to judge the grammaticality, faithfulness, and method of fusion for summary sentences. Our analysis reveals that system sentences are mostly grammatical, but often fail to remain faithful to the original article.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a16faa446a831cf7ee9d3c8519aaf00053146b4f',\n",
       "  'title': 'Global Optimization under Length Constraint for Neural Text Summarization',\n",
       "  'authorId': '1850928',\n",
       "  'authorName': 'Takuya Makino',\n",
       "  'abstract': 'We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN/Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest processing speed; only 6.70% overlength summaries on CNN/Daily and 7.8% on long summary of Mainichi, compared to the approximately 20% to 50% on CNN/Daily Mail and 10% to 30% on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset Mainich that is created with strict length constraints. The ex- perimental results show approximately 30% to 40% improved post-editing time by use of in-length summaries.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ef18db2a18ac61e72783a613328842ce86ef00bf',\n",
       "  'title': 'AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models',\n",
       "  'authorId': '1384668226',\n",
       "  'authorName': 'Yichun Yin',\n",
       "  'abstract': 'Pre-trained language models (PLMs) have achieved great success in natural language processing. Most of PLMs follow the default setting of architecture hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT. Few studies have been conducted to explore the design of architecture hyper-parameters in BERT, especially for the more efficient PLMs with tiny sizes, which are essential for practical deployment on resource-constrained devices. In this paper, we adopt the one-shot Neural Architecture Search (NAS) to automatically search architecture hyper-parameters. Specifically, we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny PLMs for various latency constraints. We name our method AutoTinyBERT and evaluate its effectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show that our method outperforms both the SOTA search-based baseline (NAS-BERT) and the SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM, and MobileBERT). In addition, based on the obtained architectures, we propose a more efficient development method that is even faster than the development of a single PLM. The source code and models will be publicly available upon publication.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '0af30a8edb0842f0d7fad913169a863abb63cc11',\n",
       "  'title': 'Developing a Curated Topic Model for COVID-19 Medical Research Literature',\n",
       "  'authorId': '1680292',\n",
       "  'authorName': 'P. Resnik',\n",
       "  'abstract': 'Topic models can facilitate search, navigation, and knowledge discovery in large document collections. However, automatic generation of topic models can produce results that fail to meet the needs of users. We advocate for a set of user-focused desiderata in topic modeling for the COVID-19 literature, and describe an effort in progress to develop a curated topic model for COVID-19 articles informed by subject matter expertise and the way medical researchers engage with medical literature.',\n",
       "  'year': 2020,\n",
       "  'venue': 'NLP4COVID@EMNLP'},\n",
       " {'paperId': '2d9b284e331ccb32f314bdf2c27a8f56254223cb',\n",
       "  'title': 'Knowledge Organization and Application: Brief Comments on Papers in the Session',\n",
       "  'authorId': '1714374',\n",
       "  'authorName': 'A. Joshi',\n",
       "  'abstract': 'Brackman describes a l a t t i ce l i ke structured inheritance network (KLONE) as a language for exp l i c i t representation of natural language conceptual information. Multiple descriptions can be represented. How does the f a c i l i t y d i f fe r from a similar one in KRL? Belief representations appear to be only impl ic i t . Quantification is handled through a set of \"structural descriptions.\" I t is not clear how negation is handled. The main application is for the command and control of advanced graphics manioulators through natural language. Is there an impl ic i t claim here that the KLONE representations are suitable for both natural language concepts as well as for those in the visual domain?',\n",
       "  'year': 1979,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3dfb1f50f2a34a699c339dabaa6f9b3a977973de',\n",
       "  'title': 'LongT5: Efficient Text-To-Text Transformer for Long Sequences',\n",
       "  'authorId': '51150315',\n",
       "  'authorName': 'Mandy Guo',\n",
       "  'abstract': 'Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5 , a new model that explores the effects of scaling both the input length and model size at the same time. Speciﬁcally, we integrate attention ideas from long-input transformers (ETC), and adopt pretraining strategies from summarization pretraining (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call Transient Global (TGlobal), which mimics ETC’s local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization and question answering tasks, as well as outper-form the original T5 models on these tasks. We have open sourced our architecture and training code, as well as our pre-trained model checkpoints.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL-HLT'},\n",
       " {'paperId': 'cefe1e8659ac629d0e80f35e87921c5ea27c8056',\n",
       "  'title': 'Addressing Low-Resource Scenarios with Character-aware Embeddings',\n",
       "  'authorId': '51940291',\n",
       "  'authorName': 'Sean Papay',\n",
       "  'abstract': 'Most modern approaches to computing word embeddings assume the availability of text corpora with billions of words. In this paper, we explore a setup where only corpora with millions of words are available, and many words in any new text are out of vocabulary. This setup is both of practical interests – modeling the situation for specific domains and low-resource languages – and of psycholinguistic interest, since it corresponds much more closely to the actual experiences and challenges of human language learning and use. We compare standard skip-gram word embeddings with character-based embeddings on word relatedness prediction. Skip-grams excel on large corpora, while character-based embeddings do well on small corpora generally and rare and complex words specifically. The models can be combined easily.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': 'b8aaddc12706f8dde14f88b250199fd709c082fe',\n",
       "  'title': 'Semantic Interpretation Using KL-ONE',\n",
       "  'authorId': '2195088',\n",
       "  'authorName': 'N. Sondheimer',\n",
       "  'abstract': 'This paper presents extensions to the work of Bobrow and Webber [Bobrow & Webber 80a, Bobrow & Webber 80b] on semantic interpretation using KL-ONE to represent knowledge. The approach is based on an extended case frame formalism applicable to all types of phrases, not just clauses. The frames are used to recognize semantically acceptable phrases, identify their structure, and, relate them to their meaning representation through translation rules. Approaches are presented for generating KL-ONE structures as the meaning of a sentence, for capturing semantic generalizations through abstract case frames, and for handling pronouns and relative clauses.',\n",
       "  'year': 1984,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'dbe05d7f92f0b66fe15fb805f38f590fb78fecd6',\n",
       "  'title': 'Integrating Predictions from Neural-Network Relation Classifiers into Coreference and Bridging Resolution',\n",
       "  'authorId': '2065296481',\n",
       "  'authorName': 'Ina Roesiger',\n",
       "  'abstract': 'Cases of coreference and bridging resolution often require knowledge about semantic relations between anaphors and antecedents. We suggest state-of-the-art neural-network classifiers trained on relation benchmarks to predict and integrate likelihoods for relations. Two experiments with representations differing in noise and complexity improve our bridging but not our coreference resolver.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': 'cb0de2de79533d4faada3d745f43702eb89d1a60',\n",
       "  'title': 'Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards',\n",
       "  'authorId': '1584940075',\n",
       "  'authorName': 'Angelina McMillan-Major',\n",
       "  'abstract': 'Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task, especially given the variety of backgrounds, skills, and incentives of the people involved in the building of natural language processing (NLP) tools. Nevertheless, the adoption of standard documentation practices across the field of NLP promotes more accessible and detailed descriptions of NLP datasets and models, while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation, we present two case studies of efforts that aim to develop reusable documentation templates – the HuggingFace data card, a general purpose card for datasets in NLP, and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these templates, including the identification of relevant stakeholder groups, the definition of a set of guiding principles, the use of existing templates as our foundation, and iterative revisions based on feedback.',\n",
       "  'year': 2021,\n",
       "  'venue': 'GEM'},\n",
       " {'paperId': '191896dd48d01485ccfaa695e7e464b082974ef5',\n",
       "  'title': 'Leveraging FrameNet to Improve Automatic Event Detection',\n",
       "  'authorId': '50152545',\n",
       "  'authorName': 'Shulin Liu',\n",
       "  'abstract': 'Frames defined in FrameNet (FN) share highly similar structures with events in ACE event extraction program. An event in ACE is composed of an event trigger and a set of arguments. Analogously, a frame in FN is composed of a lexical unit and a set of frame elements, which play similar roles as triggers and arguments of ACE events respectively. Besides having similar structures, many frames in FN actually express certain types of events. The above observations motivate us to explore whether there exists a good mapping from frames to event-types and if it is possible to improve event detection by using FN. In this paper, we propose a global inference approach to detect events in FN. Further, based on the detected results, we analyze possible mappings from frames to event-types. Finally, we improve the performance of event detection and achieve a new state-of-the-art result by using the events automatically detected from FN.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd1a32da778a17ec3931db4bd7a00e05d1156df6a',\n",
       "  'title': 'Detecting Retries of Voice Search Queries',\n",
       "  'authorId': '2574576',\n",
       "  'authorName': 'Rivka Levitan',\n",
       "  'abstract': 'When a system fails to correctly recognize a voice search query, the user will frequently retry the query, either by repeating it exactly or rephrasing it in an attempt to adapt to the system’s failure. It is desirable to be able to identify queries as retries both offline, as a valuable quality signal, and online, as contextual information that can aid recognition. We present a method than can identify retries offline with 81% accuracy using similarity measures between two subsequent queries as well as system and user signals of recognition accuracy. The retry rate predicted by this method correlates significantly with a gold standard measure of accuracy, suggesting that it may be useful as an offline predictor of accuracy.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd1a7f08af36ebc223eb552eced70115bcf21ce7d',\n",
       "  'title': 'Bilingually-constrained Phrase Embeddings for Machine Translation',\n",
       "  'authorId': '38358352',\n",
       "  'authorName': 'Jiajun Zhang',\n",
       "  'abstract': 'We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings. The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of nontranslation pairs simultaneously. After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other. We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a0d7b0b0defe96644466aa60873f9e1bc3f592b0',\n",
       "  'title': 'A Normalizer for UGC in Brazilian Portuguese',\n",
       "  'authorId': '2763112',\n",
       "  'authorName': 'M. Duran',\n",
       "  'abstract': 'User-generated contents (UGC) represent an important source of information for governments, companies, political candidates and consumers. However, most of the Natural Language Processing tools and techniques are developed from and for texts of standard language, and UGC is a type of text especially full of creativity and idiosyncrasies, which represents noise for NLP purposes. This paper presents UGCNormal, a lexicon-based tool for UGC normalization. It encompasses a tokenizer, a sentence segmentation tool, a phonetic-based speller and some lexicons, which were originated from a deep analysis of a corpus of product reviews in Brazilian Portuguese. The normalizer was evaluated in two different data sets and carried out from 31% to 89% of the appropriate corrections, depending on the type of text noise. The use of UGCNormal was also validated in a task of POS tagging, which improved from 91.35% to 93.15% in accuracy and in a task of opinion classification, which improved the average of F1-score measures (F1-score positive and F1-score negative) from 0.736 to 0.758.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NUT@IJCNLP'},\n",
       " {'paperId': '8d022981b93e5d4f716e491a20d6e1280d13b317',\n",
       "  'title': 'Accurate Word Segmentation and POS Tagging for Japanese Microblogs: Corpus Annotation and Joint Modeling with Lexical Normalization',\n",
       "  'authorId': '145766950',\n",
       "  'authorName': 'Nobuhiro Kaji',\n",
       "  'abstract': 'Microblogs have recently received widespread interest from NLP researchers. However, current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts. We developed an annotated corpus and proposed a joint model for overcoming this situation. Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance. Our joint model with lexical normalization handles the orthographic diversity of microblog texts. We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'd5cb26451f96c9d24a58fe9021f134a863a28d23',\n",
       "  'title': 'SWEAT: Scoring Polarization of Topics across Different Corpora',\n",
       "  'authorId': '49224924',\n",
       "  'authorName': 'Federico Bianchi',\n",
       "  'abstract': 'Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, SWEAT uses two additional wordsets, deemed to have opposite valence, to represent two different poles. We validate our approach and illustrate a case study to show the usefulness of the introduced measure.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '6e6e9866f93005699e983862b1767c58bc2f3ef6',\n",
       "  'title': 'Zero-shot Chinese Discourse Dependency Parsing via Cross-lingual Mapping',\n",
       "  'authorId': '2117232627',\n",
       "  'authorName': 'Yi Cheng',\n",
       "  'abstract': 'Due to the absence of labeled data, discourse parsing still remains challenging in some languages. In this paper, we present a simple and efficient method to conduct zero-shot Chinese text-level dependency parsing by leveraging English discourse labeled data and parsing techniques. We first construct the Chinese-English mapping from the level of sentence and elementary discourse unit (EDU), and then exploit the parsing results of the corresponding English translations to obtain the discourse trees for the Chinese text. This method can automatically conduct Chinese discourse parsing, with no need of a large scale of Chinese labeled data.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the 1st Workshop on Discourse Structure in Neural NLG'},\n",
       " {'paperId': 'e3919e94c811fd85f5038926fa354619861674f9',\n",
       "  'title': 'Question Answering on Freebase via Relation Extraction and Textual Evidence',\n",
       "  'authorId': '151485141',\n",
       "  'authorName': 'Kun Xu',\n",
       "  'abstract': 'Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F_1 of 53.3%, a substantial improvement over the state-of-the-art.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '27b76c88f829a556617163a000edefd5a4144846',\n",
       "  'title': 'Serial Combination of Rules and Statistics: A Case Study in Czech Tagging',\n",
       "  'authorId': '144002335',\n",
       "  'authorName': 'Jan Hajic',\n",
       "  'abstract': 'A hybrid system is described which combines the strength of manual rule-writing and statistical learning, obtaining results superior to both methods if applied separately. The combination of a rule-based system and a statistical one is not parallel but serial: the rule-based system performing partial disambiguation with recall close to 100% is applied first, and a trigram HMM tagger runs on its results. An experiment in Czech tagging has been performed with encouraging results.',\n",
       "  'year': 2001,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c7fe0dfebe46f7cc1f1fdc181a2b096794c18520',\n",
       "  'title': 'Multi-source morphosyntactic tagging for spoken Rusyn',\n",
       "  'authorId': '3080637',\n",
       "  'authorName': 'Yves Scherrer',\n",
       "  'abstract': 'This paper deals with the development of morphosyntactic taggers for spoken varieties of the Slavic minority language Rusyn. As neither annotated corpora nor parallel corpora are electronically available for Rusyn, we propose to combine existing resources from the etymologically close Slavic languages Russian, Ukrainian, Slovak, and Polish and adapt them to Rusyn. Using MarMoT as tagging toolkit, we show that a tagger trained on a balanced set of the four source languages outperforms single language taggers by about 9%, and that additional automatically induced morphosyntactic lexicons lead to further improvements. The best observed accuracies for Rusyn are 82.4% for part-of-speech tagging and 75.5% for full morphological tagging.',\n",
       "  'year': 2017,\n",
       "  'venue': 'VarDial'},\n",
       " {'paperId': '7405b63875ae9e216f27df29728ba55f56488100',\n",
       "  'title': 'Low-resource Entity Set Expansion: A Comprehensive Study on User-generated Text',\n",
       "  'authorId': '30046004',\n",
       "  'authorName': 'Yutong Shao',\n",
       "  'abstract': 'Entity set expansion (ESE) aims at obtaining a more complete set of entities given a textual corpus and a seed set of entities of a concept. Although it is a critical task in many NLP applications, existing benchmarks are limited to well-formed text (e.g., Wikipedia) and well-deﬁned concepts (e.g., countries and diseases). Furthermore, only a small number of predictions are evaluated compared to the actual size of an entity set. A rigorous assessment of ESE methods warrants more comprehensive benchmarks and evaluation. In this paper, we consider user-generated text to understand the generalizability of ESE methods. We develop new benchmarks and propose more rigorous evaluation metrics for assessing performance of ESE methods. Additionally, we identify phenomena such as non-named entities, multifaceted entities, vague concepts that are more prevalent in user-generated text than well-formed text, and use them to proﬁle ESE methods. We observe that the strong performance of state-of-the-art ESE methods does not generalize well to user-generated text. We conduct comprehensive empirical analysis and draw insights from the ﬁndings.',\n",
       "  'year': 2022,\n",
       "  'venue': 'NAACL-HLT'},\n",
       " {'paperId': '5071764a6096e2208a03286d3ff6a2818afd12c9',\n",
       "  'title': 'Have my arguments been replied to? Argument Pair Extraction as Machine Reading Comprehension',\n",
       "  'authorId': '2008034310',\n",
       "  'authorName': 'Jianzhu Bao',\n",
       "  'abstract': 'Argument pair extraction (APE) aims to automatically mine argument pairs from two interrelated argumentative documents. Existing studies typically identify argument pairs indirectly by predicting sentence-level relations between two documents, neglecting the modeling of the holistic argument-level interactions. Towards this issue, we propose to address APE via a machine reading comprehension (MRC) framework with two phases. The first phase employs an argument mining (AM) query to identify all arguments in two documents. The second phase considers each identified argument as an APE query to extract its paired arguments from another document, allowing to better capture the argument-level interactions. Also, this framework enables these two phases to be jointly trained in a single MRC model, thereby maximizing the mutual benefits of them. Experimental results demonstrate that our approach achieves the best performance, outperforming the state-of-the-art method by 7.11% in F1 score.',\n",
       "  'year': 2022,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3e27ba835ba716cf026fe67f6355f0e6ee208f27',\n",
       "  'title': 'FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition',\n",
       "  'authorId': '148398655',\n",
       "  'authorName': 'Yichong Leng',\n",
       "  'abstract': 'Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one sentence at a time, failing to leverage the voting effect1 from multiple candidates to better detect and correct error tokens. In this work, we propose FastCorrect 2, an error correction model that takes multiple ASR candidates as input for better correction accuracy. FastCorrect 2 adopts non-autoregressive generation for fast inference, which consists of an encoder that processes multiple source sentences and a decoder that generates the target sentence in parallel from the adjusted source sentence, where the adjustment is based on the predicted duration of each source token. However, there are some issues when handling multiple source sentences. First, it is non-trivial to leverage the voting effect from multiple source sentences since they usually vary in length. Thus, we propose a novel alignment algorithm to maximize the degree of token alignment among multiple sentences in terms of token and pronunciation similarity. Second, the decoder can only take one adjusted source sentence as input, while there are multiple source sentences. Thus, we develop a candidate predictor to detect the most suitable candidate for the decoder. Experiments on our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce the WER over the previous correction model with single candidate by 3.2% and 2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR error correction. FastCorrect 2 achieves better per∗This work was conducted at Microsoft. Corresponding author: Xu Tan, xuta@microsoft.com See the second paragraph in Section 1. formance than the cascaded re-scoring and correction pipeline and can serve as a unified postprocessing module for ASR.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '518ba6463051462e84f3485427639595d557bc68',\n",
       "  'title': 'Towards a Better Semantic Role Labeling of Complex Predicates',\n",
       "  'authorId': '2322347',\n",
       "  'authorName': 'Glorianna Jagfeld',\n",
       "  'abstract': 'We propose a way to automatically improve the annotation of verbal complex predicates in PropBank which until now has been treating language mostly in a compositional manner. In order to minimize the manual re-annotation effort, we build on the recently introduced concept of aliasing complex predicates to existing PropBank rolesets which encompass the same meaning and argument structure. We suggest to find aliases automatically by applying a multilingual distributional model that uses the translations of simple and complex predicates as features. Furthermore, we set up an annotation effort to obtain a frequency balanced, realistic test set for this task. Our method reaches an accuracy of 44% on this test set and 72% for the more frequent test items in a lenient evaluation, which is not far from the upper bounds from human annotation.',\n",
       "  'year': 2015,\n",
       "  'venue': 'HLT-NAACL'},\n",
       " {'paperId': '823f335eee85b42502c8c6cb3ce38b4ae274ef89',\n",
       "  'title': 'Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation',\n",
       "  'authorId': '38842528',\n",
       "  'authorName': 'Matt Post',\n",
       "  'abstract': 'The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm’s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '3d8e0757b6d9a096abc91db57a3eaae3bfa4c900',\n",
       "  'title': 'Epita at SemEval-2018 Task 1: Sentiment Analysis Using Transfer Learning Approach',\n",
       "  'authorId': '1410080030',\n",
       "  'authorName': 'Guillaume Daval-Frerot',\n",
       "  'abstract': 'In this paper we present our system for detecting valence task. The major issue was to apply a state-of-the-art system despite the small dataset provided: the system would quickly overfit. The main idea of our proposal is to use transfer learning, which allows to avoid learning from scratch. Indeed, we start to train a first model to predict if a tweet is positive, negative or neutral. For this we use an external dataset which is larger and similar to the target dataset. Then, the pre-trained model is re-used as the starting point to train a new model that classifies a tweet into one of the seven various levels of sentiment intensity. Our system, trained using transfer learning, achieves 0.776 and 0.763 respectively for Pearson correlation coefficient and weighted quadratic kappa metrics on the subtask evaluation dataset.',\n",
       "  'year': 2018,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '91ea7176cc41f7b97867d483075df7886aa3dc33',\n",
       "  'title': 'Practical Transformer-based Multilingual Text Classification',\n",
       "  'authorId': '2108725400',\n",
       "  'authorName': 'Cindy Wang',\n",
       "  'abstract': 'Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '6540ee01a87c3b3435da73f4e3297489d525c151',\n",
       "  'title': 'Don’t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference',\n",
       "  'authorId': '2083259',\n",
       "  'authorName': 'Yonatan Belinkov',\n",
       "  'abstract': 'Natural Language Inference (NLI) datasets often contain hypothesis-only biases—artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '26b0cc61b33d52e4ac866a307f09c3416a25e398',\n",
       "  'title': 'Neural Models of Text Normalization for Speech Applications',\n",
       "  'authorId': '40479003',\n",
       "  'authorName': 'Hao Zhang',\n",
       "  'abstract': 'Machine learning, including neural network techniques, have been applied to virtually every domain in natural language processing. One problem that has been somewhat resistant to effective machine learning solutions is text normalization for speech applications such as text-to-speech synthesis (TTS). In this application, one must decide, for example, that 123 is verbalized as one hundred twenty three in 123 pages but as one twenty three in 123 King Ave. For this task, state-of-the-art industrial systems depend heavily on hand-written language-specific grammars.We propose neural network models that treat text normalization for TTS as a sequence-to-sequence problem, in which the input is a text token in context, and the output is the verbalization of that token. We find that the most effective model, in accuracy and efficiency, is one where the sentential context is computed once and the results of that computation are combined with the computation of each token in sequence to compute the verbalization. This model allows for a great deal of flexibility in terms of representing the context, and also allows us to integrate tagging and segmentation into the process.These models perform very well overall, but occasionally they will predict wildly inappropriate verbalizations, such as reading 3 cm as three kilometers. Although rare, such verbalizations are a major issue for TTS applications. We thus use finite-state covering grammars to guide the neural models, either during training and decoding, or just during decoding, away from such “unrecoverable” errors. Such grammars can largely be learned from data.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Computational Linguistics'},\n",
       " {'paperId': '0895e6039533fa50681c3ca5bea9c7b77b44cead',\n",
       "  'title': 'Identification and Classification of Emotional Key Phrases from Psychological Texts',\n",
       "  'authorId': '47415893',\n",
       "  'authorName': 'A. Paul',\n",
       "  'abstract': 'Emotions, a complex state of feeling results in physical and psychological changes that infl uence human behavior. Thus, in order to extract the emotional key phrases from psychological texts, here, we have presented a phrase level emotion identification and classification sy stem. The system takes pre-defined emotional statements of seven basic emotion classes (anger, disgust, fear, guilt, joy, sadness and shame) as input and extracts seven types of emotional trigrams. The trigrams were represented as Context Vectors. Between a pair of Context Vectors, an Affinity Score was calculated based on the law of gravitation with respect to different distance metrics (e.g., Chebyshev, Euclidean and Hamming). The words, Part-Of-Speech (POS) tags, TF-IDF scores, variance along with Affinity Score and ranked score of the vectors were employed as important features in a supervised classific ation framework after a rigorous analysis. The comparative results carried out for four different classifiers e.g., NaiveBayes, J48, Decision Tree and BayesNet show satisfactory performances.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL 2015'},\n",
       " {'paperId': 'bfeb827d06c1a3583b5cc6d25241203a81f6af09',\n",
       "  'title': 'Knowledge Enhanced Contextual Word Representations',\n",
       "  'authorId': '39139825',\n",
       "  'authorName': 'Matthew E. Peters',\n",
       "  'abstract': 'Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '062466fb189fd3d4ab2f56a05937a8ae6df7bd06',\n",
       "  'title': 'A Comprehensive Assessment of Dialog Evaluation Metrics',\n",
       "  'authorId': '47999368',\n",
       "  'authorName': 'Yi-Ting Yeh',\n",
       "  'abstract': 'Automatic evaluation metrics are a crucial component of dialog systems research. Standard language evaluation metrics are known to be ineffective for evaluating dialog. As such, recent research has proposed a number of novel, dialog-specific metrics that correlate better with human judgements. Due to the fast pace of research, many of these metrics have been assessed on different datasets and there has as yet been no time for a systematic comparison between them. To this end, this paper provides a comprehensive assessment of recently proposed dialog evaluation metrics on a number of datasets. In this paper, 23 different automatic evaluation metrics are evaluated on 10 different datasets. Furthermore, the metrics are assessed in different settings, to better qualify their respective strengths and weaknesses. This comprehensive assessment offers several takeaways pertaining to dialog evaluation metrics in general. It also suggests how to best assess evaluation metrics and indicates promising directions for future work.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EANCS'},\n",
       " {'paperId': '1e6171d9937a36b2dba6b622bd5857f2407d1b2e',\n",
       "  'title': 'Smoothing and Shrinking the Sparse Seq2Seq Search Space',\n",
       "  'authorId': '49470779',\n",
       "  'authorName': 'Ben Peters',\n",
       "  'abstract': 'Current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: models give high scores to short, inadequate hypotheses and often make the empty string the argmax—the so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for neural machine translation. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both cross-entropy and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and machine translation for 7 language pairs.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'ea2ad7e330070aed3e909a2e263ae88e320663b3',\n",
       "  'title': 'An adaptable task-oriented dialog system for stand-alone embedded devices',\n",
       "  'authorId': '145378739',\n",
       "  'authorName': 'Long Duong',\n",
       "  'abstract': 'This paper describes a spoken-language end-to-end task-oriented dialogue system for small embedded devices such as home appliances. While the current system implements a smart alarm clock with advanced calendar scheduling functionality, the system is designed to make it easy to port to other application domains (e.g., the dialogue component factors out domain-specific execution from domain-general actions such as requesting and updating slot values). The system does not require internet connectivity because all components, including speech recognition, natural language understanding, dialogue management, execution and text-to-speech, run locally on the embedded device (our demo uses a Raspberry Pi). This simplifies deployment, minimizes server costs and most importantly, eliminates user privacy risks. The demo video in alarm domain is here youtu.be/N3IBMGocvHU',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '1845a57621ba2ce9ff3c2d1dcaa1f7f4b04b2186',\n",
       "  'title': 'Modeling Joint Entity and Relation Extraction with Table Representation',\n",
       "  'authorId': '1731657',\n",
       "  'authorName': 'Makoto Miwa',\n",
       "  'abstract': 'This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence. We introduce a novel simple and flexible table representation of entities and relations. We investigate several feature settings, search orders, and learning methods with inexact search on the table. The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders.',\n",
       "  'year': 2014,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '4690e0fa97eb40dcc62d6949b6c4ad52f8256717',\n",
       "  'title': 'An Empirically-based System for Processing Definite Descriptions',\n",
       "  'authorId': '144845513',\n",
       "  'authorName': 'R. Vieira',\n",
       "  'abstract': 'We present an implemented system for processing definite descriptions in arbitrary domains. The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora. The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions.',\n",
       "  'year': 2000,\n",
       "  'venue': 'CL'},\n",
       " {'paperId': 'c6801d553a43530b192309ef4364a43e33e4067f',\n",
       "  'title': 'Data augmentation using back-translation for context-aware neural machine translation',\n",
       "  'authorId': '1453730821',\n",
       "  'authorName': 'Amane Sugiyama',\n",
       "  'abstract': 'A single sentence does not always convey information that is enough to translate it into other languages. Some target languages need to add or specialize words that are omitted or ambiguous in the source languages (e.g, zero pronouns in translating Japanese to English or epicene pronouns in translating English to French). To translate such ambiguous sentences, we need contexts beyond a single sentence, and have so far explored context-aware neural machine translation (NMT). However, a large amount of parallel corpora is not easily available to train accurate context-aware NMT models. In this study, we first obtain large-scale pseudo parallel corpora by back-translating monolingual data, and then investigate its impact on the translation accuracy of context-aware NMT models. We evaluated context-aware NMT models trained with small parallel corpora and the large-scale pseudo parallel corpora on English-Japanese and English-French datasets to demonstrate the large impact of the data augmentation for context-aware NMT models.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'c21aad241dd93d385f4624d601812e4c4aa09346',\n",
       "  'title': 'Dissecting the Practical Lexical Function Model for Compositional Distributional Semantics',\n",
       "  'authorId': '2116391511',\n",
       "  'authorName': 'Abhijeet Gupta',\n",
       "  'abstract': 'The Practical Lexical Function model (PLF) is a recently proposed compositional distributional semantic model which provides an elegant account of composition, striking a balance between expressiveness and robustness and performing at the state-of-the-art. In this paper, we identify an inconsistency in PLF between the objective function at training and the prediction at testing which leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEM@NAACL-HLT'},\n",
       " {'paperId': '4d60cb995d813d38287d3aab230a5f35873f82a6',\n",
       "  'title': 'Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer',\n",
       "  'authorId': '51933789',\n",
       "  'authorName': 'N. Ilinykh',\n",
       "  'abstract': 'We explore how a multi-modal transformer trained for generation of longer image descriptions learns syntactic and semantic representations about entities and relations grounded in objects at the level of masked self-attention (text generation) and cross-modal attention (information fusion). We observe that cross-attention learns the visual grounding of noun phrases into objects and high-level semantic information about spatial relations, while text-to-text attention captures low-level syntactic knowledge between words. This concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-only). Our code is available here: https://github.com/GU-CLASP/attention-as-grounding.',\n",
       "  'year': 2022,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '2f2e88b547467c9349d886bbdaf166e70be9b658',\n",
       "  'title': 'The ACL Anthology: Current State and Future Directions',\n",
       "  'authorId': '1793218',\n",
       "  'authorName': 'D. Gildea',\n",
       "  'abstract': 'The Association of Computational Linguistic’s Anthology is the open source archive, and the main source for computational linguistics and natural language processing’s scientific literature. The ACL Anthology is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of Docker images will improve the Anthology’s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the Anthology. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards.',\n",
       "  'year': 2018,\n",
       "  'venue': ''},\n",
       " {'paperId': '79de899ee38aaa59b43ea90576b05d53defeab65',\n",
       "  'title': 'Multi-source annotation projection of coreference chains: assessing strategies and testing opportunities',\n",
       "  'authorId': '3457105',\n",
       "  'authorName': 'Yulia Grishina',\n",
       "  'abstract': 'In this paper, we examine the possibility of using annotation projection from multiple sources for automatically obtaining coreference annotations in the target language. We implement a multi-source annotation projection algorithm and apply it on an English-German-Russian parallel corpus in order to transfer coreference chains from two sources to the target side. Operating in two settings – a low-resource and a more linguistically-informed one – we show that automatic coreference transfer could benefit from combining information from multiple languages, and assess the quality of both the extraction and the linking of target coreference mentions.',\n",
       "  'year': 2017,\n",
       "  'venue': ''},\n",
       " {'paperId': 'aee48556b27e05fa12f04878aa977a35dd259649',\n",
       "  'title': 'Unsupervised Learning of Style-sensitive Word Vectors',\n",
       "  'authorId': '31487889',\n",
       "  'authorName': 'Reina Akama',\n",
       "  'abstract': 'This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner. We propose extending the continuous bag of words (CBOW) embedding model (Mikolov et al., 2013b) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent. In addition, we introduce a novel task to predict lexical stylistic similarity and to create a benchmark dataset for this task. Our experiment with this dataset supports our assumption and demonstrates that the proposed extensions contribute to the acquisition of style-sensitive word embeddings.',\n",
       "  'year': 2018,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '501acb88710ac23a99562f3987b842c8a5e234bd',\n",
       "  'title': 'Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning',\n",
       "  'authorId': '2672644',\n",
       "  'authorName': 'Angeliki Lazaridou',\n",
       "  'abstract': 'Zero-shot methods in language, vision and other domains rely on a cross-space mapping function that projects vectors from the relevant feature space (e.g., visualfeature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors. Zero-shot cross-space mapping methods hold great promise as a way to scale up annotation tasks well beyond the labels in the training data (e.g., recognizing objects that were never seen in training). However, the current performance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications. In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better methods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word translation) and cross-modal (image labeling) zero-shot experiments.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '05b1127ee39504516009b25384ca2bd7f2e1b9d9',\n",
       "  'title': 'Deep Communicating Agents for Abstractive Summarization',\n",
       "  'authorId': '1709797',\n",
       "  'authorName': 'Asli Celikyilmaz',\n",
       "  'abstract': 'We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '0d938d1305f8ef3c0930cdd21381201147a7be88',\n",
       "  'title': 'Generating E-Commerce Product Titles and Predicting their Quality',\n",
       "  'authorId': '34876539',\n",
       "  'authorName': 'José G. C. de Souza',\n",
       "  'abstract': 'E-commerce platforms present products using titles that summarize product information. These titles cannot be created by hand, therefore an algorithmic solution is required. The task of automatically generating these titles given noisy user provided titles is one way to achieve the goal. The setting requires the generation process to be fast and the generated title to be both human-readable and concise. Furthermore, we need to understand if such generated titles are usable. As such, we propose approaches that (i) automatically generate product titles, (ii) predict their quality. Our approach scales to millions of products and both automatic and human evaluations performed on real-world data indicate our approaches are effective and applicable to existing e-commerce scenarios.',\n",
       "  'year': 2018,\n",
       "  'venue': 'INLG'},\n",
       " {'paperId': 'af73d7815f13794223384004096ff4fc62c3d4a9',\n",
       "  'title': 'Video-Grounded Dialogues with Pretrained Generation Language Models',\n",
       "  'authorId': '2064728738',\n",
       "  'authorName': 'Hung Le',\n",
       "  'abstract': 'Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network. Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'f03afeef6f36da2f5b6531b65c996a7b8ea521dd',\n",
       "  'title': 'EmailSum: Abstractive Email Thread Summarization',\n",
       "  'authorId': '7670835',\n",
       "  'authorName': 'Shiyue Zhang',\n",
       "  'abstract': 'Recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions, etc.). Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. To spur research in thread summarization, we have developed an abstractive Email Thread Summarization (EmailSum) dataset, which contains human-annotated short (<30 words) and long (<100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. We perform a comprehensive empirical study to explore different summarization techniques (including extractive and abstractive methods, single-document and hierarchical models, as well as transfer and semisupervised learning) and conduct human evaluations on both short and long summary generation tasks. Our results reveal the key challenges of current abstractive summarization models in this task, such as understanding the sender’s intent and identifying the roles of sender and receiver. Furthermore, we find that widely used automatic evaluation metrics (ROUGE, BERTScore) are weakly correlated with human judgments on this email thread summarization task. Hence, we emphasize the importance of human evaluation and the development of better metrics by the community.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a04cc4321dd221ed5056174c60c7ab074e573a4f',\n",
       "  'title': 'Deep Questions without Deep Understanding',\n",
       "  'authorId': '1701735',\n",
       "  'authorName': 'I. Labutov',\n",
       "  'abstract': 'We develop an approach for generating deep (i.e, high-level) comprehension questions from novel text that bypasses the myriad challenges of creating a full semantic representation. We do this by decomposing the task into an ontologycrowd-relevance workflow, consisting of first representing the original text in a low-dimensional ontology, then crowdsourcing candidate question templates aligned with that space, and finally ranking potentially relevant templates for a novel region of text. If ontological labels are not available, we infer them from the text. We demonstrate the effectiveness of this method on a corpus of articles from Wikipedia alongside human judgments, and find that we can generate relevant deep questions with a precision of over 85% while maintaining a recall of 70%.',\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a3acdb9ea9aeb41bfa6f501c5cc3c8c67844bf78',\n",
       "  'title': 'Named Entity Recognition System for Dialectal Arabic',\n",
       "  'authorId': '2216860',\n",
       "  'authorName': 'Ayah Zirikly',\n",
       "  'abstract': 'To date, majority of research for Arabic Named Entity Recognition (NER) addresses the task for Modern Standard Arabic (MSA) and mainly focuses on the newswire genre. Despite some common characteristics between MSA and Dialectal Arabic (DA), the significant differences between the two language varieties hinder such MSA specific systems from solving NER for Dialectal Arabic. In this paper, we present an NER system for DA specifically focusing on the Egyptian Dialect (EGY). Our system delivers ≈ 16% improvement in F1-score over state-of-theart features.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ANLP@EMNLP'},\n",
       " {'paperId': '49d21384ffc3710604c8cb7765d02768cdf89c82',\n",
       "  'title': 'A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents',\n",
       "  'authorId': '3451318',\n",
       "  'authorName': 'A. C. Curry',\n",
       "  'abstract': 'How should conversational agents respond to verbal abuse through the user? To answer this question, we conduct a large-scale crowd-sourced evaluation of abuse response strategies employed by current state-of-the-art systems. Our results show that some strategies, such as “polite refusal”, score highly across the board, while for other strategies demographic factors, such as age, as well as the severity of the preceding abuse influence the user’s perception of which response is appropriate. In addition, we find that most data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness.',\n",
       "  'year': 2019,\n",
       "  'venue': 'SIGdial'},\n",
       " {'paperId': 'ec070add234e7ef8f7b80b91702e54a37d3be483',\n",
       "  'title': 'Exact Hard Monotonic Attention for Character-Level Transduction',\n",
       "  'authorId': '50425845',\n",
       "  'authorName': 'Shijie Wu',\n",
       "  'abstract': 'Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'c5c870618731bd80f74c06258e69bfe0d3a18dff',\n",
       "  'title': 'Word-like character n-gram embedding',\n",
       "  'authorId': '2115988519',\n",
       "  'authorName': 'Geewook Kim',\n",
       "  'abstract': 'We propose a new word embedding method called word-like character n-gram embedding, which learns distributed representations of words by embedding word-like character n-grams. Our method is an extension of recently proposed segmentation-free word embedding, which directly embeds frequent character n-grams from a raw corpus. However, its n-gram vocabulary tends to contain too many non-word n-grams. We solved this problem by introducing an idea of expected word frequency. Compared to the previously proposed methods, our method can embed more words, along with the words that are not included in a given basic word dictionary. Since our method does not rely on word segmentation with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many neologisms and informal words (e.g., Chinese SNS dataset). Our experimental results on Sina Weibo (a Chinese microblog service) and Twitter show that the proposed method can embed more words and improve the performance of downstream tasks.',\n",
       "  'year': 2018,\n",
       "  'venue': 'NUT@EMNLP'},\n",
       " {'paperId': '98332ae5475bfa286d9ab74e20d86c72ceb3cde6',\n",
       "  'title': 'The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction',\n",
       "  'authorId': '1958685',\n",
       "  'authorName': 'Dimitrios Alikaniotis',\n",
       "  'abstract': 'Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.',\n",
       "  'year': 2019,\n",
       "  'venue': 'BEA@ACL'},\n",
       " {'paperId': 'aac4d2cec1170c09f3577a31a27d9dee6b520c63',\n",
       "  'title': 'Classifying the reported ability in clinical mobility descriptions',\n",
       "  'authorId': '1405270604',\n",
       "  'authorName': 'Denis Newman-Griffis',\n",
       "  'abstract': 'Assessing how individuals perform different activities is key information for modeling health states of individuals and populations. Descriptions of activity performance in clinical free text are complex, including syntactic negation and similarities to textual entailment tasks. We explore a variety of methods for the novel task of classifying four types of assertions about activity performance: Able, Unable, Unclear, and None (no information). We find that ensembling an SVM trained with lexical features and a CNN achieves 77.9% macro F1 score on our task, and yields nearly 80% recall on the rare Unclear and Unable samples. Finally, we highlight several challenges in classifying performance assertions, including capturing information about sources of assistance, incorporating syntactic structure and negation scope, and handling new modalities at test time. Our findings establish a strong baseline for this novel task, and identify intriguing areas for further research.',\n",
       "  'year': 2019,\n",
       "  'venue': 'BioNLP@ACL'},\n",
       " {'paperId': '1912a15d4f114e6dda5074b0b2b8159572ed98c6',\n",
       "  'title': 'GEC into the future: Where are we going and how do we get there?',\n",
       "  'authorId': '2325708',\n",
       "  'authorName': 'Keisuke Sakaguchi',\n",
       "  'abstract': 'The field of grammatical error correction (GEC) has made tremendous bounds in the last ten years, but new questions and obstacles are revealing themselves. In this position paper, we discuss the issues that need to be addressed and provide recommendations for the field to continue to make progress, and propose a new shared task. We invite suggestions and critiques from the audience to make the new shared task a community-driven venture.',\n",
       "  'year': 2017,\n",
       "  'venue': 'BEA@EMNLP'},\n",
       " {'paperId': '46e84b0a7b3761d1a3c1577c66225453ab2cbc1c',\n",
       "  'title': 'MAVEN: A Massive General Domain Event Detection Dataset',\n",
       "  'authorId': '48631777',\n",
       "  'authorName': 'Xiaozhi Wang',\n",
       "  'abstract': 'Event detection (ED), which identifies event trigger words and classifies event types according to contexts, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Small scale of existing datasets is not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Limited event types of existing datasets lead to the trained models cannot be easily adapted to general-domain scenarios. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 117,200 event mention instances, and 207 event types. MAVEN alleviates the lack of data problem and covers much more general event types. Besides the dataset, we reproduce the recent state-of-the-art ED models and conduct a thorough evaluation for these models on MAVEN. The experimental results and empirical analyses show that existing ED methods cannot achieve promising results as on the small datasets, which suggests ED in real world remains a challenging task and requires further research efforts. The dataset and baseline code will be released in the future to promote this field.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '96d1aa1c6718c2e333428f23e4cf732bab3ff029',\n",
       "  'title': 'Reducing Annotation Efforts in Supervised Short Answer Scoring',\n",
       "  'authorId': '1780779',\n",
       "  'authorName': 'Torsten Zesch',\n",
       "  'abstract': 'Automated short answer scoring is increasingly used to give students timely feedback about their learning progress. Building scoring models comes with high costs, as stateof-the-art methods using supervised learning require large amounts of hand-annotated data. We analyze the potential of recently proposed methods for semi-supervised learning based on clustering. We find that all examined methods (centroids, all clusters, selected pure clusters) are mainly effective for very short answers and do not generalize well to severalsentence responses.',\n",
       "  'year': 2015,\n",
       "  'venue': 'BEA@NAACL-HLT'},\n",
       " {'paperId': 'dcf7fb34ae0be4eb7a838679b6bef0736375bbac',\n",
       "  'title': 'ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages',\n",
       "  'authorId': '144182018',\n",
       "  'authorName': 'Colin Lockard',\n",
       "  'abstract': 'In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'aaf16de76d04c736b5fd626201c6e418ff45482d',\n",
       "  'title': 'Task-Aware Representation of Sentences for Generic Text Classification',\n",
       "  'authorId': '40257094',\n",
       "  'authorName': 'Kishaloy Halder',\n",
       "  'abstract': 'State-of-the-art approaches for text classification leverage a transformer architecture with a linear layer on top that outputs a class distribution for a given prediction problem. While effective, this approach suffers from conceptual limitations that affect its utility in few-shot or zero-shot transfer learning scenarios. First, the number of classes to predict needs to be pre-defined. In a transfer learning setting, in which new classes are added to an already trained classifier, all information contained in a linear layer is therefore discarded, and a new layer is trained from scratch. Second, this approach only learns the semantics of classes implicitly from training examples, as opposed to leveraging the explicit semantic information provided by the natural language names of the classes. For instance, a classifier trained to predict the topics of news articles might have classes like “business” or “sports” that themselves carry semantic information. Extending a classifier to predict a new class named “politics” with only a handful of training examples would benefit from both leveraging the semantic information in the name of a new class and using the information contained in the already trained linear layer. This paper presents a novel formulation of text classification that addresses these limitations. It imbues the notion of the task at hand into the transformer model itself by factorizing arbitrary classification problems into a generic binary classification problem. We present experiments in few-shot and zero-shot transfer learning that show that our approach significantly outperforms previous approaches on small training data and can even learn to predict new classes with no training examples at all. The implementation of our model is publicly available at: https://github.com/flairNLP/flair.',\n",
       "  'year': 2020,\n",
       "  'venue': 'COLING'},\n",
       " {'paperId': '5137fd738f1ccaee032db073427f17eec42c4a4c',\n",
       "  'title': 'GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems',\n",
       "  'authorId': '2064493724',\n",
       "  'authorName': 'Bosheng Ding',\n",
       "  'abstract': 'Over the last few years, there has been a move towards data curation for multilingual task-oriented dialogue (ToD) systems that can serve people speaking different languages. However, existing multilingual ToD datasets either have a limited coverage of languages due to the high cost of data curation, or ignore the fact that dialogue entities barely exist in countries speaking these languages. To tackle these limitations, we introduce a novel data curation method that generates GlobalWoZ — a large-scale multilingual ToD dataset globalized from an English ToD dataset for three unexplored use cases of multilingual ToD systems. Our method is based on translating dialogue templates and filling them with local entities in the target-language countries. Besides, we extend the coverage of target languages to 20 languages. We will release our dataset and a set of strong baselines to encourage research on multilingual ToD systems for real use cases.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'a7e42bc1f09a5bf23258739a8d26d50b557986dd',\n",
       "  'title': 'Relating Neural Text Degeneration to Exposure Bias',\n",
       "  'authorId': '153033862',\n",
       "  'authorName': 'Ting-Rui Chiang',\n",
       "  'abstract': 'This work focuses on relating two mysteries in neural-based text generation: exposure bias, and text degeneration. Despite the long time since exposure bias was mentioned and the numerous studies for its remedy, to our knowledge, its impact on text generation has not yet been verified. Text degeneration is a problem that the widely-used pre-trained language model GPT-2 was recently found to suffer from (Holtzman et al., 2020). Motivated by the unknown causation of the text degeneration, in this paper we attempt to relate these two mysteries. Specifically, we first qualitatively and quantitatively identify mistakes made before text degeneration occurs. Then we investigate the significance of the mistakes by inspecting the hidden states in GPT-2. Our results show that text degeneration is likely to be partly caused by exposure bias. We also study the self-reinforcing mechanism of text degeneration, explaining why the mistakes amplify. In sum, our study provides a more concrete foundation for further investigation on exposure bias and text degeneration problems.',\n",
       "  'year': 2021,\n",
       "  'venue': 'BLACKBOXNLP'},\n",
       " {'paperId': '1d00075feb9f2fecc1d035897b98eb2f45461b2e',\n",
       "  'title': 'Semantic Parsing with Semi-Supervised Sequential Autoencoders',\n",
       "  'authorId': '2367821',\n",
       "  'authorName': 'Tomás Kociský',\n",
       "  'abstract': 'We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '48709b65320bf979d19c1874aebf6bf8bd85de88',\n",
       "  'title': \"Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP\",\n",
       "  'authorId': '145046059',\n",
       "  'authorName': 'Anna Rogers',\n",
       "  'abstract': 'A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the community. Our proposal aims to contribute to the development of a consistent standard for data (re-)use, embraced across NLP conferences.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '73ed20862e71fd47ffeff6010af695eff81d28d4',\n",
       "  'title': 'Neural GRANNy at SemEval-2019 Task 2: A combined approach for better modeling of semantic relationships in semantic frame induction',\n",
       "  'authorId': '6153848',\n",
       "  'authorName': 'N. Arefyev',\n",
       "  'abstract': 'We describe our solutions for semantic frame and role induction subtasks of SemEval 2019 Task 2. Our approaches got the highest scores, and the solution for the frame induction problem officially took the first place. The main contributions of this paper are related to the semantic frame induction problem. We propose a combined approach that employs two different types of vector representations: dense representations from hidden layers of a masked language model, and sparse representations based on substitutes for the target word in the context. The first one better groups synonyms, the second one is better at disambiguating homonyms. Extending the context to include nearby sentences improves the results in both cases. New Hearst-like patterns for verbs are introduced that prove to be effective for frame induction. Finally, we propose an approach to selecting the number of clusters in agglomerative clustering.',\n",
       "  'year': 2019,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'a3ba111d74c455305a00572efdd59f23156b44cd',\n",
       "  'title': 'Joint Event Trigger Identification and Event Coreference Resolution with Structured Perceptron',\n",
       "  'authorId': '50007145',\n",
       "  'authorName': 'J. Araki',\n",
       "  'abstract': 'Events and their coreference offer useful semantic and discourse resources. We show that the semantic and discourse aspects of events interact with each other. However, traditional approaches addressed event extraction and event coreference resolution either separately or sequentially, which limits their interactions. This paper proposes a document-level structured learning model that simultaneously identifies event triggers and resolves event coreference. We demonstrate that the joint model outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'bfdfcbd8e71b651feb39d61772556524e18df390',\n",
       "  'title': 'Detecting harassment in real-time as conversations develop',\n",
       "  'authorId': '46614094',\n",
       "  'authorName': 'Wessel Stoop',\n",
       "  'abstract': 'We developed a machine-learning-based method to detect video game players that harass teammates or opponents in chat earlier in the conversation. This real-time technology would allow gaming companies to intervene during games, such as issue warnings or muting or banning a player. In a proof-of-concept experiment on League of Legends data we compute and visualize evaluation metrics for a machine learning classifier as conversations unfold, and observe that the optimal precision and recall of detecting toxic players at each moment in the conversation depends on the confidence threshold of the classifier: the threshold should start low, and increase as the conversation unfolds. How fast this sliding threshold should increase depends on the training set size.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Third Workshop on Abusive Language Online'},\n",
       " {'paperId': '6911e8834c877c3a9f8f2abe5acbcb83a8286a2b',\n",
       "  'title': 'Recursive LSTM Tree Representation for Arc-Standard Transition-Based Dependency Parsing',\n",
       "  'authorId': '23533202',\n",
       "  'authorName': 'Mohab Elkaref',\n",
       "  'abstract': 'We propose a method to represent dependency trees as dense vectors through the recursive application of Long Short-Term Memory networks to build Recursive LSTM Trees (RLTs). We show that the dense vectors produced by Recursive LSTM Trees replace the need for structural features by using them as feature vectors for a greedy Arc-Standard transition-based dependency parser. We also show that RLTs have the ability to incorporate useful information from the bi-LSTM contextualized representation used by Cross and Huang (2016) and Kiperwasser and Goldberg (2016b). The resulting dense vectors are able to express both structural information relating to the dependency tree, as well as sequential information relating to the position in the sentence. The resulting parser only requires the vector representations of the top two items on the parser stack, which is, to the best of our knowledge, the smallest feature set ever published for Arc-Standard parsers to date, while still managing to achieve competitive results.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)'},\n",
       " {'paperId': '00b30ed463625da04166eb78ca617539b41a9846',\n",
       "  'title': 'jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models',\n",
       "  'authorId': '100984698',\n",
       "  'authorName': 'Yada Pruksachatkun',\n",
       "  'abstract': 'We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks. jiant enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, transfer learning, and multitask training experiments. jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published performance on a variety of tasks and models, e.g., RoBERTa and BERT.',\n",
       "  'year': 2020,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3b7de1a544f70e617a9a97c9677f0e04371ba100',\n",
       "  'title': 'That’s Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text',\n",
       "  'authorId': '2021437',\n",
       "  'authorName': 'Manjuan Duan',\n",
       "  'abstract': 'We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving “vicious” ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for selfmonitoring, we find that with a stateof-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. However, by using an SVM ranker to combine the realizer’s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy.',\n",
       "  'year': 2014,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '53396f5401e1b0bd3ba2f2ba69bc09ce1034fc09',\n",
       "  'title': 'Vyākarana: A Colorless Green Benchmark for Syntactic Evaluation in Indic Languages',\n",
       "  'authorId': '1734809795',\n",
       "  'authorName': 'Rajaswa Patil',\n",
       "  'abstract': 'While there has been significant progress towards developing NLU resources for Indic languages, syntactic evaluation has been relatively less explored. Unlike English, Indic languages have rich morphosyntax, grammatical genders, free linear word-order, and highly inflectional morphology. In this paper, we introduce Vyākarana: a benchmark of Colorless Green sentences in Indic languages for syntactic evaluation of multilingual language models. The benchmark comprises four syntax-related tasks: PoS Tagging, Syntax Tree-depth Prediction, Grammatical Case Marking, and Subject-Verb Agreement. We use the datasets from the evaluation tasks to probe five multilingual language models of varying architectures for syntax in Indic languages. Due to its prevalence, we also include a code-switching setting in our experiments. Our results show that the token-level and sentence-level representations from the Indic language models (IndicBERT and MuRIL) do not capture the syntax in Indic languages as efficiently as the other highly multilingual language models. Further, our layer-wise probing experiments reveal that while mBERT, DistilmBERT, and XLM-R localize the syntax in middle layers, the Indic language models do not show such syntactic localization.',\n",
       "  'year': 2021,\n",
       "  'venue': 'MRL'},\n",
       " {'paperId': '06520a24df16f68c64f5c951e80d20c64c39185f',\n",
       "  'title': 'archer at SemEval-2021 Task 1: Contextualising Lexical Complexity',\n",
       "  'authorId': '1873561',\n",
       "  'authorName': 'Irene Russo',\n",
       "  'abstract': 'Evaluating the complexity of a target word in a sentential context is the aim of the Lexical Complexity Prediction task at SemEval-2021. This paper presents the system created to assess single words lexical complexity, combining linguistic and psycholinguistic variables in a set of experiments involving random forest and XGboost regressors. Beyond encoding out-of-context information about the lemma, we implemented features based on pre-trained language models to model the target word’s in-context complexity.',\n",
       "  'year': 2021,\n",
       "  'venue': 'SEMEVAL'},\n",
       " {'paperId': '09132aef0ab608662e48175acbc009f4f82ad0eb',\n",
       "  'title': 'Neural Duplicate Question Detection without Labeled Training Data',\n",
       "  'authorId': '22240011',\n",
       "  'authorName': 'Andreas Rücklé',\n",
       "  'abstract': 'Supervised training of neural models to duplicate question detection in community Question Answering (CQA) requires large amounts of labeled question pairs, which can be costly to obtain. To minimize this cost, recent works thus often used alternative methods, e.g., adversarial domain adaptation. In this work, we propose two novel methods—weak supervision using the title and body of a question, and the automatic generation of duplicate questions—and show that both can achieve improved performances even though they do not require any labeled data. We provide a comparison of popular training strategies and show that our proposed approaches are more effective in many cases because they can utilize larger amounts of data from the CQA forums. Finally, we show that weak supervision with question title and body information is also an effective method to train CQA answer selection models without direct answer supervision.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '1bdb13721109c703486099522edb456a1fe6f44e',\n",
       "  'title': 'Reading between the Lines: Exploring Infilling in Visual Narratives',\n",
       "  'authorId': '37619618',\n",
       "  'authorName': 'Khyathi Raghavi Chandu',\n",
       "  'abstract': 'Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling. We also demonstrate the effects of interposing new text with missing images during inference. The code and the dataset will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.',\n",
       "  'year': 2020,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '1e27ed9a7163ea55d0f4cdcd178ab8a7661b5209',\n",
       "  'title': 'Hierarchical Embeddings for Hypernymy Detection and Directionality',\n",
       "  'authorId': '49254521',\n",
       "  'authorName': 'K. Nguyen',\n",
       "  'abstract': 'We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym–hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-the-art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.',\n",
       "  'year': 2017,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '887bff4baa44bc3ca8dc9f8d67f6c24f7b19eabd',\n",
       "  'title': 'Chain Based RNN for Relation Classification',\n",
       "  'authorId': '39043512',\n",
       "  'authorName': 'J. Ebrahimi',\n",
       "  'abstract': 'We present a novel approach for relation classification, using a recursive neural network (RNN), based on the shortest path between two entities in a dependency graph. Previous works on RNN are based on constituencybased parsing because phrasal nodes in a parse tree can capture compositionality in a sentence. Compared with constituency-based parse trees, dependency graphs can represent relations more compactly. This is particularly important in sentences with distant entities, where the parse tree spans words that are not relevant to the relation. In such cases RNN cannot be trained effectively in a timely manner. However, due to the lack of phrasal nodes in dependency graphs, application of RNN is not straightforward. In order to tackle this problem, we utilize dependency constituent units called chains. Our experiments on two relation classification datasets show that Chain based RNN provides a shallower network, which performs considerably faster and achieves better classification results.',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'bb0baab42046a6d705d89f62c160db730e3d0769',\n",
       "  'title': 'ECNU at SemEval-2016 Task 5: Extracting Effective Features from Relevant Fragments in Sentence for Aspect-Based Sentiment Analysis in Reviews',\n",
       "  'authorId': '3466344',\n",
       "  'authorName': 'M. Jiang',\n",
       "  'abstract': 'This paper describes our systems submitted to the Sentence-level and Text-level AspectBased Sentiment Analysis (ABSA) task (i.e., Task 5) in SemEval-2016. The task involves two phases, namely, Aspect Detection phase and Sentiment Polarity Classification phase. We participated in the second phase of both subtasks in laptop and restaurant domains, which focuses on the sentiment analysis based on the given aspect. In this task, we extracted four types of features (i.e., Sentiment Lexicon Features, Linguistic Features, Topic Model Features and Word2vec Feature) from certain fragments related to aspect rather than the whole sentence. Then the proposed features are fed into supervised classifiers for sentiment analysis. Our submissions rank above average.',\n",
       "  'year': 2016,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '7856f936fcac1b9df21bc7c530169e548d2212c3',\n",
       "  'title': 'Generating Knowledge Graph Paths from Textual Definitions using Sequence-to-Sequence Models',\n",
       "  'authorId': '47264336',\n",
       "  'authorName': 'Victor Prokhorov',\n",
       "  'abstract': 'We present a novel method for mapping unrestricted text to knowledge graph entities by framing the task as a sequence-to-sequence problem. Specifically, given the encoded state of an input text, our decoder directly predicts paths in the knowledge graph, starting from the root and ending at the the target node following hypernym-hyponym relationships. In this way, and in contrast to other text-to-entity mapping systems, our model outputs hierarchically structured predictions that are fully interpretable in the context of the underlying ontology, in an end-to-end manner. We present a proof-of-concept experiment with encouraging results, comparable to those of state-of-the-art systems.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '4b63af049b14cb57e0e82a6d214e73036de72a85',\n",
       "  'title': 'The Effects of Data Size and Frequency Range on Distributional Semantic Models',\n",
       "  'authorId': '1689109',\n",
       "  'authorName': 'Magnus Sahlgren',\n",
       "  'abstract': 'This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'b3893cc63c2ecf5de8787a45121dd466dc3e2d19',\n",
       "  'title': 'Character-based PCFG Induction for Modeling the Syntactic Acquisition of Morphologically Rich Languages',\n",
       "  'authorId': '50496698',\n",
       "  'authorName': 'Lifeng Jin',\n",
       "  'abstract': 'Unsupervised PCFG induction models, which build syntactic structures from raw text, can be used to evaluate the extent to which syntactic knowledge can be acquired from distributional information alone. However, many state-of-the-art PCFG induction models are word-based, meaning that they cannot directly inspect functional affixes, which may provide crucial information for syntactic acquisition in child learners. This work first introduces a neural PCFG induction model that allows a clean ablation of the influence of subword information in grammar induction. Experiments on child-directed speech demonstrate first that the incorporation of subword information results in more accurate grammars with categories that word-based induction models have difficulty finding, and second that this effect is amplified in morphologically richer languages that rely on functional affixes to express grammatical relations. A subsequent evaluation on multilingual treebanks shows that the model with subword information achieves state-ofthe-art results on many languages, further supporting a distributional model of syntactic acquisition.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '416e3ffff2fe2d43343f6b721721a482829f882d',\n",
       "  'title': 'Data-to-text Generation with Entity Modeling',\n",
       "  'authorId': '2990638',\n",
       "  'authorName': 'Ratish Puduppully',\n",
       "  'abstract': 'Recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end. These models rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically, treating entities as nothing more than vocabulary tokens. In this work we propose an entity-centric neural architecture for data-to-text generation. Our model creates entity-specific representations which are dynamically updated. Text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step. We present experiments on the RotoWire benchmark and a (five times larger) new dataset on the baseball domain which we create. Our results show that the proposed model outperforms competitive baselines in automatic and human evaluation.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '7f7df62b8118099d9a59f660519ff5507eec9653',\n",
       "  'title': 'Separating Actor-View from Speaker-View Opinion Expressions using Linguistic Features',\n",
       "  'authorId': '144036063',\n",
       "  'authorName': 'Michael Wiegand',\n",
       "  'abstract': 'We examine different features and classifiers for the categorization of opinion words into actor and speaker view. To our knowledge, this is the first comprehensive work to address sentiment views on the word level taking into consideration opinion verbs, nouns and adjectives. We consider many high-level features requiring only few labeled training data. A detailed feature analysis produces linguistic insights into the nature of sentiment views. We also examine how far global constraints between different opinion words help to increase classification performance. Finally, we show that our (prior) word-level annotation correlates with contextual sentiment views.',\n",
       "  'year': 2016,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '32290d428b50f64d1cfee6ea658dc6ba977b26e9',\n",
       "  'title': 'Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia',\n",
       "  'authorId': '2051500132',\n",
       "  'authorName': 'I. Upadhyay',\n",
       "  'abstract': 'Hateful and offensive content on social media platforms can have negative effects on users and can make online communities more hostile towards certain people and hamper equality, diversity and inclusion. In this paper, we describe our approach to classify homophobia and transphobia in social media comments. We used an ensemble of transformer-based models to build our classifier. Our model ranked 2nd for English, 8th for Tamil and 10th for Tamil-English.',\n",
       "  'year': 2022,\n",
       "  'venue': 'LTEDI'},\n",
       " {'paperId': 'c1c4ea8e8ae4f1a23c1054cda4109d95a8e63a81',\n",
       "  'title': 'QU-BIGIR at SemEval 2017 Task 3: Using Similarity Features for Arabic Community Question Answering Forums',\n",
       "  'authorId': '1864588',\n",
       "  'authorName': 'Marwan Torki',\n",
       "  'abstract': 'In this paper we describe our QU-BIGIR system for the Arabic subtask D of the SemEval 2017 Task 3. Our approach builds on our participation in the past version of the same subtask. This year, our system uses different similarity measures that encodes lexical and semantic pairwise similarity of text pairs. In addition to well known similarity measures such as cosine similarity, we use other measures based on the summary statistics of word embedding representation for a given text. To rank a list of candidate question answer pairs for a given question, we learn a linear SVM classifier over our similarity features. Our best resulting run came second in subtask D with a very competitive performance to the first-ranking system.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': '8e7d063c681557c94382ff3da6415d3720fe11a7',\n",
       "  'title': 'Stance Detection with Bidirectional Conditional Encoding',\n",
       "  'authorId': '1736067',\n",
       "  'authorName': 'Isabelle Augenstein',\n",
       "  'abstract': 'Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be \"positive\", negative\" or \"neutral\". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results.',\n",
       "  'year': 2016,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '76d97b53c661bfca4fe8f4ce253c13a45645a44b',\n",
       "  'title': 'Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents',\n",
       "  'authorId': '2689239',\n",
       "  'authorName': 'Jack Hessel',\n",
       "  'abstract': 'Images and text co-occur constantly on the web, but explicit links between images and sentences (or other intra-document textual units) are often not present. We present algorithms that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difficulty, ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We find that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can suffice to predict links between specific sentences and specific images within the same document at test time.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '187002e56ca67797a4f45352680257f342b62fdd',\n",
       "  'title': 'Toward Cross-Domain Engagement Analysis in Medical Notes',\n",
       "  'authorId': '144063596',\n",
       "  'authorName': 'Sara Rosenthal',\n",
       "  'abstract': 'We present a novel annotation task evaluating a patient’s engagement with their health care regimen. The concept of engagement supplements the traditional concept of adherence with a focus on the patient’s affect, lifestyle choices, and health goal status. We describe an engagement annotation task across two patient note domains: traditional clinical notes and a novel domain, care manager notes, where we find engagement to be more common. The annotation task resulted in a kappa of .53, suggesting strong annotator intuitions regarding engagement-bearing language. In addition, we report the results of a series of preliminary engagement classification experiments using domain adaptation.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BioNLP'},\n",
       " {'paperId': '678e60a0b8f5e7a4307afc4f4db3a94f10791f43',\n",
       "  'title': 'Non-Sentential Utterances in Dialogue: A: Corpus-Based Study',\n",
       "  'authorId': '144151273',\n",
       "  'authorName': 'R. Fernández',\n",
       "  'abstract': 'Dialogue is full of intuitively complete utterances that are not sentential in their outward form, most prototypically the \"short answers\" used to respond to queries. As is well known, processing such non-sentential utterances (NSUs) is a difficult problem on both theoretical and computational grounds. In this paper we present a corpus-based study of NSUs. We propose a comprehensive, theoretically grounded classification of NSUs in dialogue based on a sub-portion of the British National Corpus (BNC). The study suggests that the interpretation of NSUs is amenable to resolution using a relatively intricate grammar combined with an utterance dynamics approach. That is, a strategy that keeps track of a highly structured dialogue record of entities that get introduced into context as a result of utterances. Complex, domain-based reasoning is not, on the whole, very much in evidence.',\n",
       "  'year': 2002,\n",
       "  'venue': 'SIGDIAL Workshop'},\n",
       " {'paperId': 'e4179bf0963012f9f13ce0ca61d3cdd12a87b336',\n",
       "  'title': 'Entity and Evidence Guided Document-Level Relation Extraction',\n",
       "  'authorId': '152530947',\n",
       "  'authorName': 'Kevin Huang',\n",
       "  'abstract': 'Document-level relation extraction is a challenging task, requiring reasoning over multiple sentences to predict a set of relations in a document. In this paper, we propose a novel framework E2GRE (Entity and Evidence Guided Relation Extraction) that jointly extracts relations and the underlying evidence sentences by using large pretrained language model (LM) as input encoder. First, we propose to guide the pretrained LM’s attention mechanism to focus on relevant context by using attention probabilities as additional features for evidence prediction. Furthermore, instead of feeding the whole document into pretrained LMs to obtain entity representation, we concatenate document text with head entities to help LMs concentrate on parts of the document that are more related to the head entity. Our E2GRE jointly learns relation extraction and evidence prediction effectively, showing large gains on both these tasks, which we find are highly correlated.',\n",
       "  'year': 2021,\n",
       "  'venue': 'REPL4NLP'},\n",
       " {'paperId': '9298f666d0ef7f88d8ea024277cb63fcfeb9f745',\n",
       "  'title': 'Sentence Simplification for Semantic Role Labelling and Information Extraction',\n",
       "  'authorId': '2106505837',\n",
       "  'authorName': 'R. Evans',\n",
       "  'abstract': 'In this paper, we report on the extrinsic evaluation of an automatic sentence simplification method with respect to two NLP tasks: semantic role labelling (SRL) and information extraction (IE). The paper begins with our observation of challenges in the intrinsic evaluation of sentence simplification systems, which motivates the use of extrinsic evaluation of these systems with respect to other NLP tasks. We describe the two NLP systems and the test data used in the extrinsic evaluation, and present arguments and evidence motivating the integration of a sentence simplification step as a means of improving the accuracy of these systems. Our evaluation reveals that their performance is improved by the simplification step: the SRL system is better able to assign semantic roles to the majority of the arguments of verbs and the IE system is better able to identify fillers for all IE template slots.',\n",
       "  'year': 2019,\n",
       "  'venue': 'RANLP'},\n",
       " {'paperId': 'd0b228b10ce2504b049b6abef690eff9b34ba68e',\n",
       "  'title': 'Data Selection With Fewer Words',\n",
       "  'authorId': '2868598',\n",
       "  'authorName': 'Amittai Axelrod',\n",
       "  'abstract': 'We present a method that improves data selection by combining a hybrid word/part-of-speech representation for corpora, with the idea of distinguishing between rare and frequent events. We validate our approach using data selection for machine translation, and show that it maintains or improves BLEU and TER translation scores while substantially improving vocabulary coverage and reducing data selection model size. Paradoxically, the coverage improvement is achieved by abstracting away over 97% of the total training corpus vocabulary using simple part-of-speech tags during the data selection process.',\n",
       "  'year': 2015,\n",
       "  'venue': 'WMT@EMNLP'},\n",
       " {'paperId': '2ff4b9ca75a37cec1607f1f49333b948b1c8d08b',\n",
       "  'title': 'SemEval-2014 Task 4: Aspect Based Sentiment Analysis',\n",
       "  'authorId': '3467179',\n",
       "  'authorName': 'Maria Pontiki',\n",
       "  'abstract': 'Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen). SemEval2014 Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams.',\n",
       "  'year': 2014,\n",
       "  'venue': 'COLING 2014'},\n",
       " {'paperId': '3cbc10619130d862969bf1a959dfc4338781b1dd',\n",
       "  'title': 'Diversity-aware Event Prediction based on a Conditional Variational Autoencoder with Reconstruction',\n",
       "  'authorId': '51195367',\n",
       "  'authorName': 'Hirokazu Kiyomaru',\n",
       "  'abstract': 'Typical event sequences are an important class of commonsense knowledge. Formalizing the task as the generation of a next event conditioned on a current event, previous work in event prediction employs sequence-to-sequence (seq2seq) models. However, what can happen after a given event is usually diverse, a fact that can hardly be captured by deterministic models. In this paper, we propose to incorporate a conditional variational autoencoder (CVAE) into seq2seq for its ability to represent diverse next events as a probabilistic distribution. We further extend the CVAE-based seq2seq with a reconstruction mechanism to prevent the model from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversity-aware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of precision and that the reconstruction mechanism improves the recall of CVAE-based models without sacrificing precision.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '91fed2f5718b9c1dc25b768c8aaf33455f0404c8',\n",
       "  'title': 'Joint Energy-based Model Training for Better Calibrated Natural Language Understanding Models',\n",
       "  'authorId': '3083253',\n",
       "  'authorName': 'Tianxing He',\n",
       "  'abstract': 'In this work, we explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. Our experiments show that EBM training can help the model reach a better calibration that is competitive to strong baselines, with little or no loss in accuracy. We discuss three variants of energy functions (namely scalar, hidden, and sharp-hidden) that can be defined on top of a text encoder, and compare them in experiments. Due to the discreteness of text data, we adopt noise contrastive estimation (NCE) to train the energy-based model. To make NCE training more effective, we train an auto-regressive noise model with the masked language model (MLM) objective.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EACL'},\n",
       " {'paperId': '99908574b73cb1461d7d15c527a87c2217cb7283',\n",
       "  'title': 'Model Selection for Type-Supervised Learning with Application to POS Tagging',\n",
       "  'authorId': '3259253',\n",
       "  'authorName': 'Kristina Toutanova',\n",
       "  'abstract': 'Model selection (picking, for example, the feature set and the regularization strength) is crucial for building high-accuracy NLP models. In supervised learning, we can estimate the accuracy of a model on a subset of the labeled data and choose the model with the highest accuracy. In contrast, here we focus on type-supervised learning, which uses constraints over the possible labels for word types for supervision, and labeled data is either not available or very small. For the setting where no labeled data is available, we perform a comparative study of previously proposed and one novel model selection criterion on type-supervised POS-tagging in nine languages. For the setting where a small labeled set is available, we show that the set should be used for semi-supervised learning rather than for model selection only ‐ using it for model selection reduces the error by less than 5%, whereas using it for semi-supervised learning reduces the error by 44%.',\n",
       "  'year': 2015,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': 'f30f6d3d82c83db21cb241ec76eccb76fd4cec81',\n",
       "  'title': 'A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora',\n",
       "  'authorId': '1732180',\n",
       "  'authorName': 'Éric Gaussier',\n",
       "  'abstract': 'We present a geometric view on bilingual lexicon extraction from comparable corpora, which allows to re-interpret the methods proposed so far and identify unresolved problems. This motivates three new methods that aim at solving these problems. Empirical evaluation shows the strengths and weaknesses of these methods, as well as a significant gain in the accuracy of extracted lexicons.',\n",
       "  'year': 2004,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'd706645fbbc6edfad5fb642b1dfc3019fcabbd99',\n",
       "  'title': 'The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation',\n",
       "  'authorId': '37796923',\n",
       "  'authorName': 'Marzena Karpinska',\n",
       "  'abstract': 'Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '9300d37a6526fe9ffc4465608e86e1cd89f73add',\n",
       "  'title': 'SemEval-2015 Task 8: SpaceEval',\n",
       "  'authorId': '1707726',\n",
       "  'authorName': 'J. Pustejovsky',\n",
       "  'abstract': 'Human languages exhibit a variety of strategies for communicating spatial information, including toponyms, spatial nominals, locations that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': 'aab6f539bab791aea9ec3e80dfe13a1c017204d7',\n",
       "  'title': 'Stochastic Top-k ListNet',\n",
       "  'authorId': '30687479',\n",
       "  'authorName': 'Tianyi Luo',\n",
       "  'abstract': 'ListNet is a well-known listwise learning to rank model and has gained much attention in recent years. A particular problem of ListNet, however, is the high computation complexity in model training, mainly due to the large number of object permutations involved in computing the gradients. This paper proposes a stochastic ListNet approach which computes the gradient within a bounded permutation subset. It significantly reduces the computation complexity of model training and allows extension to Top-k models, which is impossible with the conventional implementation based on full-set permutations. Meanwhile, the new approach utilizes partial ranking information of human labels, which helps improve model quality. Our experiments demonstrated that the stochastic ListNet method indeed leads to better ranking performance and speeds up the model training remarkably.',\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '09ba232886fb2aaca21b16fd9b684c6ce29de1b6',\n",
       "  'title': 'Something’s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features',\n",
       "  'authorId': '2689239',\n",
       "  'authorName': 'Jack Hessel',\n",
       "  'abstract': 'Controversial posts are those that split the preferences of a community, receiving both significant positive and significant negative feedback. Our inclusion of the word “community” here is deliberate: what is controversial to some audiences may not be so to others. Using data from several different communities on reddit.com, we predict the ultimate controversiality of posts, leveraging features drawn from both the textual content and the tree structure of the early comments that initiate the discussion. We find that even when only a handful of comments are available, e.g., the first 5 comments made within 15 minutes of the original post, discussion features often add predictive capacity to strong content-and- rate only baselines. Additional experiments on domain transfer suggest that conversation- structure features often generalize to other communities better than conversation-content features do.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '5b2a13b85ef9d3ad61639c3b400e447f3217ec98',\n",
       "  'title': 'Answer Generation for Retrieval-based Question Answering Systems',\n",
       "  'authorId': '23608432',\n",
       "  'authorName': 'Chao-Chun Hsu',\n",
       "  'abstract': 'Recent advancements in transformer-based models have greatly improved the ability of Question Answering (QA) systems to provide correct answers; in particular, answer sentence selection (AS2) models, core components of retrieval-based systems, have achieved impressive results. While generally effective, these models fail to provide a satisfying answer when all retrieved candidates are of poor quality, even if they contain correct information. In AS2, models are trained to select the best answer sentence among a set of candidates retrieved for a given question. In this work, we propose to generate answers from a set of AS2 top candidates. Rather than selecting the best candidate, we train a sequence to sequence transformer model to generate an answer from a candidate set. Our tests on three English AS2 datasets show improvement up to 32 absolute points in accuracy over the state of the art.',\n",
       "  'year': 2021,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': 'a55923e8f85b287d984722b4881a80f08986feb1',\n",
       "  'title': 'Combining Mention Context and Hyperlinks from Wikipedia for Named Entity Disambiguation',\n",
       "  'authorId': '1760443',\n",
       "  'authorName': 'Ander Barrena',\n",
       "  'abstract': 'Named entity disambiguation is the task of linking entity mentions to their intended referent, as represented in a Knowledge Base, usually derived from Wikipedia. In this paper, we combine local mention context and global hyperlink structure from Wikipedia in a probabilistic framework. We test our method in eight datasets, improving the state-of-the-art results in five. Our results show that the two models of context, namely, words in the context and hyperlink pathways to other entities in the context, are complementary. Our results are not tuned to any of the datasets, showing that it is robust to out-of-domain scenarios, and that further improvements are possible.',\n",
       "  'year': 2015,\n",
       "  'venue': '*SEM@NAACL-HLT'},\n",
       " {'paperId': '17a5889f93a5fd0060b73c5e8ef2e580b52de335',\n",
       "  'title': 'Towards a General Abstract Meaning Representation Corpus for Brazilian Portuguese',\n",
       "  'authorId': '35664665',\n",
       "  'authorName': 'Marco Antonio Sobrevilla Cabezudo',\n",
       "  'abstract': 'Abstract Meaning Representation (AMR) is a recent and prominent semantic representation with good acceptance and several applications in the Natural Language Processing area. For English, there is a large annotated corpus (with approximately 39K sentences) that supports the research with the representation. However, to the best of our knowledge, there is only one restricted corpus for Portuguese, which contains 1,527 sentences. In this context, this paper presents an effort to build a general purpose AMR-annotated corpus for Brazilian Portuguese by translating and adapting AMR English guidelines. Our results show that such approach is feasible, but there are some challenging phenomena to solve. More than this, efforts are necessary to increase the coverage of the corresponding lexical resource that supports the annotation.',\n",
       "  'year': 2019,\n",
       "  'venue': 'LAW@ACL'},\n",
       " {'paperId': '09a34aad92e6f416c81f47e60de0809616b49cce',\n",
       "  'title': 'Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text',\n",
       "  'authorId': '108508643',\n",
       "  'authorName': 'Ian Porada',\n",
       "  'abstract': 'Modeling semantic plausibility requires commonsense knowledge about the world and has been used as a testbed for exploring various knowledge representations. Previous work has focused specifically on modeling physical plausibility and shown that distributional methods fail when tested in a supervised setting. At the same time, distributional models, namely large pretrained language models, have led to improved results for many natural language understanding tasks. In this work, we show that these pretrained language models are in fact effective at modeling physical plausibility in the supervised setting. We therefore present the more difficult problem of learning to model physical plausibility directly from text. We create a training set by extracting attested events from a large corpus, and we provide a baseline for training on these attested events in a self-supervised manner and testing on a physical plausibility task. We believe results could be further improved by injecting explicit commonsense knowledge into a distributional model.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': '16a3498a4d61aa3ac222b8750f661c36e046db6c',\n",
       "  'title': 'Classifying Electronic Consults for Triage Status and Question Type.',\n",
       "  'authorId': '15491192',\n",
       "  'authorName': 'Xiyu Ding',\n",
       "  'abstract': 'Electronic consult (eConsult) systems allow specialists more flexibility to respond to referrals more efficiently, thereby increasing access in under-resourced healthcare settings like safety net systems. Understanding the usage patterns of eConsult system is an important part of improving specialist efficiency. In this work, we develop and apply classifiers to a dataset of eConsult questions from primary care providers to specialists, classifying the messages for how they were triaged by the specialist office, and the underlying type of clinical question posed by the primary care provider. We show that pre-trained transformer models are strong baselines, with improving performance from domain-specific training and shared representations.',\n",
       "  'year': 2020,\n",
       "  'venue': 'Proceedings of the conference. Association for Computational Linguistics. Meeting'},\n",
       " {'paperId': '34ed201f2da24dcfc06b65f7ffa58d61f2297c4d',\n",
       "  'title': 'Phylogenetic simulations over constraint-based grammar formalisms',\n",
       "  'authorId': '144184797',\n",
       "  'authorName': 'Andrew Lamont',\n",
       "  'abstract': 'Computational phylogenetics has been shown to be effective over grammatical characteristics. Recent work suggests that constraintbased formalisms are compatible with such an approach (Eden, 2013). In this paper, we report on simulations to determine how useful constraint-based formalisms are in phylogenetic research and under what conditions.',\n",
       "  'year': 2016,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'e56a36dd9efc573bc4dab3d3d2fcc183448b6a31',\n",
       "  'title': 'Unsupervised Entity Linking with Abstract Meaning Representation',\n",
       "  'authorId': '34741133',\n",
       "  'authorName': 'Xiaoman Pan',\n",
       "  'abstract': 'Most successful Entity Linking (EL) methods aim to link mentions to their referent entities in a structured Knowledge Base (KB) by comparing their respective contexts, often using similarity measures. While the KB structure is given, current methods have suffered from impoverished information representations on the mention side. In this paper, we demonstrate the effectiveness of Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to select high quality sets of entity “collaborators” to feed a simple similarity measure (Jaccard) to link entity mentions. Experimental results show that AMR captures contextual properties discriminative enough to make linking decisions, without the need for EL training data, and that system with AMR parsing output outperforms hand labeled traditional semantic roles as context representation for EL. Finally, we show promising preliminary results for using AMR to select sets of “coherent” entity mentions for collective entity linking 1 .',\n",
       "  'year': 2015,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'e29d45aa2e86f7504718a879f71df5266d5ef4c7',\n",
       "  'title': 'Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference',\n",
       "  'authorId': '2115829571',\n",
       "  'authorName': 'Yuxia Wang',\n",
       "  'abstract': 'Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels due to its subjectivity. Several recent efforts have been made to acknowledge and embrace the existence of ambiguity, and explore how to capture the human disagreement distribution. In contrast with directly learning from gold ambiguity labels, relying on special resource, we argue that the model has naturally captured the human ambiguity distribution as long as it’s calibrated, i.e. the predictive probability can reflect the true correctness likelihood. Our experiments show that when model is well-calibrated, either by label smoothing or temperature scaling, it can obtain competitive performance as prior work, on both divergence scores between predictive probability and the true human opinion distribution, and the accuracy. This reveals the overhead of collecting gold ambiguity labels can be cut, by broadly solving how to calibrate the NLI network.',\n",
       "  'year': 2022,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '4f7c69c4e3f662d277da5a51284f82eab1514af4',\n",
       "  'title': 'Dependency Parsing as Sequence Labeling with Head-Based Encoding and Multi-Task Learning',\n",
       "  'authorId': '1793857',\n",
       "  'authorName': 'Ophélie Lacroix',\n",
       "  'abstract': 'Dependency parsing as sequence labeling has recently proved to be a relevant alternative to the traditional transitionand graph-based approaches. It offers a good trade-off between parsing accuracy and speed. However, recent work on dependency parsing as sequence labeling ignore the pre-processing time of Part-of-Speech tagging – which is required for this task – in the evaluation of speed while other studies showed that Part-of-Speech tags are not essential to achieve state-ofthe-art parsing scores. In this paper, we compare the accuracy and speed of shared and stacked multi-task learning strategies – as well as a strategy that combines both – to learn Part-of-Speech tagging and dependency parsing in a single sequence labeling pipeline. In addition, we propose an alternative encoding of the dependencies as labels which does not use Part-of-Speech tags and improves dependency parsing accuracy for most of the languages we evaluate.',\n",
       "  'year': 2019,\n",
       "  'venue': 'Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, SyntaxFest 2019)'},\n",
       " {'paperId': '6dd9e5afc0e06cba5f22157a6b7e65b099375aac',\n",
       "  'title': 'Computational Support for Finding Word Classes: A Case Study of Abui',\n",
       "  'authorId': '21002356',\n",
       "  'authorName': 'Olga Zamaraeva',\n",
       "  'abstract': 'We present a system that automatically groups verb stems into inflection classes, performing a case study of Abui verbs. Starting from a relatively small number of fully glossed Abui sentences, we train a morphological precision grammar and use it to automatically analyze and gloss words from the unglossed portion of our corpus. Then we group stems into classes based on their cooccurrence patterns with several prefix series of interest. We compare our results to a curated collection of elicited examples and illustrate how our approach can be useful for field linguists as it can help them refine their analysis by accounting for more patterns in the data.',\n",
       "  'year': 2017,\n",
       "  'venue': ''},\n",
       " {'paperId': '1dc912b222a56ae4f189a2e95db6e6e69625772e',\n",
       "  'title': 'Minimal Supervision for Morphological Inflection',\n",
       "  'authorId': '40060272',\n",
       "  'authorName': 'Omer Goldman',\n",
       "  'abstract': 'Neural models for the various flavours of morphological reinflection tasks have proven to be extremely accurate given ample labeled data, yet labeled data may be slow and costly to obtain. In this work we aim to overcome this annotation bottleneck by bootstrapping labeled data from a seed as small as five labeled inflection tables, accompanied by a large bulk of unlabeled text. Our bootstrapping method exploits the orthographic and semantic regularities in morphological systems in a two-phased setup, where word tagging based on analogies is followed by word pairing based on distances. Our experiments with the Paradigm Cell Filling Problem over eight typologically different languages show that in languages with relatively simple morphology, orthographic regularities on their own allow inflection models to achieve respectable accuracy. Combined orthographic and semantic regularities alleviate difficulties with particularly complex morpho-phonological systems. We further show that our bootstrapping methods substantially outperform hallucination-based methods commonly used for overcoming the annotation bottleneck in morphological reinflection tasks.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'a82bf098cbc8a7d3f3297c645ce37faea002d2d9',\n",
       "  'title': 'Leveraging Principal Parts for Morphological Inflection',\n",
       "  'authorId': '40461530',\n",
       "  'authorName': 'L. Liu',\n",
       "  'abstract': 'This paper presents the submission by the CU Ling team from the University of Colorado to SIGMORPHON 2020 shared task 0 on morphological inflection. The task is to generate the target inflected word form given a lemma form and a target morphosyntactic description. Our system uses the Transformer architecture. Our overall approach is to treat the morphological inflection task as a paradigm cell filling problem and to design the system to leverage principal parts information for better morphological inflection when the training data is limited. We train one model for each language separately without external data. The overall average performance of our submission ranks the first in both average accuracy and Levenshtein distance from the gold inflection among all submissions including those using external resources.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SIGMORPHON'},\n",
       " {'paperId': '9fbc16debacc5cd9b3188b7ecb73f846c3fd8b97',\n",
       "  'title': \"Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits\",\n",
       "  'authorId': '3422710',\n",
       "  'authorName': 'Julia Kreutzer',\n",
       "  'abstract': 'Training data for machine translation (MT) is often sourced from a multitude of large corpora that are multi-faceted in nature, e.g. containing contents from multiple domains or different levels of quality or complexity. Naturally, these facets do not occur with equal frequency, nor are they equally important for the test scenario at hand. In this work, we propose to optimize this balance jointly with MT model parameters to relieve system developers from manual schedule design. A multi-armed bandit is trained to dynamically choose between facets in a way that is most beneficial for the MT system. We evaluate it on three different multi-facet applications: balancing translationese and natural training data, or data from multiple domains or multiple language pairs. We find that bandit learning leads to competitive MT systems across tasks, and our analysis provides insights into its learned strategies and the underlying data sets.',\n",
       "  'year': 2021,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'f1a91516a7801dbf3d8e54f48bcfa77743820b1e',\n",
       "  'title': 'Bringing Order to Neural Word Embeddings with Embeddings Augmented by Random Permutations (EARP)',\n",
       "  'authorId': '2696466',\n",
       "  'authorName': 'T. Cohen',\n",
       "  'abstract': 'Word order is clearly a vital part of human language, but it has been used comparatively lightly in distributional vector models. This paper presents a new method for incorporating word order information into word vector embedding models by combining the benefits of permutation-based order encoding with the more recent method of skip-gram with negative sampling. The new method introduced here is called Embeddings Augmented by Random Permutations (EARP). It operates by applying permutations to the coordinates of context vector representations during the process of training. Results show an 8% improvement in accuracy on the challenging Bigger Analogy Test Set, and smaller but consistent improvements on other analogy reference sets. These findings demonstrate the importance of order-based information in analogical retrieval tasks, and the utility of random permutations as a means to augment neural embeddings.',\n",
       "  'year': 2018,\n",
       "  'venue': 'CoNLL'},\n",
       " {'paperId': 'bd2239d6cea24604ff3687d37f3d475f6d7b12bc',\n",
       "  'title': 'Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?',\n",
       "  'authorId': '51402451',\n",
       "  'authorName': 'Zhengzhong Liang',\n",
       "  'abstract': 'Large pretrained language models (LM) have been used successfully for multi-hop question answering. However, most of these directions are not interpretable, as they do not make the inference hops necessary to explain a candidate answer explicitly. In this work, we investigate the capability of a state-of-the-art transformer LM to generate explicit inference hops, i.e., to infer a new statement necessary to answer a question given some premise input statements. Our analysis shows that such LMs can generate new statements for some simple inference types, but performance remains poor for complex, real-world inference types such as those that require monotonicity, composition, and commonsense knowledge.',\n",
       "  'year': 2020,\n",
       "  'venue': 'INSIGHTS'},\n",
       " {'paperId': '44e42f12708cf67c4f1161b6f953364e13176360',\n",
       "  'title': 'Visualizing Linguistic Change as Dimension Interactions',\n",
       "  'authorId': '7204694',\n",
       "  'authorName': 'Christin Schätzle',\n",
       "  'abstract': 'Historical change typically is the result of complex interactions between several linguistic factors. Identifying the relevant factors and understanding how they interact across the temporal dimension is the core remit of historical linguistics. With respect to corpus work, this entails a separate annotation, extraction and painstaking pair-wise comparison of the relevant bits of information. This paper presents a significant extension of HistoBankVis, a multilayer visualization system which allows a fast and interactive exploration of complex linguistic data. Linguistic factors can be understood as data dimensions which show complex interrelationships. We model these relationships with the Parallel Sets technique. We demonstrate the powerful potential of this technique by applying the system to understanding the interaction of case, grammatical relations and word order in the history of Icelandic.',\n",
       "  'year': 2019,\n",
       "  'venue': 'LChange@ACL'},\n",
       " {'paperId': 'e1d585b0a45a0de476f2c2976e139b4f1fea7373',\n",
       "  'title': 'Sensible: L2 Translation Assistance by Emulating the Manual Post-Editing Process',\n",
       "  'authorId': '40268710',\n",
       "  'authorName': 'Liling Tan',\n",
       "  'abstract': 'This paper describes the Post-Editor Z system submitted to the L2 writing assistant task in SemEval-2014. The aim of task is to build a translation assistance system to translate untranslated sentence fragments. This is not unlike the task of post-editing where human translators improve machine-generated translations. Post-Editor Z emulates the manual process of post-editing by (i) crawling and extracting parallel sentences that contain the untranslated fragments from a Web-based translation memory, (ii) extracting the possible translations of the fragments indexed by the translation memory and (iii) applying simple cosine-based sentence similarity to rank possible translations for the untranslated fragment.',\n",
       "  'year': 2014,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '51bdd0b839fe8ab1daafa05d1ed03edff601f4c7',\n",
       "  'title': 'Named Entity Recognition using an HMM-based Chunk Tagger',\n",
       "  'authorId': '143740945',\n",
       "  'authorName': 'Guodong Zhou',\n",
       "  'abstract': 'This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules.',\n",
       "  'year': 2002,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '9d703b7dea1d2e79616e39b17bf19a7a6566f67e',\n",
       "  'title': 'Letting Emotions Flow: Success Prediction by Modeling the Flow of Emotions in Books',\n",
       "  'authorId': '2513454',\n",
       "  'authorName': 'Suraj Maharjan',\n",
       "  'abstract': 'Books have the power to make us feel happiness, sadness, pain, surprise, or sorrow. An author’s dexterity in the use of these emotions captivates readers and makes it difficult for them to put the book down. In this paper, we model the flow of emotions over a book using recurrent neural networks and quantify its usefulness in predicting success in books. We obtained the best weighted F1-score of 69% for predicting books’ success in a multitask setting (simultaneously predicting success and genre of books).',\n",
       "  'year': 2018,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': 'efd803e9ca5f36b7753fbfd11ec098e3567f4d3d',\n",
       "  'title': 'Rewards with Negative Examples for Reinforced Topic-Focused Abstractive Summarization',\n",
       "  'authorId': '46180513',\n",
       "  'authorName': 'Khalil Mrini',\n",
       "  'abstract': 'We consider the problem of topic-focused abstractive summarization, where the goal is to generate an abstractive summary focused on a particular topic, a phrase of one or multiple words. We hypothesize that the task of generating topic-focused summaries can be improved by showing the model what it must not focus on. We introduce a deep reinforcement learning approach to topic-focused abstractive summarization, trained on rewards with a novel negative example baseline. We define the input in this problem as the source text preceded by the topic. We adapt the CNN-Daily Mail and New York Times summarization datasets for this task. We then show through experiments on existing rewards that the use of a negative example baseline can outperform the use of a self-critical baseline, in Rouge, BERTScore, and human evaluation metrics.',\n",
       "  'year': 2021,\n",
       "  'venue': 'NEWSUM'},\n",
       " {'paperId': 'f18a207f264a12de38389875e527cf21716219c5',\n",
       "  'title': 'Irish National Morphology Database: a high-accuracy open-source dataset of Irish words',\n",
       "  'authorId': '145315995',\n",
       "  'authorName': 'M. Mechura',\n",
       "  'abstract': 'The Irish National Morphology Database is a human-verified, Official Standard-compliant dataset containing the inflected forms and other morpho-syntactic properties of Irish nouns, adjectives, verbs and prepositions. It is being developed by Foras na Gaeilge as part of the New English-Irish Dictionary project. This paper introduces this dataset and its accompanying software library Gramadan.',\n",
       "  'year': 2014,\n",
       "  'venue': ''},\n",
       " {'paperId': '4a40e159ad4ff3814dcd711a424890d8cb7c35c9',\n",
       "  'title': 'CNRC at SemEval-2016 Task 1: Experiments in Crosslingual Semantic Textual Similarity',\n",
       "  'authorId': '37315056',\n",
       "  'authorName': 'Chi-kiu Lo',\n",
       "  'abstract': 'We describe the systems entered by the National Research Council Canada in the SemEval-2016 Task1: Crosslingual Semantic Textual Similarity. We tried two approaches: One computes a true crosslingual similarity based on features extracted from lexical semantics and shallow semantic structures of the source and target fragments, combined using a linear model. The other approach relies on Statistical Machine Translation, followed by a monolingual semantic similarity, relying again on syntactic and semantic features. We report our experiments using trial data, as well as ofﬁcial ﬁnal results on the evaluation data.',\n",
       "  'year': 2016,\n",
       "  'venue': '*SEMEVAL'},\n",
       " {'paperId': '211e70d0845e8db8f3f964e54f0d6b4eaff866da',\n",
       "  'title': 'Cross-referencing Using Fine-grained Topic Modeling',\n",
       "  'authorId': '2407114',\n",
       "  'authorName': 'Jeffrey Lund',\n",
       "  'abstract': 'Cross-referencing, which links passages of text to other related passages, can be a valuable study aid for facilitating comprehension of a text. However, cross-referencing requires first, a comprehensive thematic knowledge of the entire corpus, and second, a focused search through the corpus specifically to find such useful connections. Due to this, cross-reference resources are prohibitively expensive and exist only for the most well-studied texts (e.g. religious texts). We develop a topic-based system for automatically producing candidate cross-references which can be easily verified by human annotators. Our system utilizes fine-grained topic modeling with thousands of highly nuanced and specific topics to identify verse pairs which are topically related. We demonstrate that our system can be cost effective compared to having annotators acquire the expertise necessary to produce cross-reference resources unaided.',\n",
       "  'year': 2019,\n",
       "  'venue': 'NAACL'},\n",
       " {'paperId': '7dc098ba68a2f4d10a344c43fcf7305926097136',\n",
       "  'title': 'Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint Embedding for Chinese Character Variation Representation',\n",
       "  'authorId': '1695957219',\n",
       "  'authorName': 'Zhuoren Jiang',\n",
       "  'abstract': 'The task of Chinese text spam detection is very challenging due to both glyph and phonetic variations of Chinese characters. This paper proposes a novel framework to jointly model Chinese variational, semantic, and contextualized representations for Chinese text spam detection task. In particular, a Variation Family-enhanced Graph Embedding (VFGE) algorithm is designed based on a Chinese character variation graph. The VFGE can learn both the graph embeddings of the Chinese characters (local) and the latent variation families (global). Furthermore, an enhanced bidirectional language model, with a combination gate function and an aggregation learning function, is proposed to integrate the graph and text information while capturing the sequential information. Extensive experiments have been conducted on both SMS and review datasets, to show the proposed method outperforms a series of state-of-the-art models for Chinese spam detection.',\n",
       "  'year': 2019,\n",
       "  'venue': 'EMNLP'},\n",
       " {'paperId': 'ecd2c12415a507b2a28b9e66abd7f83e09970093',\n",
       "  'title': 'Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning',\n",
       "  'authorId': '8200875',\n",
       "  'authorName': 'Tiancheng Zhao',\n",
       "  'abstract': 'This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.',\n",
       "  'year': 2016,\n",
       "  'venue': 'SIGDIAL Conference'},\n",
       " {'paperId': '974825697ec6734c8ce298ac6d40e313e5008478',\n",
       "  'title': 'Extending a Dutch Text-to-Pictograph Converter to English and Spanish',\n",
       "  'authorId': '3451971',\n",
       "  'authorName': 'L. Sevens',\n",
       "  'abstract': 'We describe how a Dutch Text-to-Pictograph translation system, designed to augment written text for people with Intellectual or Developmental Disabilities (IDD), was adapted in order to be usable for English and Spanish. The original system has a language-independent design. As far as the textual part is concerned, it is adaptable to all natural languages for which interlingual WordNet [1] links, lemmatizers and part-of-speech taggers are available. As far as the pictographic part is concerned, it can be modified for various pictographic languages. The evaluations show that our results are in line with the performance of the original Dutch system. Text-to-Pictograph translation has a wide application potential in the domain of Augmentative and Alternative Communication (AAC). The system will be released as an open source product. Index Terms: Augmentative and Alternative Communication, Pictographic Languages, Text-to-Pictograph Translation',\n",
       "  'year': 2015,\n",
       "  'venue': 'SLPAT@Interspeech'},\n",
       " {'paperId': '993b830e988c3bff730e7014219cd8c710471ad5',\n",
       "  'title': 'Cross-lingual dependency parsing for closely related languages - Helsinki’s submission to VarDial 2017',\n",
       "  'authorId': '143675545',\n",
       "  'authorName': 'J. Tiedemann',\n",
       "  'abstract': 'This paper describes the submission from the University of Helsinki to the shared task on cross-lingual dependency parsing at VarDial 2017. We present work on annotation projection and treebank translation that gave good results for all three target languages in the test set. In particular, Slovak seems to work well with information coming from the Czech treebank, which is in line with related work. The attachment scores for cross-lingual models even surpass the fully supervised models trained on the target language treebank. Croatian is the most difficult language in the test set and the improvements over the baseline are rather modest. Norwegian works best with information coming from Swedish whereas Danish contributes surprisingly little.',\n",
       "  'year': 2017,\n",
       "  'venue': 'VarDial'},\n",
       " {'paperId': '1917c1deb0bf5320ab068cdb8a904859ca804101',\n",
       "  'title': 'KU-CST at the SIGMORPHON 2020 Task 2 on Unsupervised Morphological Paradigm Completion',\n",
       "  'authorId': '2650725',\n",
       "  'authorName': 'Manex Agirrezabal',\n",
       "  'abstract': 'We present a model for the unsupervised dis- covery of morphological paradigms. The goal of this model is to induce morphological paradigms from the bible (raw text) and a list of lemmas. We have created a model that splits each lemma in a stem and a suffix, and then we try to create a plausible suffix list by con- sidering lemma pairs. Our model was not able to outperform the official baseline, and there is still room for improvement, but we believe that the ideas presented here are worth considering.',\n",
       "  'year': 2020,\n",
       "  'venue': 'SIGMORPHON'},\n",
       " {'paperId': '028a689379b5095b2ee3043838a5b63ba2a3ee22',\n",
       "  'title': 'Tagging English by Path Voting Constraints',\n",
       "  'authorId': '1748051',\n",
       "  'authorName': 'Gökhan Tür',\n",
       "  'abstract': 'We describe a constraint-based tagging approach where individual constraint rules vote on sequences of matching tokens and tags. Disambiguation of all tokens in a sentence is performed at the very end by selecting tags that appear on the path that receives the highest vote. This constraint application paradigm makes the outcome of the disambiguation independent of the rule sequence, and hence relieves the rule developer from worrying about potentially conflicting rule sequencing. The approach can also combine statistically and manually obtained constraints, and incorporate negative constraint rules to rule out certain patterns. We have applied this approach to tagging English text from the Wall Street Journal and the Brown Corpora. Our results from the Wall Street Journal Corpus indicate that with 400 statistically derived constraint rules and about 800 hand-crafted constraint rules, we can attain an average accuracy of 97.89% on the training corpus and an average accuracy of 97.50% on the testing corpus. We can also relax the single tag per token limitation and allow ambiguous tagging which lets us trade recall and precision.',\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '6d461ac9c8df76b338465c6630c70476c9f9b019',\n",
       "  'title': 'A Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization',\n",
       "  'authorId': '1912135',\n",
       "  'authorName': 'V. Jindal',\n",
       "  'abstract': 'Text categorization has become a key research field in the NLP community. However, most works in this area are focused on Western languages ignoring other Semitic languages like Arabic. These languages are of immense political and social importance necessitating robust categorization techniques. In this paper, we present a novel three-stage technique to efficiently classify Arabic documents into different categories based on the words they contain. We leverage the significance of root-words in Arabic and incorporate a combination of Markov clustering and Deep Belief Networks to classify Arabic words into separate groups (clusters). Our approach is tested on two public datasets giving a F-Measure of 91.02%.',\n",
       "  'year': 2016,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '467d9b8ef81658188594df3224a731a67c2ad084',\n",
       "  'title': 'The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing',\n",
       "  'authorId': '9543306',\n",
       "  'authorName': 'Rik van Noord',\n",
       "  'abstract': 'We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.',\n",
       "  'year': 2017,\n",
       "  'venue': 'SemEval@ACL'},\n",
       " {'paperId': '1db86e01300e2f30fd08b46e63ea11656cb6dcf5',\n",
       "  'title': 'TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models',\n",
       "  'authorId': '2143775574',\n",
       "  'authorName': 'Jie He',\n",
       "  'abstract': 'In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.',\n",
       "  'year': 2021,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '54a2becf5824274588ab3a9c30654604b77b1f27',\n",
       "  'title': 'A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis',\n",
       "  'authorId': '35935570',\n",
       "  'authorName': 'Jean-Benoit Delbrouck',\n",
       "  'abstract': 'Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding (TBJE) for the task of Emotion Recognition and Sentiment Analysis. In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source .',\n",
       "  'year': 2020,\n",
       "  'venue': 'CHALLENGEHML'},\n",
       " {'paperId': 'b94e557cd08f8d985b2a666bb8fb455207c3eb8b',\n",
       "  'title': 'Machine Translation already does Work',\n",
       "  'authorId': '48505446',\n",
       "  'authorName': 'M. King',\n",
       "  'abstract': 'The first difficulty in answering a question like \"Does machine translation work is that the question itself is ill-posed. It takes for granted that there is one single thing called machine translation and that everyone is agreed about what it is. But in fact, even a cursory glance at the systems already around, either in regular operational use or under development, will reveal a wide range of different types of systems. If we take first the dimension determined by who/what does most of the work, the machine or the translator or revisor, at one end of the scale are systems where the human does not intervene at all during the process of translation-\"batch\" systems for convenience here. Even amongst the batch systems there is considerable variety: the degree of pre-editing i~ermitted or required varies greatly, as does the amount of post-editing foreseen. Some systems insist that anything translated by the machine should require no post-editing, and thus (sometimes) reject as unsuitable for machine treatment a part of the text. Others take it for granted that machine translation will normaly be post-edited, just as human translation is normally revised. Some systems aim at giving nothing more than a very rough raw translation , to be used by the human translator only as a starting point for producing his own translation. Some systems require that the document to be translated conform to a restricted syntax, others leave the author relatively free. Next comes a class of systems that one might style \"interactive\" systems, where the bulk of the work is still done by the machine, but where the system interacts with a human to a greater or lesser degree. Such systems may ask the human, for example, to resolve an ambiguity in the source text, to choose between a set of target language terms, to decide on correct use of prepositions, or any combination of the-~e and other similar tasks. Shifting towards the end of the scale where the bulk of the work is done by a human translator aided by a computer system, there are. systems which will automatically insert identified technical terms, or replace a phrase occurring repeatedly in the text by its translation wherever it appears, leaving the rest of the translation to be done by the human translator, systems where the translator as he produces the translation can consult specialist or general dictionaries, either constructed by the translator …',\n",
       "  'year': 1986,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '3eaabbf7b5315264ab845883a822c07cbf632fb0',\n",
       "  'title': 'Dialogue Act Classification in Team Communication for Robot Assisted Disaster Response',\n",
       "  'authorId': '114626341',\n",
       "  'authorName': 'T. Anikina',\n",
       "  'abstract': 'We present the results we obtained on the classification of dialogue acts in a corpus of human-human team communication in the domain of robot-assisted disaster response. We annotated dialogue acts according to the ISO 24617-2 standard scheme and carried out experiments using the FastText linear classifier as well as several neural architectures, including feed-forward, recurrent and convolutional neural models with different types of embeddings, context and attention mechanism. The best performance was achieved with a ”Divide & Merge” architecture presented in the paper, using trainable GloVe embeddings and a structured dialogue history. This model learns from the current utterance and the preceding context separately and then combines the two generated representations. Average accuracy of 10-fold cross-validation is 79.8%, F-score 71.8%.',\n",
       "  'year': 2019,\n",
       "  'venue': 'SIGdial'},\n",
       " {'paperId': '7983b67c4cefb9e9577f6925785f0b27907d9229',\n",
       "  'title': 'Iterative Recursive Attention Model for Interpretable Sequence Classification',\n",
       "  'authorId': '3466460',\n",
       "  'authorName': 'Martin Tutek',\n",
       "  'abstract': 'Natural language processing has greatly benefited from the introduction of the attention mechanism. However, standard attention models are of limited interpretability for tasks that involve a series of inference steps. We describe an iterative recursive attention model, which constructs incremental representations of input data through reusing results of previously computed queries. We train our model on sentiment classification datasets and demonstrate its capacity to identify and combine different aspects of the input in an easily interpretable manner, while obtaining performance close to the state of the art.',\n",
       "  'year': 2018,\n",
       "  'venue': 'BlackboxNLP@EMNLP'},\n",
       " {'paperId': '8b0a0f6d1cd6f3aa9b54be45d5127bb016a98171',\n",
       "  'title': 'The birth of Romanian BERT',\n",
       "  'authorId': '2978516',\n",
       "  'authorName': 'Stefan Daniel Dumitrescu',\n",
       "  'abstract': 'Large-scale pretrained language models have become ubiquitous in Natural Language Processing. However, most of these models are available either in high-resource languages, in particular English, or as multilingual models that compromise performance on individual languages for coverage. This paper introduces Romanian BERT, the first purely Romanian transformer-based language model, pretrained on a large text corpus. We discuss corpus com-position and cleaning, the model training process, as well as an extensive evaluation of the model on various Romanian datasets. We opensource not only the model itself, but also a repository that contains information on how to obtain the corpus, fine-tune and use this model in production (with practical examples), and how to fully replicate the evaluation process.',\n",
       "  'year': 2020,\n",
       "  'venue': 'FINDINGS'},\n",
       " {'paperId': '30964c12b5729fea0fad78ae1b03214e2632496c',\n",
       "  'title': 'Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning',\n",
       "  'authorId': '24859244',\n",
       "  'authorName': 'Minlong Peng',\n",
       "  'abstract': 'In this work, we explore the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries. To this end, we formulate the task as a positive-unlabeled (PU) learning problem and accordingly propose a novel PU learning algorithm to perform the task. We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https://github.com/v-mipeng/LexiconNER.',\n",
       "  'year': 2019,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': 'ba7c11b33ea8a4779137d55a312c732c33f15b5a',\n",
       "  'title': 'What Makes Evaluation Hard?',\n",
       "  'authorId': '2837889',\n",
       "  'authorName': 'H. Tennant',\n",
       "  'abstract': 'Ideally, an evaluation technique should describe an algorithm that an evaluator could use that would result in a score or a vector of scores that depict the level of performance of the natural language system under test. The scores should mirror the subjective evaluation of the system that a qualified judge would make. The evaluation technique should yield consistent scores for multiple tests of one system, and the scores for several systems should serve as a means for comparison among systems. Unfortunately, there is no such evaluation technique for natural language understanding systems. In the following sections, I will attempt to highlight some of the difficulties',\n",
       "  'year': 1981,\n",
       "  'venue': 'ACL'},\n",
       " {'paperId': '2b7076bc3949b27cc6ce23905be8347952507f4f',\n",
       "  'title': 'Modelling Sarcasm in Twitter, a Novel Approach',\n",
       "  'authorId': '47316000',\n",
       "  'authorName': 'Francesco Barbieri',\n",
       "  'abstract': 'Automatic detection of figurative language is a challenging task in computational linguistics. Recognising both literal and figurative meaning is not trivial for a machine and in some cases it is hard even for humans. For this reason novel and accurate systems able to recognise figurative languages are necessary. We present in this paper a novel computational model capable to detect sarcasm in the social network Twitter (a popular microblogging service which allows users to post short messages). Our model is easy to implement and, unlike previous systems, it does not include patterns of words as features. Our seven sets of lexical features aim to detect sarcasm by its inner structure (for example unexpectedness, intensity of the terms or imbalance between registers), abstracting from the use of specific terms.',\n",
       "  'year': 2014,\n",
       "  'venue': 'WASSA@ACL'},\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a6b5b",
   "metadata": {},
   "source": [
    "Data apears to have 7 features (Whose titles are pretty self explanatory):\n",
    "\n",
    "- paperId\n",
    "- authorId\n",
    "- authorName (Dep. var)\n",
    "- abstract\n",
    "- year\n",
    "- venue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6287a07",
   "metadata": {},
   "source": [
    "Converting the json into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ee0d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n",
       "      <td>Detecting linguistic idiosyncratic interests i...</td>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>Children with autism spectrum disorder often e...</td>\n",
       "      <td>2014</td>\n",
       "      <td>CLPsych@ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n",
       "      <td>Bigrams and BiLSTMs Two Neural Networks for Se...</td>\n",
       "      <td>2782720</td>\n",
       "      <td>Yuri Bizzoni</td>\n",
       "      <td>We present and compare two alternative deep ne...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fig-Lang@NAACL-HLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n",
       "      <td>In Factuality: Efficient Integration of Releva...</td>\n",
       "      <td>144748442</td>\n",
       "      <td>Peter Vickers</td>\n",
       "      <td>Visual Question Answering (VQA) methods aim at...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9</td>\n",
       "      <td>Variational Graph Autoencoding as Cheap Superv...</td>\n",
       "      <td>46331602</td>\n",
       "      <td>Irene Li</td>\n",
       "      <td>Coreference resolution over semantic graphs li...</td>\n",
       "      <td>2022</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07588dd5d0252c7abc99b3834a81bf23741ead4b</td>\n",
       "      <td>LIMIT-BERT : Linguistics Informed Multi-Task BERT</td>\n",
       "      <td>30887404</td>\n",
       "      <td>Junru Zhou</td>\n",
       "      <td>In this paper, we present Linguistics Informed...</td>\n",
       "      <td>2019</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12124</th>\n",
       "      <td>5868a7bfe6a4590d332ca66b8097dbe5490c8a73</td>\n",
       "      <td>SmBoP: Semi-autoregressive Bottom-up Semantic ...</td>\n",
       "      <td>2001128224</td>\n",
       "      <td>Ohad Rubin</td>\n",
       "      <td>The de-facto standard decoding method for sema...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NAACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12125</th>\n",
       "      <td>6fbfa7138235b99df43391bff3917b85393c3ca1</td>\n",
       "      <td>UW-Stanford System Description for AESW 2016 S...</td>\n",
       "      <td>3209288</td>\n",
       "      <td>D. Flickinger</td>\n",
       "      <td>This is a report on the methods used and resul...</td>\n",
       "      <td>2016</td>\n",
       "      <td>BEA@NAACL-HLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12126</th>\n",
       "      <td>7226d14e6dea73dfad521256248ec2b19ae66ad8</td>\n",
       "      <td>From Raw Text to Enhanced Universal Dependenci...</td>\n",
       "      <td>144254013</td>\n",
       "      <td>G. Bouma</td>\n",
       "      <td>We describe the second IWPT task on end-to-end...</td>\n",
       "      <td>2021</td>\n",
       "      <td>IWPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12127</th>\n",
       "      <td>cb0f3ee1e98faf92429d601cdcd76c69c1e484eb</td>\n",
       "      <td>Neural Network Acceptability Judgments</td>\n",
       "      <td>46236380</td>\n",
       "      <td>Alex Warstadt</td>\n",
       "      <td>Abstract This paper investigates the ability o...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Transactions of the Association for Computatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12128</th>\n",
       "      <td>30248bea9a739ddba04ec633ee3e156ca8a35e24</td>\n",
       "      <td>Bridging Text and Knowledge with Frames</td>\n",
       "      <td>144928136</td>\n",
       "      <td>S. Narayanan</td>\n",
       "      <td>FrameNet is the best currently operational ver...</td>\n",
       "      <td>2014</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12129 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        paperId  \\\n",
       "0      0b341b6938308a6d5f47edf490f6e46eae3835fa   \n",
       "1      c682727ee058aadbe9dbf838dcb036322818f588   \n",
       "2      0f9b5b32229a7245e43754430c0c88f8e7f0d8af   \n",
       "3      7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9   \n",
       "4      07588dd5d0252c7abc99b3834a81bf23741ead4b   \n",
       "...                                         ...   \n",
       "12124  5868a7bfe6a4590d332ca66b8097dbe5490c8a73   \n",
       "12125  6fbfa7138235b99df43391bff3917b85393c3ca1   \n",
       "12126  7226d14e6dea73dfad521256248ec2b19ae66ad8   \n",
       "12127  cb0f3ee1e98faf92429d601cdcd76c69c1e484eb   \n",
       "12128  30248bea9a739ddba04ec633ee3e156ca8a35e24   \n",
       "\n",
       "                                                   title    authorId  \\\n",
       "0      Detecting linguistic idiosyncratic interests i...     3188285   \n",
       "1      Bigrams and BiLSTMs Two Neural Networks for Se...     2782720   \n",
       "2      In Factuality: Efficient Integration of Releva...   144748442   \n",
       "3      Variational Graph Autoencoding as Cheap Superv...    46331602   \n",
       "4      LIMIT-BERT : Linguistics Informed Multi-Task BERT    30887404   \n",
       "...                                                  ...         ...   \n",
       "12124  SmBoP: Semi-autoregressive Bottom-up Semantic ...  2001128224   \n",
       "12125  UW-Stanford System Description for AESW 2016 S...     3209288   \n",
       "12126  From Raw Text to Enhanced Universal Dependenci...   144254013   \n",
       "12127             Neural Network Acceptability Judgments    46236380   \n",
       "12128            Bridging Text and Knowledge with Frames   144928136   \n",
       "\n",
       "              authorName                                           abstract  \\\n",
       "0      Masoud Rouhizadeh  Children with autism spectrum disorder often e...   \n",
       "1           Yuri Bizzoni  We present and compare two alternative deep ne...   \n",
       "2          Peter Vickers  Visual Question Answering (VQA) methods aim at...   \n",
       "3               Irene Li  Coreference resolution over semantic graphs li...   \n",
       "4             Junru Zhou  In this paper, we present Linguistics Informed...   \n",
       "...                  ...                                                ...   \n",
       "12124         Ohad Rubin  The de-facto standard decoding method for sema...   \n",
       "12125      D. Flickinger  This is a report on the methods used and resul...   \n",
       "12126           G. Bouma  We describe the second IWPT task on end-to-end...   \n",
       "12127      Alex Warstadt  Abstract This paper investigates the ability o...   \n",
       "12128       S. Narayanan  FrameNet is the best currently operational ver...   \n",
       "\n",
       "       year                                              venue  \n",
       "0      2014                                        CLPsych@ACL  \n",
       "1      2018                                 Fig-Lang@NAACL-HLT  \n",
       "2      2021                                                ACL  \n",
       "3      2022                                                ACL  \n",
       "4      2019                                           FINDINGS  \n",
       "...     ...                                                ...  \n",
       "12124  2020                                              NAACL  \n",
       "12125  2016                                      BEA@NAACL-HLT  \n",
       "12126  2021                                               IWPT  \n",
       "12127  2018  Transactions of the Association for Computatio...  \n",
       "12128  2014                                                     \n",
       "\n",
       "[12129 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_raw = pd.read_json('train.json')\n",
    "train_df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55ccd3",
   "metadata": {},
   "source": [
    "A bit of EDA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f05a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12129\n",
      "Intuitively, paper IDs are all unique. Nº paper Ids = Nº rows\n",
      "#############\n",
      "5625\n",
      "13\n",
      "1\n",
      "There are 5625 different authors. The author with the most papers have 13, while the minimun registered number of paper per author is 1.\n",
      "#############\n",
      "[1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Paper have been published in 44 different years between 1979 and 2022\n",
      "#############\n",
      "False\n",
      "Nicely cleansed data: no NaN values\n",
      "#############\n"
     ]
    }
   ],
   "source": [
    "EDA0 = train_df_raw[\"paperId\"].unique()\n",
    "print(len(EDA0))\n",
    "\n",
    "print('''Intuitively, paper IDs are all unique. Nº paper Ids = Nº rows''')\n",
    "print('#############')\n",
    "\n",
    "EDA1 = train_df_raw[\"authorId\"].unique()\n",
    "EDA11 = train_df_raw[\"authorId\"].value_counts()\n",
    "print(len(EDA1))\n",
    "print(max(EDA11))\n",
    "print(min(EDA11))\n",
    "\n",
    "print('''There are 5625 different authors. The author with the most papers have 13, while the minimun registered number of paper per author is 1.''')\n",
    "print('#############')\n",
    "\n",
    "EDA2 = [int(i) for i in train_df_raw[\"year\"].unique()]\n",
    "EDA22 = train_df_raw[\"year\"].value_counts()\n",
    "EDA2.sort()\n",
    "print(EDA2)\n",
    "\n",
    "print('''Paper have been published in 44 different years between 1979 and 2022''')\n",
    "print('#############')\n",
    "\n",
    "EDA3 = train_df_raw.isna().values.any()\n",
    "print(EDA3)\n",
    "\n",
    "print('''Nicely cleansed data: no NaN values''')\n",
    "print('#############')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc703504",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Check the possible existence of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3db57",
   "metadata": {},
   "source": [
    "### Ideas for the clasificator\n",
    "- Try to relate authors to papers by keywords either in abstract or title (Re might be useful). To-do: Develop a method to extract keywords from titels and abstracts.\n",
    "- Discriminate by venue. Each author probably presents papers in only one venue. Analogously from a venue we could infer that his author belongs to a list of those who have presented there.\n",
    "- Year also seems to provide great opportunities for dicrimination. It might be a posibility that authors tend to concentrate their publications in a group of years relatively limited.\n",
    "- Exploit the combinations between the previous three. Certain topics will be \"trendier\" in a given groups of years, Certain venues may only take place in given years etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4562d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832897ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f32a75",
   "metadata": {},
   "source": [
    "### Dirty box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a5027",
   "metadata": {},
   "source": [
    "Crear un dataset en el que cada fila sea un autor, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f472643c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1750769       13\n",
       "1747849       13\n",
       "51042088      12\n",
       "2854981       12\n",
       "3422953       11\n",
       "              ..\n",
       "40192974       1\n",
       "2013172        1\n",
       "2106294609     1\n",
       "5677323        1\n",
       "144928136      1\n",
       "Name: authorId, Length: 5625, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDA11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37277cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='authorId', ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data=EDA11, discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa93d921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019    1845\n",
       "2021    1839\n",
       "2020    1723\n",
       "2018    1361\n",
       "2016    1014\n",
       "2017    1000\n",
       "2015     901\n",
       "2014     800\n",
       "2022     396\n",
       "2002     115\n",
       "1998     112\n",
       "2000      98\n",
       "2003      91\n",
       "1997      67\n",
       "2005      64\n",
       "2006      56\n",
       "1996      48\n",
       "2004      48\n",
       "1992      40\n",
       "2013      38\n",
       "1984      38\n",
       "2001      38\n",
       "1994      35\n",
       "1999      35\n",
       "1983      28\n",
       "1993      28\n",
       "1995      27\n",
       "1988      24\n",
       "1991      24\n",
       "2011      20\n",
       "2010      18\n",
       "1982      17\n",
       "1981      16\n",
       "1990      15\n",
       "1985      14\n",
       "1986      13\n",
       "1987      12\n",
       "1989      12\n",
       "2008      11\n",
       "2009      11\n",
       "2007      10\n",
       "2012      10\n",
       "1980      10\n",
       "1979       7\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDA22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5384f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data=list(EDA22), discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e465f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_raw[\"venue\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e84fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an ordered list of most frequent filtered words in the abstracts\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Opening JSON file, and returning the object as a list of dictionaries. Reminder: it's loading from my local path.\n",
    "f = open('C:/Users/Jamie/Downloads/train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "# Creating a list with all the abstracts in it\n",
    "# Also cleaning everything into lower case and only alphanumerical\n",
    "# Change the 'abstract' to 'title' to get the information about the titles\n",
    "X = []\n",
    "for item in data:\n",
    "    abstract = item.get('abstract')\n",
    "    abstract = re.sub(\"[^a-zA-Z0-9 ]\",\"\",abstract)\n",
    "    X.append(abstract.lower())\n",
    "\n",
    "# Creating a list of all the words \n",
    "word_list = [word for line in X for word in line.split()]    \n",
    "\n",
    "# Removing common irrelevant words from the word_list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Turning that into a frequency dictionary\n",
    "frequency_list = {}\n",
    "for word in filtered_sentence:\n",
    "    if word not in frequency_list:\n",
    "        frequency_list[word] = 0\n",
    "    frequency_list[word] += 1\n",
    "\n",
    "# And into an ordered dictionary, ordered on the frequency count\n",
    "# The dictionary is currently limited to words which occur 1.000 times or more. This can be altered.\n",
    "# This is then turnt into a list, so that we can refer to indexnumbers for variables\n",
    "ordered = dict(sorted(frequency_list.items(), key=lambda item: item[1],reverse=True))\n",
    "orderedDict = {k:v for (k,v) in ordered.items() if v > 1000}\n",
    "orderedListAbstracts = []\n",
    "for item in orderedDict:\n",
    "    orderedListAbstracts.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a052ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an ordered list of most frequent filtered words in the Titles\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Opening JSON file, and returning the object as a list of dictionaries. Reminder: it's loading from my local path.\n",
    "f = open('C:/Users/Jamie/Downloads/train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "# Creating a list with all the titles in it\n",
    "# Also cleaning everything into lower case and only alphanumerical\n",
    "# Change the 'title' to 'abstract' to get the information about the abstracts\n",
    "X = []\n",
    "for item in data:\n",
    "    title = item.get('title')\n",
    "    title = re.sub(\"[^a-zA-Z0-9 ]\",\"\",title)\n",
    "    X.append(title.lower())\n",
    "\n",
    "# Creating a list of all the words \n",
    "word_list = [word for line in X for word in line.split()]    \n",
    "\n",
    "# Removing common irrelevant words from the word_list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Turning that into a frequency dictionary\n",
    "frequency_list = {}\n",
    "for word in filtered_sentence:\n",
    "    if word not in frequency_list:\n",
    "        frequency_list[word] = 0\n",
    "    frequency_list[word] += 1\n",
    "\n",
    "# And into an ordered dictionary, ordered on the frequency count\n",
    "# The dictionary is currently limited to words which occur 100 times or more. This can be altered.\n",
    "# This is then turnt into a list, so that we can refer to indexnumbers for variables\n",
    "ordered = dict(sorted(frequency_list.items(), key=lambda item: item[1],reverse=True))\n",
    "orderedDict = {k:v for (k,v) in ordered.items() if v > 100}\n",
    "orderedListTitles = []\n",
    "for item in orderedDict:\n",
    "    orderedListTitles.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1419e078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 140 ['language', 'learning', 'neural', 'translation', 'using', 'task', 'machine', 'models', 'word', 'text'] ['model', 'models', 'language', 'task', 'data', 'paper', 'show', 'results', 'system', 'performance']\n"
     ]
    }
   ],
   "source": [
    "x = len(orderedListTitles)\n",
    "y = len(orderedListAbstracts)\n",
    "\n",
    "print(x, y, orderedListTitles[0:10], orderedListAbstracts[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9e7407ba02a9e154e785cc147e32b3eb3f6885c4c19aaf3e850b5bb80532a02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
