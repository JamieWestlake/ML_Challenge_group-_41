{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0255333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the nltk package is not installed, execute te following:\n",
    "\n",
    "#! pip install stopwords\n",
    "#! pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5538b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1a6f2",
   "metadata": {},
   "source": [
    "# 0. Loading vanilla dataset as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29833b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n",
       "      <td>Detecting linguistic idiosyncratic interests i...</td>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>Children with autism spectrum disorder often e...</td>\n",
       "      <td>2014</td>\n",
       "      <td>CLPsych@ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n",
       "      <td>Bigrams and BiLSTMs Two Neural Networks for Se...</td>\n",
       "      <td>2782720</td>\n",
       "      <td>Yuri Bizzoni</td>\n",
       "      <td>We present and compare two alternative deep ne...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fig-Lang@NAACL-HLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n",
       "      <td>In Factuality: Efficient Integration of Releva...</td>\n",
       "      <td>144748442</td>\n",
       "      <td>Peter Vickers</td>\n",
       "      <td>Visual Question Answering (VQA) methods aim at...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9</td>\n",
       "      <td>Variational Graph Autoencoding as Cheap Superv...</td>\n",
       "      <td>46331602</td>\n",
       "      <td>Irene Li</td>\n",
       "      <td>Coreference resolution over semantic graphs li...</td>\n",
       "      <td>2022</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07588dd5d0252c7abc99b3834a81bf23741ead4b</td>\n",
       "      <td>LIMIT-BERT : Linguistics Informed Multi-Task BERT</td>\n",
       "      <td>30887404</td>\n",
       "      <td>Junru Zhou</td>\n",
       "      <td>In this paper, we present Linguistics Informed...</td>\n",
       "      <td>2019</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId  \\\n",
       "0  0b341b6938308a6d5f47edf490f6e46eae3835fa   \n",
       "1  c682727ee058aadbe9dbf838dcb036322818f588   \n",
       "2  0f9b5b32229a7245e43754430c0c88f8e7f0d8af   \n",
       "3  7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9   \n",
       "4  07588dd5d0252c7abc99b3834a81bf23741ead4b   \n",
       "\n",
       "                                               title   authorId  \\\n",
       "0  Detecting linguistic idiosyncratic interests i...    3188285   \n",
       "1  Bigrams and BiLSTMs Two Neural Networks for Se...    2782720   \n",
       "2  In Factuality: Efficient Integration of Releva...  144748442   \n",
       "3  Variational Graph Autoencoding as Cheap Superv...   46331602   \n",
       "4  LIMIT-BERT : Linguistics Informed Multi-Task BERT   30887404   \n",
       "\n",
       "          authorName                                           abstract  year  \\\n",
       "0  Masoud Rouhizadeh  Children with autism spectrum disorder often e...  2014   \n",
       "1       Yuri Bizzoni  We present and compare two alternative deep ne...  2018   \n",
       "2      Peter Vickers  Visual Question Answering (VQA) methods aim at...  2021   \n",
       "3           Irene Li  Coreference resolution over semantic graphs li...  2022   \n",
       "4         Junru Zhou  In this paper, we present Linguistics Informed...  2019   \n",
       "\n",
       "                venue  \n",
       "0         CLPsych@ACL  \n",
       "1  Fig-Lang@NAACL-HLT  \n",
       "2                 ACL  \n",
       "3                 ACL  \n",
       "4            FINDINGS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_raw = pd.read_json('data/train.json')\n",
    "train_df_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71595ded",
   "metadata": {},
   "source": [
    "# A. Frequency lists for abstracts and titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1effc",
   "metadata": {},
   "source": [
    "### A.1 Creating an ordered list of most frequent filtered words in the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34654e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an ordered list of most frequent filtered words in the abstracts\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Opening JSON file, and returning the object as a list of dictionaries. Reminder: it's loading from my local path.\n",
    "f = open('data/train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "# Creating a list with all the abstracts in it\n",
    "# Also cleaning everything into lower case and only alphanumerical\n",
    "# Change the 'abstract' to 'title' to get the information about the titles\n",
    "X = []\n",
    "for item in data:\n",
    "    abstract = item.get('abstract')\n",
    "    abstract = re.sub(\"[^a-zA-Z0-9 ]\",\"\",abstract)\n",
    "    X.append(abstract.lower())\n",
    "\n",
    "# Creating a list of all the words \n",
    "word_list = [word for line in X for word in line.split()]    \n",
    "\n",
    "# Removing common irrelevant words from the word_list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Turning that into a frequency dictionary\n",
    "frequency_list = {}\n",
    "for word in filtered_sentence:\n",
    "    if word not in frequency_list:\n",
    "        frequency_list[word] = 0\n",
    "    frequency_list[word] += 1\n",
    "\n",
    "# And into an ordered dictionary, ordered on the frequency count\n",
    "# The dictionary is currently limited to words which occur 1.000 times or more. This can be altered.\n",
    "# This is then turnt into a list, so that we can refer to indexnumbers for variables\n",
    "ordered = dict(sorted(frequency_list.items(), key=lambda item: item[1],reverse=True))\n",
    "orderedDict_abstracts = {k:v for (k,v) in ordered.items() if v > 1000}\n",
    "orderedListAbstracts = []\n",
    "for item in orderedDict_abstracts:\n",
    "    orderedListAbstracts.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d323db",
   "metadata": {},
   "source": [
    "### A.2 Creating an ordered list of most frequent filtered words in the Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fc896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an ordered list of most frequent filtered words in the Titles\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Opening JSON file, and returning the object as a list of dictionaries. Reminder: it's loading from my local path.\n",
    "f = open('data/train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "# Creating a list with all the titles in it\n",
    "# Also cleaning everything into lower case and only alphanumerical\n",
    "# Change the 'title' to 'abstract' to get the information about the abstracts\n",
    "X = []\n",
    "for item in data:\n",
    "    title = item.get('title')\n",
    "    title = re.sub(\"[^a-zA-Z0-9 ]\",\"\",title)\n",
    "    X.append(title.lower())\n",
    "\n",
    "# Creating a list of all the words \n",
    "word_list = [word for line in X for word in line.split()]    \n",
    "\n",
    "# Removing common irrelevant words from the word_list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Turning that into a frequency dictionary\n",
    "frequency_list = {}\n",
    "for word in filtered_sentence:\n",
    "    if word not in frequency_list:\n",
    "        frequency_list[word] = 0\n",
    "    frequency_list[word] += 1\n",
    "\n",
    "# And into an ordered dictionary, ordered on the frequency count\n",
    "# The dictionary is currently limited to words which occur 100 times or more. This can be altered.\n",
    "# This is then turnt into a list, so that we can refer to indexnumbers for variables\n",
    "ordered = dict(sorted(frequency_list.items(), key=lambda item: item[1],reverse=True))\n",
    "orderedDict_titles = {k:v for (k,v) in ordered.items() if v > 100}\n",
    "orderedListTitles = []\n",
    "for item in orderedDict_titles:\n",
    "    orderedListTitles.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854e788",
   "metadata": {},
   "source": [
    "### A.3 Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef569d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 140 ['language', 'learning', 'neural', 'translation', 'using', 'task', 'machine', 'models', 'word', 'text'] ['model', 'models', 'language', 'task', 'data', 'paper', 'show', 'results', 'system', 'performance']\n"
     ]
    }
   ],
   "source": [
    "x = len(orderedListTitles)\n",
    "y = len(orderedListAbstracts)\n",
    "\n",
    "print(x, y, orderedListTitles[0:10], orderedListAbstracts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a690b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dictionaries of word frequencies over determined thresholds\n",
    "# orderedDict_titles\n",
    "# orderedDict_abstracts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
